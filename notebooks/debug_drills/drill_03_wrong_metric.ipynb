{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”§ Debug Drill: The High-Accuracy Flop\n",
    "\n",
    "**Scenario:**\n",
    "Your colleague deployed a churn model to production. \"It has 89% accuracy!\" they said.\n",
    "\n",
    "But after a month, the retention team is frustrated.\n",
    "\n",
    "\"We called 500 customers your model flagged as at-risk,\" they say. \"Only 30 of them actually churned. That's worse than random!\"\n",
    "\n",
    "**Your Task:**\n",
    "1. Run the notebook and find why high accuracy doesn't mean good performance\n",
    "2. Identify the correct metric for this use case\n",
    "3. Fix the evaluation\n",
    "4. Write a 3-bullet postmortem\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    df = pd.read_csv('https://raw.githubusercontent.com/189investmentai/ml-foundations-interactive/main/streamcart_customers.csv')\n",
    "except:\n",
    "    df = pd.read_csv('../../data/streamcart_customers.csv')\n",
    "\n",
    "print(f\"Loaded {len(df):,} customers\")\n",
    "print(f\"Churn rate: {df['churn_30d'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== COLLEAGUE'S CODE =====\n",
    "\n",
    "features = ['tenure_months', 'logins_last_30d', 'support_tickets_last_30d', 'nps_score']\n",
    "\n",
    "X = df[features].fillna(0)\n",
    "y = df['churn_30d']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Colleague's evaluation (WRONG METRIC FOR THE USE CASE)\n",
    "y_pred = model.predict(X_test)  # Binary predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Model Performance\")\n",
    "print(f\"Accuracy: {accuracy:.1%}\")\n",
    "print(f\"\\n'Ship it!' - Your colleague\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the retention team actually did:\n",
    "# They called the top 500 customers the model predicted as \"churn\"\n",
    "\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "top_500_idx = np.argsort(y_proba)[-500:]\n",
    "\n",
    "# How many actually churned?\n",
    "actual_churners_in_top_500 = y_test.iloc[top_500_idx].sum()\n",
    "precision_at_500 = y_test.iloc[top_500_idx].mean()\n",
    "\n",
    "print(f\"\\nðŸ˜¤ Retention Team's Reality Check\")\n",
    "print(f\"Customers called: 500\")\n",
    "print(f\"Actual churners found: {actual_churners_in_top_500}\")\n",
    "print(f\"Precision@500: {precision_at_500:.1%}\")\n",
    "print(f\"\\n'This is barely better than random ({y_test.mean():.1%})!'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Your Investigation\n",
    "\n",
    "### Step 1: Why is accuracy misleading?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a \"dummy\" model that predicts NO ONE will churn\n",
    "# What's its accuracy?\n",
    "\n",
    "dummy_predictions = np.zeros(len(y_test))  # Predict 0 for everyone\n",
    "dummy_accuracy = accuracy_score(y_test, dummy_predictions)\n",
    "\n",
    "print(f\"Dummy model accuracy: {dummy_accuracy:.1%}\")\n",
    "print(f\"Our model accuracy: {accuracy:.1%}\")\n",
    "print(f\"\\nâ“ Why is the dummy model almost as good?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do you notice? Write your diagnosis:\n",
    "\n",
    "diagnosis = \"\"\"\n",
    "YOUR DIAGNOSIS:\n",
    "\n",
    "Why is accuracy misleading for this problem?\n",
    "\n",
    "What metric should we use instead, given that the team can only call 500 customers?\n",
    "\n",
    "\"\"\"\n",
    "print(diagnosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Use the right metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the correct metrics for this use case\n",
    "#\n",
    "# Given: Team can call 500 customers per week\n",
    "# Goal: Maximize churners found in those 500 calls\n",
    "# \n",
    "# What metric captures this? Precision@K!\n",
    "\n",
    "def precision_at_k(y_true, y_proba, k):\n",
    "    \"\"\"Calculate precision in top K predictions.\"\"\"\n",
    "    top_k_idx = np.argsort(y_proba)[-k:]\n",
    "    return y_true.iloc[top_k_idx].mean()\n",
    "\n",
    "# Calculate for different K values\n",
    "# ...\n",
    "\n",
    "# Compare to random baseline\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train a better model optimizing for the RIGHT goal\n",
    "\n",
    "# Try different model configurations\n",
    "# Track Precision@500, not accuracy\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SELF-CHECK\n",
    "# ============================================\n",
    "\n",
    "# A good model should have Precision@500 at least 1.5x the baseline\n",
    "baseline = y_test.mean()\n",
    "# lift = your_precision_at_500 / baseline\n",
    "\n",
    "# assert lift > 1.5, f\"Lift is only {lift:.1f}x - model isn't useful!\"\n",
    "# print(f\"âœ“ Precision@500 lift: {lift:.1f}x baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Write your postmortem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postmortem = \"\"\"\n",
    "## Postmortem: Wrong Metric Bug\n",
    "\n",
    "### What happened:\n",
    "- (Your answer)\n",
    "\n",
    "### Root cause:\n",
    "- (Your answer - explain why accuracy is wrong for imbalanced data)\n",
    "\n",
    "### How to prevent:\n",
    "- (Your answer - mention business constraints â†’ metric choice)\n",
    "\"\"\"\n",
    "\n",
    "print(postmortem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Drill Complete!\n",
    "\n",
    "**Key lesson:** Always choose metrics based on business constraints. High accuracy means nothing if the model doesn't help you make better decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
