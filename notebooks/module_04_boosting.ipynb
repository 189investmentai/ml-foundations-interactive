{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: Combining Many Weak Learners\n",
    "\n",
    "**Goal:** Understand gradient boosting (LightGBM/XGBoost) and tune for real performance\n",
    "\n",
    "**Time:** ~20 minutes\n",
    "\n",
    "**What you'll do:**\n",
    "1. Train LightGBM and compare to random forest\n",
    "2. Understand the key hyperparameters\n",
    "3. Detect and prevent overfitting\n",
    "4. Use early stopping\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install lightgbm if needed (Colab has it pre-installed)\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except ImportError:\n",
    "    !pip install lightgbm -q\n",
    "    import lightgbm as lgb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    df = pd.read_csv('https://raw.githubusercontent.com/189investmentai/ml-foundations-interactive/main/streamcart_customers.csv')\n",
    "except:\n",
    "    df = pd.read_csv('../data/streamcart_customers.csv')\n",
    "\n",
    "# Prepare features\n",
    "features = ['tenure_months', 'logins_last_7d', 'logins_last_30d',\n",
    "            'support_tickets_last_30d', 'items_skipped_last_3_boxes', 'nps_score']\n",
    "\n",
    "X = df[features].fillna(-1)  # LightGBM handles -1 as missing\n",
    "y = df['churn_30d']\n",
    "\n",
    "# Split: train, validation, test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(X_train):,} | Validation: {len(X_val):,} | Test: {len(X_test):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Train LightGBM\n",
    "\n",
    "LightGBM = fast gradient boosting. Industry standard for tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic LightGBM model\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "lgb_probs = lgb_model.predict_proba(X_test)[:, 1]\n",
    "lgb_auc = roc_auc_score(y_test, lgb_probs)\n",
    "\n",
    "print(f\"LightGBM AUC: {lgb_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to random forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_auc = roc_auc_score(y_test, rf_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f\"=== AUC Comparison ===\")\n",
    "print(f\"Random Forest: {rf_auc:.3f}\")\n",
    "print(f\"LightGBM:      {lgb_auc:.3f}\")\n",
    "print(f\"\\nDifference: {lgb_auc - rf_auc:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Key Hyperparameters\n",
    "\n",
    "These 4 parameters matter most. Learn what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Big 4 hyperparameters\n",
    "hyperparams = \"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  HYPERPARAMETER    â”‚  WHAT IT DOES                â”‚  TYPICAL RANGE         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  n_estimators      â”‚  Number of trees (rounds)    â”‚  100-1000              â”‚\n",
    "â”‚  max_depth         â”‚  How deep each tree goes     â”‚  3-10                  â”‚\n",
    "â”‚  learning_rate     â”‚  Step size per tree          â”‚  0.01-0.3              â”‚\n",
    "â”‚  num_leaves        â”‚  Max leaves per tree         â”‚  15-127                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "KEY INSIGHT: learning_rate and n_estimators trade off.\n",
    "- Lower learning_rate + more trees = same result but slower\n",
    "- Lower learning_rate usually gives better generalization\n",
    "\"\"\"\n",
    "print(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with different learning rates\n",
    "#\n",
    "# Train 3 models with learning_rate = [0.01, 0.1, 0.3]\n",
    "# All with n_estimators=100, max_depth=5\n",
    "\n",
    "learning_rates = [0.01, 0.1, 0.3]\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=100, max_depth=5, learning_rate=lr, random_state=42, verbose=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_auc = roc_auc_score(y_train, model.predict_proba(X_train)[:, 1])\n",
    "    test_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    \n",
    "    results.append({'learning_rate': lr, 'train_auc': train_auc, 'test_auc': test_auc})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['gap'] = results_df['train_auc'] - results_df['test_auc']\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you notice?\n",
    "\n",
    "- Higher learning rates = bigger train-test gap = overfitting\n",
    "- Lower learning rates = need more trees to reach same performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Early Stopping\n",
    "\n",
    "The best way to prevent overfitting: stop when validation performance stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with early stopping\n",
    "lgb_early = lgb.LGBMClassifier(\n",
    "    n_estimators=1000,  # Set high, let early stopping decide\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Fit with early stopping\n",
    "lgb_early.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=False)]\n",
    ")\n",
    "\n",
    "print(f\"Stopped at {lgb_early.best_iteration_} trees (out of max 1000)\")\n",
    "\n",
    "# Evaluate\n",
    "early_probs = lgb_early.predict_proba(X_test)[:, 1]\n",
    "early_auc = roc_auc_score(y_test, early_probs)\n",
    "print(f\"Test AUC: {early_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training process\n",
    "evals_result = {}\n",
    "lgb_viz = lgb.LGBMClassifier(\n",
    "    n_estimators=300, max_depth=5, learning_rate=0.05, random_state=42, verbose=-1\n",
    ")\n",
    "\n",
    "lgb_viz.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    eval_names=['train', 'validation'],\n",
    "    callbacks=[lgb.record_evaluation(evals_result)]\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(evals_result['train']['binary_logloss'], label='Train')\n",
    "plt.plot(evals_result['validation']['binary_logloss'], label='Validation')\n",
    "plt.xlabel('Boosting Round')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('Training Progress (lower is better)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark overfitting region\n",
    "min_val = np.argmin(evals_result['validation']['binary_logloss'])\n",
    "plt.axvline(x=min_val, color='red', linestyle='--', label=f'Best iteration ({min_val})')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ’¡ After iteration {min_val}, validation loss increases while train keeps decreasing.\")\n",
    "print(f\"   That's overfitting! Early stopping would stop at {min_val}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': lgb_early.feature_importances_\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(importance_df['feature'], importance_df['importance'], color='darkgreen')\n",
    "plt.xlabel('Importance (Split Count)')\n",
    "plt.title('LightGBM Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Business Metric Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision@500 for all models\n",
    "k = 500\n",
    "baseline = y_test.mean()\n",
    "\n",
    "def precision_at_k(y_true, y_proba, k):\n",
    "    top_k = np.argsort(y_proba)[-k:]\n",
    "    return y_true.iloc[top_k].mean()\n",
    "\n",
    "rf_prec = precision_at_k(y_test, rf_model.predict_proba(X_test)[:, 1], k)\n",
    "lgb_prec = precision_at_k(y_test, lgb_probs, k)\n",
    "early_prec = precision_at_k(y_test, early_probs, k)\n",
    "\n",
    "print(f\"=== Precision@{k} ===\")\n",
    "print(f\"Random baseline:       {baseline:.1%}\")\n",
    "print(f\"Random Forest:         {rf_prec:.1%} (lift: {rf_prec/baseline:.1f}x)\")\n",
    "print(f\"LightGBM (basic):      {lgb_prec:.1%} (lift: {lgb_prec/baseline:.1f}x)\")\n",
    "print(f\"LightGBM (early stop): {early_prec:.1%} (lift: {early_prec/baseline:.1f}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Quick Tuning Recipe\n",
    "\n",
    "This works well in practiceâ€”no need for complex grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The practical tuning recipe\n",
    "recipe = \"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP  â”‚  WHAT TO DO                                                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  1     â”‚  Start with learning_rate=0.05, n_estimators=1000, max_depth=5     â”‚\n",
    "â”‚  2     â”‚  Use early stopping with 20-50 rounds patience                     â”‚\n",
    "â”‚  3     â”‚  If overfitting: lower max_depth to 3-4                            â”‚\n",
    "â”‚  4     â”‚  If underfitting: raise max_depth to 6-8 or num_leaves to 64-127   â”‚\n",
    "â”‚  5     â”‚  For final model: lower learning_rate to 0.01, increase estimators â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ğŸ’¡ 80% of the gains come from:\n",
    "   - Using early stopping\n",
    "   - Not having obvious data issues (leakage, wrong features)\n",
    "\"\"\"\n",
    "print(recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Final Exercise: Explain It\n",
    "\n",
    "Your data scientist colleague asks: \"Why use LightGBM instead of XGBoost?\"\n",
    "\n",
    "Write a 3-4 sentence response explaining when each is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your response:\n",
    "\n",
    "colleague_response = \"\"\"\n",
    "YOUR RESPONSE HERE\n",
    "\n",
    "Hint: LightGBM is faster, XGBoost is more battle-tested.\n",
    "Both give similar results. Pick based on your team's preference.\n",
    "\"\"\"\n",
    "\n",
    "print(colleague_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Module 4 Complete!\n",
    "\n",
    "**What you learned:**\n",
    "- How gradient boosting works (trees fixing previous trees' mistakes)\n",
    "- The key hyperparameters and their effects\n",
    "- Why early stopping prevents overfitting\n",
    "- A practical tuning recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=== Module 4 Summary ===\")\n",
    "print(f\"\\nBest Model: LightGBM with early stopping\")\n",
    "print(f\"Test AUC: {early_auc:.3f}\")\n",
    "print(f\"Precision@500: {early_prec:.1%} ({early_prec/baseline:.1f}x lift)\")\n",
    "print(f\"Trees used: {lgb_early.best_iteration_} (vs max 1000)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next:** [Module 5: Feature Engineering â†’](./module_05_features.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
