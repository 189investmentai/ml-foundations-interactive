{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANSWER KEY: Debug Drill 04 - Overfitting Trap\n",
    "\n",
    "**Bug:** LightGBM hyperparameters are too aggressive:\n",
    "- `n_estimators=2000` (too many trees)\n",
    "- `num_leaves=256` (too complex)\n",
    "- `max_depth=-1` (unlimited depth)\n",
    "- `min_child_samples=1` (can fit single examples)\n",
    "- No early stopping\n",
    "\n",
    "**Key Lesson:** Use regularization + early stopping. Large train/test gap = overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/189investmentai/ml-foundations-interactive/main/shared/data/streamcart_customers.csv')\n",
    "\n",
    "features = ['tenure_months', 'logins_last_30d', 'orders_last_30d', \n",
    "            'support_tickets_last_30d', 'nps_score']\n",
    "X = df[features].fillna(0)\n",
    "y = df['churn_30d']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bug (Colleague's Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== BUGGY CODE =====\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_buggy = lgb.LGBMClassifier(\n",
    "    n_estimators=2000,      # Too many!\n",
    "    num_leaves=256,         # Too complex!\n",
    "    max_depth=-1,           # No limit!\n",
    "    min_child_samples=1,    # Can memorize!\n",
    "    learning_rate=0.3,      # Too fast!\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "model_buggy.fit(X_train, y_train)\n",
    "\n",
    "train_auc = roc_auc_score(y_train, model_buggy.predict_proba(X_train)[:, 1])\n",
    "test_auc = roc_auc_score(y_test, model_buggy.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f\"Buggy model:\")\n",
    "print(f\"  Training AUC: {train_auc:.4f}\")\n",
    "print(f\"  Test AUC: {test_auc:.4f}\")\n",
    "print(f\"  Gap: {train_auc - test_auc:.4f}\")\n",
    "print(f\"\\nThis gap indicates severe overfitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Is Wrong\n",
    "\n",
    "| Parameter | Problem |\n",
    "|-----------|--------|\n",
    "| `n_estimators=2000` | Way too many trees without early stopping |\n",
    "| `num_leaves=256` | Trees are too complex, can memorize data |\n",
    "| `max_depth=-1` | No depth limit = arbitrarily deep trees |\n",
    "| `min_child_samples=1` | Can create leaves with single examples |\n",
    "| `learning_rate=0.3` | High LR + many trees = overfitting |\n",
    "\n",
    "**The giveaway:** Train AUC near 1.0 but test AUC much lower = model memorized training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FIXED CODE =====\n",
    "\n",
    "# Split into train/val/test for early stopping\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n",
    "\n",
    "model_fixed = lgb.LGBMClassifier(\n",
    "    n_estimators=1000,       # High, but early stopping will find optimal\n",
    "    num_leaves=31,           # Reasonable complexity (default)\n",
    "    max_depth=6,             # Limit depth\n",
    "    min_child_samples=20,    # Require 20+ samples per leaf\n",
    "    learning_rate=0.05,      # Lower = more gradual learning\n",
    "    reg_alpha=0.1,           # L1 regularization\n",
    "    reg_lambda=0.1,          # L2 regularization\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Early stopping monitors validation set\n",
    "model_fixed.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    ")\n",
    "\n",
    "train_auc = roc_auc_score(y_train, model_fixed.predict_proba(X_train)[:, 1])\n",
    "val_auc = roc_auc_score(y_val, model_fixed.predict_proba(X_val)[:, 1])\n",
    "test_auc = roc_auc_score(y_test, model_fixed.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f\"Fixed model:\")\n",
    "print(f\"  Training AUC: {train_auc:.4f}\")\n",
    "print(f\"  Validation AUC: {val_auc:.4f}\")\n",
    "print(f\"  Test AUC: {test_auc:.4f}\")\n",
    "print(f\"  Train-Test Gap: {train_auc - test_auc:.4f}\")\n",
    "print(f\"\\nTrees used: {model_fixed.best_iteration_} (stopped early!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-check\n",
    "gap = abs(train_auc - test_auc)\n",
    "assert gap < 0.10, f\"Gap still too large: {gap:.4f}\"\n",
    "assert test_auc > 0.60, f\"Test AUC too low: {test_auc:.4f}\"\n",
    "print(\"PASS: Overfitting controlled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Comparison\n",
    "\n",
    "| Parameter | Buggy | Fixed | Why |\n",
    "|-----------|-------|-------|-----|\n",
    "| n_estimators | 2000 | 1000 + early stopping | Let data decide |\n",
    "| num_leaves | 256 | 31 | Simpler trees |\n",
    "| max_depth | -1 (unlimited) | 6 | Limit complexity |\n",
    "| min_child_samples | 1 | 20 | Require evidence |\n",
    "| learning_rate | 0.3 | 0.05 | Slower, more stable |\n",
    "| regularization | None | L1+L2 | Penalize complexity |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completed Postmortem\n",
    "\n",
    "### Root cause:\n",
    "- Hyperparameters allowed the model to memorize training data\n",
    "- No early stopping meant all 2000 trees were used, even after validation performance peaked\n",
    "\n",
    "### How we detected it:\n",
    "- Massive gap between training AUC (~0.99) and test AUC (~0.60)\n",
    "- Train performance \"too good to be true\"\n",
    "\n",
    "### Prevention for next time:\n",
    "- Always use early stopping with a validation set\n",
    "- Start with conservative hyperparameters (default num_leaves=31, max_depth=6)\n",
    "- Monitor train/val gap during tuning-if gap grows, add regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
