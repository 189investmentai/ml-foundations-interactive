{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANSWER KEY: Debug Drill 03 - Wrong Metric\n",
    "\n",
    "**Bug:** Colleague optimized for accuracy on an imbalanced dataset where only 8% of customers churn.\n",
    "\n",
    "**Key Lesson:** Accuracy is misleading for imbalanced problems. A model predicting \"no one churns\" gets 92% accuracy while being useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/189investmentai/ml-foundations-interactive/main/data/streamcart_customers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bug (Colleague's Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== BUGGY CODE =====\n",
    "features = ['tenure_months', 'logins_last_30d', 'orders_last_30d', 'support_tickets_last_30d']\n",
    "X = df[features].fillna(0)\n",
    "y = df['churn_30d']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Colleague says: 'My model has {accuracy:.1%} accuracy! Ship it!'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Is Wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the class balance\n",
    "churn_rate = y.mean()\n",
    "print(f\"Class balance:\")\n",
    "print(f\"  Churners: {churn_rate:.1%}\")\n",
    "print(f\"  Non-churners: {1-churn_rate:.1%}\")\n",
    "\n",
    "# What's the \"naive\" accuracy?\n",
    "naive_accuracy = 1 - churn_rate\n",
    "print(f\"\\nNaive baseline (predict 'no churn' for everyone): {naive_accuracy:.1%} accuracy\")\n",
    "print(f\"Colleague's model: {accuracy:.1%} accuracy\")\n",
    "print(f\"\\nThe model barely beats predicting 'no one churns'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the model actually predicting?\n",
    "print(f\"\\nModel predictions:\")\n",
    "print(f\"  Predicted churn: {y_pred.sum()}\")\n",
    "print(f\"  Predicted no churn: {len(y_pred) - y_pred.sum()}\")\n",
    "print(f\"  Actual churners in test: {y_test.sum()}\")\n",
    "\n",
    "if y_pred.sum() < 10:\n",
    "    print(\"\\nThe model is barely predicting anyone will churn!\")\n",
    "    print(\"It's essentially predicting the majority class for everyone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FIXED CODE =====\n",
    "# Use appropriate metrics for imbalanced data\n",
    "\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Correct evaluation for imbalanced data:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# AUC - measures ranking ability\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"AUC: {auc:.3f}\")\n",
    "print(\"  (0.5 = random, 1.0 = perfect ranking)\")\n",
    "\n",
    "# Precision@K - what matters if we can only call K customers\n",
    "k = 100  # Assume we can call 100 customers\n",
    "top_k_indices = np.argsort(y_proba)[::-1][:k]\n",
    "precision_at_k = y_test.iloc[top_k_indices].mean()\n",
    "print(f\"\\nPrecision@{k}: {precision_at_k:.1%}\")\n",
    "print(f\"  (Base rate: {churn_rate:.1%})\")\n",
    "print(f\"  (Lift: {precision_at_k/churn_rate:.1f}x vs random)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to business baseline\n",
    "print(\"\\nBusiness comparison:\")\n",
    "print(f\"Random selection of 100 customers: ~{int(100*churn_rate)} churners\")\n",
    "print(f\"Model's top 100 customers: ~{int(100*precision_at_k)} churners\")\n",
    "print(f\"\\nThe model finds {precision_at_k/churn_rate:.1f}x more churners than random!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-check\n",
    "assert auc > 0.55, \"AUC should be better than random\"\n",
    "assert precision_at_k > churn_rate, \"Model should beat baseline\"\n",
    "print(\"\\nPASS: Using correct metrics for imbalanced data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completed Postmortem\n",
    "\n",
    "### What happened:\n",
    "- Colleague celebrated high accuracy on a dataset where 92% of customers don't churn\n",
    "- The model learned to predict \"no churn\" for almost everyone (safe but useless)\n",
    "\n",
    "### Root cause:\n",
    "- Accuracy rewards predicting the majority class\n",
    "- With 8% churn rate, predicting \"no one churns\" gives 92% accuracy automatically\n",
    "\n",
    "### How to prevent:\n",
    "- For imbalanced problems, use AUC, Precision@K, or Recall\n",
    "- Always compare to a naive baseline (majority class prediction)\n",
    "- Ask: \"What would random selection achieve?\" and ensure the model beats it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
