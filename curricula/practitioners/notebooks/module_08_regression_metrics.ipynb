{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 8: Regression Metrics\n",
        "\n",
        "**Goal:** Understand when to use MAE vs RMSE vs RÂ², interpret residual plots, and communicate model performance.\n",
        "\n",
        "**Prerequisites:** Module 3 (Linear Regression)\n",
        "\n",
        "**Expected Runtime:** ~20 minutes\n",
        "\n",
        "**Outputs:**\n",
        "- Side-by-side metric comparisons\n",
        "- Residual analysis plots\n",
        "- Business-ready performance summary\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 11"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Generate Sample Data\n",
        "\n",
        "We'll create a synthetic LTV prediction problem with known characteristics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate synthetic customer LTV data\n",
        "n_samples = 500\n",
        "\n",
        "# Features\n",
        "tenure_months = np.random.uniform(1, 48, n_samples)\n",
        "avg_order_value = np.random.uniform(20, 200, n_samples)\n",
        "orders_per_month = np.random.uniform(0.5, 4, n_samples)\n",
        "\n",
        "# True LTV relationship (with some noise)\n",
        "true_ltv = (\n",
        "    50 +  # base value\n",
        "    tenure_months * 5 +  # loyalty effect\n",
        "    avg_order_value * 2 +  # spending effect\n",
        "    orders_per_month * 30 +  # frequency effect\n",
        "    np.random.normal(0, 50, n_samples)  # noise\n",
        ")\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'tenure_months': tenure_months,\n",
        "    'avg_order_value': avg_order_value,\n",
        "    'orders_per_month': orders_per_month,\n",
        "    'ltv': true_ltv\n",
        "})\n",
        "\n",
        "# Add a few outliers (high-value customers)\n",
        "outlier_idx = np.random.choice(n_samples, size=10, replace=False)\n",
        "df.loc[outlier_idx, 'ltv'] += np.random.uniform(300, 600, 10)\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Train a Simple Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Split data\n",
        "X = df[['tenure_months', 'avg_order_value', 'orders_per_month']]\n",
        "y = df['ltv']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Calculate All Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def calculate_all_metrics(y_true, y_pred, name=\"\"):\n",
        "    \"\"\"Calculate and display all regression metrics.\"\"\"\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    \n",
        "    # MAPE (avoiding division by zero)\n",
        "    mask = y_true != 0\n",
        "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "    \n",
        "    print(f\"\\n=== {name} Metrics ===\")\n",
        "    print(f\"MAE:  ${mae:.2f}  (average absolute error)\")\n",
        "    print(f\"RMSE: ${rmse:.2f}  (penalizes big errors)\")\n",
        "    print(f\"RÂ²:   {r2:.4f}  ({r2*100:.1f}% variance explained)\")\n",
        "    print(f\"MAPE: {mape:.1f}%  (percentage error)\")\n",
        "    print(f\"\\nRMSE/MAE ratio: {rmse/mae:.2f}\")\n",
        "    if rmse/mae > 1.3:\n",
        "        print(\"âš ï¸  Ratio > 1.3 suggests outlier errors\")\n",
        "    \n",
        "    return {'mae': mae, 'rmse': rmse, 'r2': r2, 'mape': mape}\n",
        "\n",
        "train_metrics = calculate_all_metrics(y_train, y_pred_train, \"Training Set\")\n",
        "test_metrics = calculate_all_metrics(y_test, y_pred_test, \"Test Set\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Residual Analysis\n",
        "\n",
        "Residuals = Actual - Predicted. They reveal *how* your model is wrong."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate residuals\n",
        "residuals_test = y_test - y_pred_test\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# 1. Predicted vs Actual\n",
        "ax1 = axes[0, 0]\n",
        "ax1.scatter(y_pred_test, y_test, alpha=0.5, c='#0ea5e9')\n",
        "ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'g--', lw=2, label='Perfect predictions')\n",
        "ax1.set_xlabel('Predicted LTV ($)')\n",
        "ax1.set_ylabel('Actual LTV ($)')\n",
        "ax1.set_title('Predicted vs Actual')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Residuals vs Predicted\n",
        "ax2 = axes[0, 1]\n",
        "ax2.scatter(y_pred_test, residuals_test, alpha=0.5, c='#0ea5e9')\n",
        "ax2.axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "ax2.set_xlabel('Predicted LTV ($)')\n",
        "ax2.set_ylabel('Residual ($)')\n",
        "ax2.set_title('Residuals vs Predicted (look for patterns!)')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Residual Distribution\n",
        "ax3 = axes[1, 0]\n",
        "ax3.hist(residuals_test, bins=30, color='#0ea5e9', edgecolor='white', alpha=0.7)\n",
        "ax3.axvline(x=0, color='r', linestyle='--', lw=2, label='Zero (ideal center)')\n",
        "ax3.axvline(x=residuals_test.mean(), color='orange', linestyle='-', lw=2, label=f'Mean: ${residuals_test.mean():.1f}')\n",
        "ax3.set_xlabel('Residual ($)')\n",
        "ax3.set_ylabel('Count')\n",
        "ax3.set_title('Residual Distribution (should be centered at 0)')\n",
        "ax3.legend()\n",
        "\n",
        "# 4. Q-Q Plot (normality check)\n",
        "ax4 = axes[1, 1]\n",
        "from scipy import stats\n",
        "stats.probplot(residuals_test, dist=\"norm\", plot=ax4)\n",
        "ax4.set_title('Q-Q Plot (should follow diagonal for normal errors)')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Interpretation\n",
        "print(\"\\nðŸ“Š Residual Analysis Summary:\")\n",
        "print(f\"  Mean residual: ${residuals_test.mean():.2f} (should be ~0)\")\n",
        "print(f\"  Std of residuals: ${residuals_test.std():.2f}\")\n",
        "print(f\"  Skewness: {residuals_test.skew():.2f} (0 = symmetric)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Comparing to Baseline\n",
        "\n",
        "Always compare your model to a naive baseline. The simplest: predict the mean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Baseline: predict the training mean for everyone\n",
        "baseline_pred = np.full(len(y_test), y_train.mean())\n",
        "\n",
        "baseline_mae = mean_absolute_error(y_test, baseline_pred)\n",
        "baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_pred))\n",
        "\n",
        "print(\"=== Baseline (Predict Mean) ===\")\n",
        "print(f\"Prediction: ${y_train.mean():.2f} for everyone\")\n",
        "print(f\"Baseline MAE:  ${baseline_mae:.2f}\")\n",
        "print(f\"Baseline RMSE: ${baseline_rmse:.2f}\")\n",
        "\n",
        "print(\"\\n=== Our Model ===\")\n",
        "print(f\"Model MAE:  ${test_metrics['mae']:.2f}\")\n",
        "print(f\"Model RMSE: ${test_metrics['rmse']:.2f}\")\n",
        "\n",
        "print(\"\\n=== Improvement ===\")\n",
        "mae_improvement = (baseline_mae - test_metrics['mae']) / baseline_mae * 100\n",
        "rmse_improvement = (baseline_rmse - test_metrics['rmse']) / baseline_rmse * 100\n",
        "print(f\"MAE improvement:  {mae_improvement:.1f}% reduction\")\n",
        "print(f\"RMSE improvement: {rmse_improvement:.1f}% reduction\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Impact of Outliers\n",
        "\n",
        "Let's see how outliers affect MAE vs RMSE differently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Identify outlier errors\n",
        "error_threshold = 2 * residuals_test.std()  # ~95th percentile\n",
        "outlier_mask = np.abs(residuals_test) > error_threshold\n",
        "\n",
        "print(f\"Error threshold: ${error_threshold:.2f}\")\n",
        "print(f\"Outlier predictions: {outlier_mask.sum()} ({outlier_mask.sum()/len(y_test)*100:.1f}%)\")\n",
        "\n",
        "# Metrics with and without outliers\n",
        "y_test_clean = y_test[~outlier_mask]\n",
        "y_pred_clean = y_pred_test[~outlier_mask]\n",
        "\n",
        "mae_all = mean_absolute_error(y_test, y_pred_test)\n",
        "rmse_all = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
        "\n",
        "mae_clean = mean_absolute_error(y_test_clean, y_pred_clean)\n",
        "rmse_clean = np.sqrt(mean_squared_error(y_test_clean, y_pred_clean))\n",
        "\n",
        "print(\"\\n         |  All Data  |  Without Outliers  |  Change\")\n",
        "print(\"-\" * 55)\n",
        "print(f\"MAE      |  ${mae_all:>7.2f}  |  ${mae_clean:>7.2f}          |  {(mae_all-mae_clean)/mae_all*100:>+5.1f}%\")\n",
        "print(f\"RMSE     |  ${rmse_all:>7.2f}  |  ${rmse_clean:>7.2f}          |  {(rmse_all-rmse_clean)/rmse_all*100:>+5.1f}%\")\n",
        "print(f\"RMSE/MAE |  {rmse_all/mae_all:>8.2f}  |  {rmse_clean/mae_clean:>8.2f}          |\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Notice: RMSE is more affected by outliers than MAE!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: TODO - Segmented Analysis\n",
        "\n",
        "Overall metrics can hide problems with specific customer segments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Analyze metrics by customer segment\n",
        "def segment_analysis(y_true, y_pred, segment_col, segment_name):\n",
        "    \"\"\"Analyze metrics by segment.\"\"\"\n",
        "    results = []\n",
        "    for segment in sorted(segment_col.unique()):\n",
        "        mask = segment_col == segment\n",
        "        if mask.sum() > 5:\n",
        "            mae = mean_absolute_error(y_true[mask], y_pred[mask])\n",
        "            rmse = np.sqrt(mean_squared_error(y_true[mask], y_pred[mask]))\n",
        "            r2 = r2_score(y_true[mask], y_pred[mask])\n",
        "            results.append({\n",
        "                segment_name: segment,\n",
        "                'Count': mask.sum(),\n",
        "                'MAE': mae,\n",
        "                'RMSE': rmse,\n",
        "                'RÂ²': r2\n",
        "            })\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Create LTV segments and analyze performance in each\n",
        "ltv_segments = pd.qcut(y_test, q=3, labels=['Low', 'Medium', 'High'])\n",
        "segment_results = segment_analysis(y_test.values, y_pred_test, ltv_segments, 'LTV Segment')\n",
        "\n",
        "print(\"\\n=== Performance by LTV Segment ===\")\n",
        "print(segment_results.to_string(index=False))\n",
        "\n",
        "# Insight: Which segment has the worst RÂ²? That's where the model struggles most."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Stakeholder Summary\n",
        "\n",
        "### TODO: Write a 3-bullet summary (~100 words) for stakeholders\n",
        "\n",
        "Template:\n",
        "â€¢ **Model accuracy:** Average prediction error is $____ (MAE). The model explains ____% of LTV variance (RÂ²).\n",
        "â€¢ **Segment performance:** The model performs [best/worst] for [segment] customers because [reason].\n",
        "â€¢ **Business impact:** [What does this accuracy mean for decisions? Where should we not trust the model?]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Summary:\n",
        "\n",
        "*Write your stakeholder summary here...*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: Visualization for Stakeholders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a stakeholder-friendly visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Left: Prediction accuracy by value range\n",
        "ax1 = axes[0]\n",
        "bins = [0, 200, 400, 600, 800, 1000, float('inf')]\n",
        "labels = ['$0-200', '$200-400', '$400-600', '$600-800', '$800-1000', '$1000+']\n",
        "y_binned = pd.cut(y_test, bins=bins, labels=labels)\n",
        "\n",
        "y_pred_series = pd.Series(y_pred_test, index=y_test.index)\n",
        "segment_mae = y_test.groupby(y_binned).apply(lambda x: mean_absolute_error(x, y_pred_series[x.index]))\n",
        "segment_counts = y_binned.value_counts().sort_index()\n",
        "\n",
        "ax1.bar(range(len(segment_mae)), segment_mae.values, color='#0ea5e9', alpha=0.7)\n",
        "ax1.set_xticks(range(len(labels)))\n",
        "ax1.set_xticklabels(labels, rotation=45, ha='right')\n",
        "ax1.set_ylabel('Mean Absolute Error ($)')\n",
        "ax1.set_title('Prediction Error by Customer Value')\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Right: Error distribution in business terms\n",
        "ax2 = axes[1]\n",
        "abs_errors = np.abs(residuals_test)\n",
        "thresholds = [25, 50, 100, 200]\n",
        "percentages = [(abs_errors <= t).mean() * 100 for t in thresholds]\n",
        "\n",
        "ax2.barh(range(len(thresholds)), percentages, color='#22c55e', alpha=0.7)\n",
        "ax2.set_yticks(range(len(thresholds)))\n",
        "ax2.set_yticklabels([f'Within ${t}' for t in thresholds])\n",
        "ax2.set_xlabel('% of Predictions')\n",
        "ax2.set_title('Prediction Accuracy Breakdown')\n",
        "ax2.set_xlim(0, 100)\n",
        "\n",
        "for i, pct in enumerate(percentages):\n",
        "    ax2.text(pct + 2, i, f'{pct:.0f}%', va='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“ˆ Key Takeaway for Stakeholders:\")\n",
        "print(f\"  â€¢ {percentages[1]:.0f}% of predictions are within $50 of actual LTV\")\n",
        "print(f\"  â€¢ Average prediction error: ${test_metrics['mae']:.0f}\")\n",
        "print(f\"  â€¢ Model captures {test_metrics['r2']*100:.0f}% of what drives customer value\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Check\n",
        "\n",
        "Uncomment and run the asserts below to verify your regression metrics are computed correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# SELF-CHECK: Verify your regression metrics\n",
        "assert test_metrics['mae'] > 0 and test_metrics['mae'] < 500, f\"MAE should be reasonable for LTV, got {test_metrics['mae']:.1f}\"\n",
        "assert test_metrics['rmse'] >= test_metrics['mae'], \"RMSE should be >= MAE (penalizes large errors more)\"\n",
        "assert -0.5 <= test_metrics['r2'] <= 1.0, f\"RÂ² should be in [-0.5, 1.0], got {test_metrics['r2']:.3f}\"\n",
        "assert test_metrics['mae'] < baseline_mae, f\"Model MAE ({test_metrics['mae']:.1f}) should beat baseline ({baseline_mae:.1f})\"\n",
        "print(f\"âœ… Self-check passed! MAE: {test_metrics['mae']:.1f}, RMSE: {test_metrics['rmse']:.1f}, RÂ²: {test_metrics['r2']:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **MAE** tells you average error in business units - great for communication\n",
        "2. **RMSE** penalizes big errors more - use when large mistakes are costly\n",
        "3. **RÂ²** compares to baseline - always pair with error metrics\n",
        "4. **Residual plots** reveal patterns that overall metrics hide\n",
        "5. **Segment analysis** exposes where the model struggles\n",
        "\n",
        "### Next Steps\n",
        "- Explore the interactive playground to see how different error patterns affect metrics\n",
        "- Complete the quiz to test your understanding"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}