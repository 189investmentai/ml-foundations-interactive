{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 10: Feature Engineering\n",
        "\n",
        "**Goal:** Transform raw data into features models can use, while avoiding leakage.\n",
        "\n",
        "**Prerequisites:** Modules 3-4 (Linear/Logistic Regression)\n",
        "\n",
        "**Expected Runtime:** ~25 minutes\n",
        "\n",
        "**Outputs:**\n",
        "- Transformation comparisons\n",
        "- Encoding demonstrations\n",
        "- Leakage detection\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "plt.rcParams['figure.figsize'] = (12, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Generate Sample E-commerce Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n = 1000\n",
        "\n",
        "# Raw features\n",
        "df = pd.DataFrame({\n",
        "    'customer_id': range(n),\n",
        "    'tenure_days': np.random.uniform(30, 1000, n),\n",
        "    'revenue': np.random.exponential(100, n),  # Skewed!\n",
        "    'sessions': np.random.poisson(15, n),\n",
        "    'support_tickets': np.random.poisson(2, n),\n",
        "    'plan_type': np.random.choice(['Basic', 'Premium', 'Enterprise'], n, p=[0.5, 0.35, 0.15]),\n",
        "    'region': np.random.choice(['US', 'EU', 'APAC', 'LATAM'], n, p=[0.4, 0.3, 0.2, 0.1]),\n",
        "})\n",
        "\n",
        "# Generate target (churn) based on features\n",
        "churn_prob = 1 / (1 + np.exp(\n",
        "    2 - \n",
        "    0.002 * df['tenure_days'] - \n",
        "    0.005 * df['revenue'] + \n",
        "    0.3 * df['support_tickets'] - \n",
        "    0.05 * df['sessions']\n",
        "))\n",
        "df['churn'] = (np.random.random(n) < churn_prob).astype(int)\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(f\"\\nChurn rate: {df['churn'].mean():.1%}\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Numeric Transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Look at revenue distribution\n",
        "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "\n",
        "# Original\n",
        "axes[0].hist(df['revenue'], bins=30, color='#8b5cf6', edgecolor='white')\n",
        "axes[0].set_title(f\"Original\\nSkew: {df['revenue'].skew():.2f}\")\n",
        "axes[0].set_xlabel('Revenue')\n",
        "\n",
        "# Log transform\n",
        "log_revenue = np.log1p(df['revenue'])\n",
        "axes[1].hist(log_revenue, bins=30, color='#22c55e', edgecolor='white')\n",
        "axes[1].set_title(f\"Log Transform\\nSkew: {log_revenue.skew():.2f}\")\n",
        "axes[1].set_xlabel('log(Revenue + 1)')\n",
        "\n",
        "# Standardized\n",
        "std_revenue = (df['revenue'] - df['revenue'].mean()) / df['revenue'].std()\n",
        "axes[2].hist(std_revenue, bins=30, color='#0ea5e9', edgecolor='white')\n",
        "axes[2].set_title(f\"Standardized\\nMean: {std_revenue.mean():.2f}, Std: {std_revenue.std():.2f}\")\n",
        "axes[2].set_xlabel('Z-score')\n",
        "\n",
        "# Min-Max\n",
        "minmax_revenue = (df['revenue'] - df['revenue'].min()) / (df['revenue'].max() - df['revenue'].min())\n",
        "axes[3].hist(minmax_revenue, bins=30, color='#f97316', edgecolor='white')\n",
        "axes[3].set_title(f\"Min-Max\\nRange: [{minmax_revenue.min():.2f}, {minmax_revenue.max():.2f}]\")\n",
        "axes[3].set_xlabel('Scaled [0,1]')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ðŸ’¡ Key Insight: Log transform reduced skewness from {:.2f} to {:.2f}\".format(\n",
        "    df['revenue'].skew(), log_revenue.skew()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Categorical Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=== Categorical Variables ===\")\n",
        "print(f\"\\nplan_type: {df['plan_type'].nunique()} categories\")\n",
        "print(df['plan_type'].value_counts())\n",
        "\n",
        "print(f\"\\nregion: {df['region'].nunique()} categories\")\n",
        "print(df['region'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# One-Hot Encoding\n",
        "df_onehot = pd.get_dummies(df[['plan_type', 'region']], prefix=['plan', 'region'])\n",
        "print(\"=== One-Hot Encoding ===\")\n",
        "print(f\"Created {df_onehot.shape[1]} columns\")\n",
        "df_onehot.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ordinal Encoding (for plan_type with natural order)\n",
        "plan_order = {'Basic': 1, 'Premium': 2, 'Enterprise': 3}\n",
        "df['plan_ordinal'] = df['plan_type'].map(plan_order)\n",
        "\n",
        "print(\"=== Ordinal Encoding ===\")\n",
        "print(\"Mapping:\", plan_order)\n",
        "df[['plan_type', 'plan_ordinal']].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Target Encoding (mean churn rate by region)\n",
        "# âš ï¸ Must be done carefully to avoid leakage!\n",
        "\n",
        "# WRONG WAY: Calculate on ALL data (leakage!)\n",
        "region_target_mean = df.groupby('region')['churn'].mean()\n",
        "df['region_target_enc'] = df['region'].map(region_target_mean)\n",
        "\n",
        "print(\"=== Target Encoding (NAIVE - has leakage!) ===\")\n",
        "print(\"Region â†’ Mean Churn Rate:\")\n",
        "print(region_target_mean.round(3))\n",
        "print(\"\\nâš ï¸ Warning: This leaks test labels! See below for the correct approach.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CORRECT WAY: Cross-validated target encoding\n",
        "# Each fold uses only out-of-fold target values\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def cv_target_encode(df, column, target, n_folds=5):\n",
        "    \"\"\"\n",
        "    Target encode using cross-validation to avoid leakage.\n",
        "    Each row's encoding is computed WITHOUT using its own target value.\n",
        "    \"\"\"\n",
        "    result = pd.Series(index=df.index, dtype=float)\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "    \n",
        "    for train_idx, val_idx in kf.split(df):\n",
        "        # Calculate target mean on training fold only\n",
        "        train_means = df.iloc[train_idx].groupby(column)[target].mean()\n",
        "        global_mean = df.iloc[train_idx][target].mean()\n",
        "        \n",
        "        # Apply to validation fold\n",
        "        result.iloc[val_idx] = df.iloc[val_idx][column].map(train_means).fillna(global_mean)\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Apply CV target encoding\n",
        "df['region_target_enc_cv'] = cv_target_encode(df, 'region', 'churn')\n",
        "\n",
        "print(\"=== Target Encoding (CORRECT - CV-based) ===\")\n",
        "print(\"Region â†’ Mean Churn Rate (via CV):\")\n",
        "print(df.groupby('region')['region_target_enc_cv'].mean().round(3))\n",
        "print(\"\\nâœ… Each row's encoding was computed without seeing its own target!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Feature Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create engineered features\n",
        "df['revenue_per_session'] = df['revenue'] / (df['sessions'] + 1)\n",
        "df['ticket_rate'] = df['support_tickets'] / (df['tenure_days'] / 30)  # tickets per month\n",
        "df['log_revenue'] = np.log1p(df['revenue'])\n",
        "df['is_high_value'] = (df['revenue'] > df['revenue'].quantile(0.75)).astype(int)\n",
        "df['tenure_months'] = df['tenure_days'] / 30\n",
        "\n",
        "print(\"=== Engineered Features ===\")\n",
        "print(df[['revenue_per_session', 'ticket_rate', 'log_revenue', 'is_high_value', 'tenure_months']].describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Impact on Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compare raw vs engineered features\n",
        "y = df['churn']\n",
        "\n",
        "# Raw numeric features\n",
        "X_raw = df[['tenure_days', 'revenue', 'sessions', 'support_tickets']]\n",
        "\n",
        "# Engineered features\n",
        "X_eng = df[['tenure_months', 'log_revenue', 'sessions', 'support_tickets', \n",
        "            'revenue_per_session', 'ticket_rate', 'plan_ordinal', 'region_target_enc']]\n",
        "\n",
        "# Split\n",
        "X_raw_train, X_raw_test, X_eng_train, X_eng_test, y_train, y_test = train_test_split(\n",
        "    X_raw, X_eng, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale for logistic regression\n",
        "scaler_raw = StandardScaler()\n",
        "scaler_eng = StandardScaler()\n",
        "\n",
        "X_raw_train_scaled = scaler_raw.fit_transform(X_raw_train)\n",
        "X_raw_test_scaled = scaler_raw.transform(X_raw_test)\n",
        "\n",
        "X_eng_train_scaled = scaler_eng.fit_transform(X_eng_train)\n",
        "X_eng_test_scaled = scaler_eng.transform(X_eng_test)\n",
        "\n",
        "# Train models\n",
        "lr_raw = LogisticRegression().fit(X_raw_train_scaled, y_train)\n",
        "lr_eng = LogisticRegression().fit(X_eng_train_scaled, y_train)\n",
        "\n",
        "rf_raw = RandomForestClassifier(n_estimators=100, random_state=42).fit(X_raw_train, y_train)\n",
        "rf_eng = RandomForestClassifier(n_estimators=100, random_state=42).fit(X_eng_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "results = pd.DataFrame({\n",
        "    'Model': ['Logistic (Raw)', 'Logistic (Engineered)', 'Random Forest (Raw)', 'Random Forest (Engineered)'],\n",
        "    'Train AUC': [\n",
        "        roc_auc_score(y_train, lr_raw.predict_proba(X_raw_train_scaled)[:, 1]),\n",
        "        roc_auc_score(y_train, lr_eng.predict_proba(X_eng_train_scaled)[:, 1]),\n",
        "        roc_auc_score(y_train, rf_raw.predict_proba(X_raw_train)[:, 1]),\n",
        "        roc_auc_score(y_train, rf_eng.predict_proba(X_eng_train)[:, 1])\n",
        "    ],\n",
        "    'Test AUC': [\n",
        "        roc_auc_score(y_test, lr_raw.predict_proba(X_raw_test_scaled)[:, 1]),\n",
        "        roc_auc_score(y_test, lr_eng.predict_proba(X_eng_test_scaled)[:, 1]),\n",
        "        roc_auc_score(y_test, rf_raw.predict_proba(X_raw_test)[:, 1]),\n",
        "        roc_auc_score(y_test, rf_eng.predict_proba(X_eng_test)[:, 1])\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"=== Model Comparison ===\")\n",
        "print(results.to_string(index=False))\n",
        "\n",
        "print(\"\\nðŸ’¡ Insight: Feature engineering often helps linear models more than tree models.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Data Leakage Demo\n",
        "\n",
        "Let's see what happens when we accidentally include future information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a \"leaky\" feature - future activity that correlates with churn\n",
        "# In reality, this would be activity AFTER the prediction point\n",
        "df['future_activity'] = np.where(\n",
        "    df['churn'] == 1, \n",
        "    np.random.normal(2, 1, n),  # Churners have low future activity\n",
        "    np.random.normal(10, 2, n)  # Non-churners have high future activity\n",
        ")\n",
        "\n",
        "# Features with leakage\n",
        "X_leaky = df[['tenure_months', 'log_revenue', 'sessions', 'support_tickets', 'future_activity']]\n",
        "\n",
        "# Split (AFTER creating leaky feature - the damage is done)\n",
        "X_leaky_train, X_leaky_test, y_train, y_test = train_test_split(\n",
        "    X_leaky, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale\n",
        "scaler_leaky = StandardScaler()\n",
        "X_leaky_train_scaled = scaler_leaky.fit_transform(X_leaky_train)\n",
        "X_leaky_test_scaled = scaler_leaky.transform(X_leaky_test)\n",
        "\n",
        "# Train\n",
        "lr_leaky = LogisticRegression().fit(X_leaky_train_scaled, y_train)\n",
        "\n",
        "# Evaluate\n",
        "train_auc_leaky = roc_auc_score(y_train, lr_leaky.predict_proba(X_leaky_train_scaled)[:, 1])\n",
        "test_auc_leaky = roc_auc_score(y_test, lr_leaky.predict_proba(X_leaky_test_scaled)[:, 1])\n",
        "\n",
        "print(\"=== âš ï¸ LEAKAGE DEMO ===\")\n",
        "print(f\"\\nWith 'future_activity' feature (LEAKY):\")\n",
        "print(f\"  Train AUC: {train_auc_leaky:.3f}\")\n",
        "print(f\"  Test AUC:  {test_auc_leaky:.3f}\")\n",
        "print(f\"\\nðŸš¨ Red Flag: Suspiciously high AUC!\")\n",
        "print(\"   The model learned a shortcut using future information.\")\n",
        "print(\"   In production, this feature wouldn't exist at prediction time.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: TODO - Correct Scaling Pipeline\n",
        "\n",
        "A common mistake is fitting the scaler on all data. Let's compare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Compare correct vs incorrect scaling\n",
        "\n",
        "X_simple = df[['tenure_days', 'revenue', 'sessions', 'support_tickets']]\n",
        "\n",
        "# WRONG: Fit scaler on ALL data before split\n",
        "scaler_wrong = StandardScaler()\n",
        "X_scaled_wrong = scaler_wrong.fit_transform(X_simple)  # Fitted on everything\n",
        "\n",
        "X_train_wrong, X_test_wrong, y_train_w, y_test_w = train_test_split(\n",
        "    X_scaled_wrong, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# RIGHT: Split first, then fit scaler only on train\n",
        "X_train_right, X_test_right, y_train_r, y_test_r = train_test_split(\n",
        "    X_simple, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "scaler_right = StandardScaler()\n",
        "X_train_right_scaled = scaler_right.fit_transform(X_train_right)  # Fit only on train\n",
        "X_test_right_scaled = scaler_right.transform(X_test_right)  # Transform test\n",
        "\n",
        "# Train models\n",
        "lr_wrong = LogisticRegression().fit(X_train_wrong, y_train_w)\n",
        "lr_right = LogisticRegression().fit(X_train_right_scaled, y_train_r)\n",
        "\n",
        "print(\"=== Scaling Pipeline Comparison ===\")\n",
        "print(f\"\\nWRONG (fit on all): Test AUC = {roc_auc_score(y_test_w, lr_wrong.predict_proba(X_test_wrong)[:, 1]):.3f}\")\n",
        "print(f\"RIGHT (fit on train): Test AUC = {roc_auc_score(y_test_r, lr_right.predict_proba(X_test_right_scaled)[:, 1]):.3f}\")\n",
        "print(\"\\nðŸ’¡ In this case the difference is small, but on smaller datasets it matters more.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Check\n",
        "\n",
        "Uncomment and run the asserts below to verify your feature engineering pipeline is correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# SELF-CHECK: Verify your feature engineering\n",
        "assert X_eng_train.shape[1] > X_raw_train.shape[1], \"Engineered features should add columns\"\n",
        "assert 'results' in dir() and len(results) >= 4, \"Model comparison results should exist\"\n",
        "assert test_auc_leaky > 0.85, \"Leaky model should have suspiciously high AUC\"\n",
        "print(f\"âœ… Self-check passed! Raw features: {X_raw_train.shape[1]}, Engineered: {X_eng_train.shape[1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Stakeholder Summary\n",
        "\n",
        "### TODO: Write a 3-bullet summary (~100 words) for the PM\n",
        "\n",
        "Template:\n",
        "â€¢ **What we did:** Transformed [features] using [techniques] to help the model capture [patterns].\n",
        "â€¢ **Leakage avoided:** We ensured no future information or target labels leaked into features by [method].\n",
        "â€¢ **Impact:** Feature engineering improved AUC from ____ to ____ for [model type]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Summary:\n",
        "\n",
        "*Write your explanation here...*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Transform skewed data** with log â€” helps linear models capture patterns\n",
        "2. **Scale features** â€” essential for linear models, optional for trees\n",
        "3. **Encode categoricals** thoughtfully â€” one-hot for low cardinality, target encoding for high\n",
        "4. **Avoid leakage** â€” only use information available at prediction time\n",
        "5. **Fit transforms on train only** â€” apply to test without refitting\n",
        "\n",
        "### Next Steps\n",
        "- Explore the interactive playground for visual transformations\n",
        "- Complete the quiz to test your understanding"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}