{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10: Feature Engineering\n",
    "\n",
    "**Goal:** Transform raw data into features models can use, while avoiding leakage.\n",
    "\n",
    "**Prerequisites:** Modules 3-4 (Linear/Logistic Regression)\n",
    "\n",
    "**Expected Runtime:** ~25 minutes\n",
    "\n",
    "**Outputs:**\n",
    "- Transformation comparisons\n",
    "- Encoding demonstrations\n",
    "- Leakage detection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams['figure.figsize'] = (12, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Generate Sample E-commerce Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "# Raw features\n",
    "df = pd.DataFrame({\n",
    "    'customer_id': range(n),\n",
    "    'tenure_days': np.random.uniform(30, 1000, n),\n",
    "    'revenue': np.random.exponential(100, n),  # Skewed!\n",
    "    'sessions': np.random.poisson(15, n),\n",
    "    'support_tickets': np.random.poisson(2, n),\n",
    "    'plan_type': np.random.choice(['Basic', 'Premium', 'Enterprise'], n, p=[0.5, 0.35, 0.15]),\n",
    "    'region': np.random.choice(['US', 'EU', 'APAC', 'LATAM'], n, p=[0.4, 0.3, 0.2, 0.1]),\n",
    "})\n",
    "\n",
    "# Generate target (churn) based on features\n",
    "churn_prob = 1 / (1 + np.exp(\n",
    "    2 - \n",
    "    0.002 * df['tenure_days'] - \n",
    "    0.005 * df['revenue'] + \n",
    "    0.3 * df['support_tickets'] - \n",
    "    0.05 * df['sessions']\n",
    "))\n",
    "df['churn'] = (np.random.random(n) < churn_prob).astype(int)\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(f\"\\nChurn rate: {df['churn'].mean():.1%}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Numeric Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at revenue distribution\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# Original\n",
    "axes[0].hist(df['revenue'], bins=30, color='#8b5cf6', edgecolor='white')\n",
    "axes[0].set_title(f\"Original\\nSkew: {df['revenue'].skew():.2f}\")\n",
    "axes[0].set_xlabel('Revenue')\n",
    "\n",
    "# Log transform\n",
    "log_revenue = np.log1p(df['revenue'])\n",
    "axes[1].hist(log_revenue, bins=30, color='#22c55e', edgecolor='white')\n",
    "axes[1].set_title(f\"Log Transform\\nSkew: {log_revenue.skew():.2f}\")\n",
    "axes[1].set_xlabel('log(Revenue + 1)')\n",
    "\n",
    "# Standardized\n",
    "std_revenue = (df['revenue'] - df['revenue'].mean()) / df['revenue'].std()\n",
    "axes[2].hist(std_revenue, bins=30, color='#0ea5e9', edgecolor='white')\n",
    "axes[2].set_title(f\"Standardized\\nMean: {std_revenue.mean():.2f}, Std: {std_revenue.std():.2f}\")\n",
    "axes[2].set_xlabel('Z-score')\n",
    "\n",
    "# Min-Max\n",
    "minmax_revenue = (df['revenue'] - df['revenue'].min()) / (df['revenue'].max() - df['revenue'].min())\n",
    "axes[3].hist(minmax_revenue, bins=30, color='#f97316', edgecolor='white')\n",
    "axes[3].set_title(f\"Min-Max\\nRange: [{minmax_revenue.min():.2f}, {minmax_revenue.max():.2f}]\")\n",
    "axes[3].set_xlabel('Scaled [0,1]')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Key Insight: Log transform reduced skewness from {:.2f} to {:.2f}\".format(\n",
    "    df['revenue'].skew(), log_revenue.skew()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Categorical Variables ===\")\n",
    "print(f\"\\nplan_type: {df['plan_type'].nunique()} categories\")\n",
    "print(df['plan_type'].value_counts())\n",
    "\n",
    "print(f\"\\nregion: {df['region'].nunique()} categories\")\n",
    "print(df['region'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding\n",
    "df_onehot = pd.get_dummies(df[['plan_type', 'region']], prefix=['plan', 'region'])\n",
    "print(\"=== One-Hot Encoding ===\")\n",
    "print(f\"Created {df_onehot.shape[1]} columns\")\n",
    "df_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal Encoding (for plan_type with natural order)\n",
    "plan_order = {'Basic': 1, 'Premium': 2, 'Enterprise': 3}\n",
    "df['plan_ordinal'] = df['plan_type'].map(plan_order)\n",
    "\n",
    "print(\"=== Ordinal Encoding ===\")\n",
    "print(\"Mapping:\", plan_order)\n",
    "df[['plan_type', 'plan_ordinal']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Encoding (mean churn rate by region)\n",
    "# ‚ö†Ô∏è Must be done carefully to avoid leakage!\n",
    "\n",
    "# Calculate target mean per region on training data only\n",
    "region_target_mean = df.groupby('region')['churn'].mean()\n",
    "df['region_target_enc'] = df['region'].map(region_target_mean)\n",
    "\n",
    "print(\"=== Target Encoding ===\")\n",
    "print(\"Region ‚Üí Mean Churn Rate:\")\n",
    "print(region_target_mean.round(3))\n",
    "print(\"\\n‚ö†Ô∏è Warning: In practice, use cross-validation encoding to avoid leakage!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engineered features\n",
    "df['revenue_per_session'] = df['revenue'] / (df['sessions'] + 1)\n",
    "df['ticket_rate'] = df['support_tickets'] / (df['tenure_days'] / 30)  # tickets per month\n",
    "df['log_revenue'] = np.log1p(df['revenue'])\n",
    "df['is_high_value'] = (df['revenue'] > df['revenue'].quantile(0.75)).astype(int)\n",
    "df['tenure_months'] = df['tenure_days'] / 30\n",
    "\n",
    "print(\"=== Engineered Features ===\")\n",
    "print(df[['revenue_per_session', 'ticket_rate', 'log_revenue', 'is_high_value', 'tenure_months']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Impact on Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare raw vs engineered features\n",
    "y = df['churn']\n",
    "\n",
    "# Raw numeric features\n",
    "X_raw = df[['tenure_days', 'revenue', 'sessions', 'support_tickets']]\n",
    "\n",
    "# Engineered features\n",
    "X_eng = df[['tenure_months', 'log_revenue', 'sessions', 'support_tickets', \n",
    "            'revenue_per_session', 'ticket_rate', 'plan_ordinal', 'region_target_enc']]\n",
    "\n",
    "# Split\n",
    "X_raw_train, X_raw_test, X_eng_train, X_eng_test, y_train, y_test = train_test_split(\n",
    "    X_raw, X_eng, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale for logistic regression\n",
    "scaler_raw = StandardScaler()\n",
    "scaler_eng = StandardScaler()\n",
    "\n",
    "X_raw_train_scaled = scaler_raw.fit_transform(X_raw_train)\n",
    "X_raw_test_scaled = scaler_raw.transform(X_raw_test)\n",
    "\n",
    "X_eng_train_scaled = scaler_eng.fit_transform(X_eng_train)\n",
    "X_eng_test_scaled = scaler_eng.transform(X_eng_test)\n",
    "\n",
    "# Train models\n",
    "lr_raw = LogisticRegression().fit(X_raw_train_scaled, y_train)\n",
    "lr_eng = LogisticRegression().fit(X_eng_train_scaled, y_train)\n",
    "\n",
    "rf_raw = RandomForestClassifier(n_estimators=100, random_state=42).fit(X_raw_train, y_train)\n",
    "rf_eng = RandomForestClassifier(n_estimators=100, random_state=42).fit(X_eng_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic (Raw)', 'Logistic (Engineered)', 'Random Forest (Raw)', 'Random Forest (Engineered)'],\n",
    "    'Train AUC': [\n",
    "        roc_auc_score(y_train, lr_raw.predict_proba(X_raw_train_scaled)[:, 1]),\n",
    "        roc_auc_score(y_train, lr_eng.predict_proba(X_eng_train_scaled)[:, 1]),\n",
    "        roc_auc_score(y_train, rf_raw.predict_proba(X_raw_train)[:, 1]),\n",
    "        roc_auc_score(y_train, rf_eng.predict_proba(X_eng_train)[:, 1])\n",
    "    ],\n",
    "    'Test AUC': [\n",
    "        roc_auc_score(y_test, lr_raw.predict_proba(X_raw_test_scaled)[:, 1]),\n",
    "        roc_auc_score(y_test, lr_eng.predict_proba(X_eng_test_scaled)[:, 1]),\n",
    "        roc_auc_score(y_test, rf_raw.predict_proba(X_raw_test)[:, 1]),\n",
    "        roc_auc_score(y_test, rf_eng.predict_proba(X_eng_test)[:, 1])\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=== Model Comparison ===\")\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "print(\"\\nüí° Insight: Feature engineering often helps linear models more than tree models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Data Leakage Demo\n",
    "\n",
    "Let's see what happens when we accidentally include future information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"leaky\" feature - future activity that correlates with churn\n",
    "# In reality, this would be activity AFTER the prediction point\n",
    "df['future_activity'] = np.where(\n",
    "    df['churn'] == 1, \n",
    "    np.random.normal(2, 1, n),  # Churners have low future activity\n",
    "    np.random.normal(10, 2, n)  # Non-churners have high future activity\n",
    ")\n",
    "\n",
    "# Features with leakage\n",
    "X_leaky = df[['tenure_months', 'log_revenue', 'sessions', 'support_tickets', 'future_activity']]\n",
    "\n",
    "# Split (AFTER creating leaky feature - the damage is done)\n",
    "X_leaky_train, X_leaky_test, y_train, y_test = train_test_split(\n",
    "    X_leaky, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale\n",
    "scaler_leaky = StandardScaler()\n",
    "X_leaky_train_scaled = scaler_leaky.fit_transform(X_leaky_train)\n",
    "X_leaky_test_scaled = scaler_leaky.transform(X_leaky_test)\n",
    "\n",
    "# Train\n",
    "lr_leaky = LogisticRegression().fit(X_leaky_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_auc_leaky = roc_auc_score(y_train, lr_leaky.predict_proba(X_leaky_train_scaled)[:, 1])\n",
    "test_auc_leaky = roc_auc_score(y_test, lr_leaky.predict_proba(X_leaky_test_scaled)[:, 1])\n",
    "\n",
    "print(\"=== ‚ö†Ô∏è LEAKAGE DEMO ===\")\n",
    "print(f\"\\nWith 'future_activity' feature (LEAKY):\")\n",
    "print(f\"  Train AUC: {train_auc_leaky:.3f}\")\n",
    "print(f\"  Test AUC:  {test_auc_leaky:.3f}\")\n",
    "print(f\"\\nüö® Red Flag: Suspiciously high AUC!\")\n",
    "print(\"   The model learned a shortcut using future information.\")\n",
    "print(\"   In production, this feature wouldn't exist at prediction time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: TODO - Correct Scaling Pipeline\n",
    "\n",
    "A common mistake is fitting the scaler on all data. Let's compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare correct vs incorrect scaling\n",
    "\n",
    "X_simple = df[['tenure_days', 'revenue', 'sessions', 'support_tickets']]\n",
    "\n",
    "# WRONG: Fit scaler on ALL data before split\n",
    "scaler_wrong = StandardScaler()\n",
    "X_scaled_wrong = scaler_wrong.fit_transform(X_simple)  # Fitted on everything\n",
    "\n",
    "X_train_wrong, X_test_wrong, y_train_w, y_test_w = train_test_split(\n",
    "    X_scaled_wrong, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# RIGHT: Split first, then fit scaler only on train\n",
    "X_train_right, X_test_right, y_train_r, y_test_r = train_test_split(\n",
    "    X_simple, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "scaler_right = StandardScaler()\n",
    "X_train_right_scaled = scaler_right.fit_transform(X_train_right)  # Fit only on train\n",
    "X_test_right_scaled = scaler_right.transform(X_test_right)  # Transform test\n",
    "\n",
    "# Train models\n",
    "lr_wrong = LogisticRegression().fit(X_train_wrong, y_train_w)\n",
    "lr_right = LogisticRegression().fit(X_train_right_scaled, y_train_r)\n",
    "\n",
    "print(\"=== Scaling Pipeline Comparison ===\")\n",
    "print(f\"\\nWRONG (fit on all): Test AUC = {roc_auc_score(y_test_w, lr_wrong.predict_proba(X_test_wrong)[:, 1]):.3f}\")\n",
    "print(f\"RIGHT (fit on train): Test AUC = {roc_auc_score(y_test_r, lr_right.predict_proba(X_test_right_scaled)[:, 1]):.3f}\")\n",
    "print(\"\\nüí° In this case the difference is small, but on smaller datasets it matters more.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: TODO - Stakeholder Summary\n",
    "\n",
    "Write a brief explanation for a PM about:\n",
    "1. Why you transformed certain features\n",
    "2. What leakage is and how you avoided it\n",
    "3. How feature engineering improved the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Summary:\n",
    "\n",
    "*Write your explanation here...*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Transform skewed data** with log ‚Äî helps linear models capture patterns\n",
    "2. **Scale features** ‚Äî essential for linear models, optional for trees\n",
    "3. **Encode categoricals** thoughtfully ‚Äî one-hot for low cardinality, target encoding for high\n",
    "4. **Avoid leakage** ‚Äî only use information available at prediction time\n",
    "5. **Fit transforms on train only** ‚Äî apply to test without refitting\n",
    "\n",
    "### Next Steps\n",
    "- Explore the interactive playground for visual transformations\n",
    "- Complete the quiz to test your understanding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
