{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7: Optimization and Gradient Descent\n",
    "\n",
    "**Goal:** Understand how gradient descent works, experiment with learning rates and optimizers.\n",
    "\n",
    "**Prerequisites:** Module 3 (Linear Regression), Module 4 (Logistic Regression)\n",
    "\n",
    "**Expected Runtime:** ~20 minutes\n",
    "\n",
    "**Outputs:**\n",
    "- Visualizations of gradient descent paths\n",
    "- Comparison of optimizers on different loss surfaces\n",
    "- Understanding of learning rate effects\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plot style\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Loss Surface\n",
    "\n",
    "Gradient descent tries to find the lowest point on a \"loss surface\" — a landscape where the height represents how bad the model's predictions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple quadratic loss surface (bowl shape)\n",
    "def quadratic_loss(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "def quadratic_gradient(x, y):\n",
    "    return np.array([2*x, 2*y])\n",
    "\n",
    "# Visualize the loss surface\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = quadratic_loss(X, Y)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "# 3D view\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(X, Y, Z, cmap=cm.viridis, alpha=0.8)\n",
    "ax1.set_xlabel('θ₁')\n",
    "ax1.set_ylabel('θ₂')\n",
    "ax1.set_zlabel('Loss')\n",
    "ax1.set_title('3D View: Quadratic Bowl')\n",
    "\n",
    "# Contour view\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(X, Y, Z, levels=15, cmap='viridis')\n",
    "ax2.scatter([0], [0], color='green', s=100, marker='*', label='Minimum')\n",
    "ax2.set_xlabel('θ₁')\n",
    "ax2.set_ylabel('θ₂')\n",
    "ax2.set_title('Contour View (Bird\\'s Eye)')\n",
    "ax2.legend()\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The goal of gradient descent: Start somewhere on this surface and find the minimum (green star).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing Gradient Descent from Scratch\n",
    "\n",
    "Let's implement vanilla gradient descent to see exactly what happens at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(start, gradient_fn, loss_fn, lr=0.1, n_steps=50):\n",
    "    \"\"\"\n",
    "    Vanilla gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        start: Starting position [x, y]\n",
    "        gradient_fn: Function that returns gradient at a point\n",
    "        loss_fn: Function that returns loss at a point\n",
    "        lr: Learning rate\n",
    "        n_steps: Number of steps to take\n",
    "    \n",
    "    Returns:\n",
    "        path: List of positions visited\n",
    "        losses: List of loss values\n",
    "    \"\"\"\n",
    "    position = np.array(start, dtype=float)\n",
    "    path = [position.copy()]\n",
    "    losses = [loss_fn(position[0], position[1])]\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        grad = gradient_fn(position[0], position[1])\n",
    "        position = position - lr * grad  # The core update rule!\n",
    "        path.append(position.copy())\n",
    "        losses.append(loss_fn(position[0], position[1]))\n",
    "    \n",
    "    return np.array(path), np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gradient descent with a reasonable learning rate\n",
    "start = [-2.5, 2.5]\n",
    "path, losses = gradient_descent(start, quadratic_gradient, quadratic_loss, lr=0.1, n_steps=30)\n",
    "\n",
    "# Visualize the path\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Path on contour plot\n",
    "ax1 = axes[0]\n",
    "ax1.contour(X, Y, Z, levels=15, cmap='viridis', alpha=0.7)\n",
    "ax1.plot(path[:, 0], path[:, 1], 'o-', color='red', markersize=4, linewidth=1.5, label='GD Path')\n",
    "ax1.scatter([start[0]], [start[1]], color='red', s=100, marker='s', zorder=5, label='Start')\n",
    "ax1.scatter([0], [0], color='green', s=100, marker='*', zorder=5, label='Minimum')\n",
    "ax1.set_xlabel('θ₁')\n",
    "ax1.set_ylabel('θ₂')\n",
    "ax1.set_title(f'Gradient Descent Path (lr={0.1})')\n",
    "ax1.legend()\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "# Loss over time\n",
    "ax2 = axes[1]\n",
    "ax2.plot(losses, 'b-', linewidth=2)\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Loss Over Time')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Starting loss: {losses[0]:.4f}\")\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"Final position: ({path[-1, 0]:.4f}, {path[-1, 1]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Learning Rate Effect\n",
    "\n",
    "Learning rate is the most important hyperparameter in gradient descent. Let's see what happens with different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with different learning rates\n",
    "# Try: 0.01, 0.1, 0.5, 0.95, 1.1\n",
    "# Predict what will happen before running!\n",
    "\n",
    "learning_rates = [0.01, 0.1, 0.5, 0.95, 1.1]\n",
    "colors = ['blue', 'green', 'orange', 'red', 'purple']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Path visualization\n",
    "ax1 = axes[0]\n",
    "ax1.contour(X, Y, Z, levels=15, cmap='viridis', alpha=0.5)\n",
    "\n",
    "# Loss curves\n",
    "ax2 = axes[1]\n",
    "\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    path, losses = gradient_descent(start, quadratic_gradient, quadratic_loss, lr=lr, n_steps=30)\n",
    "    \n",
    "    # Clip for visualization (some might explode)\n",
    "    path = np.clip(path, -5, 5)\n",
    "    losses = np.clip(losses, 0, 50)\n",
    "    \n",
    "    ax1.plot(path[:, 0], path[:, 1], 'o-', color=color, markersize=3, \n",
    "             linewidth=1, label=f'lr={lr}', alpha=0.7)\n",
    "    ax2.plot(losses, color=color, linewidth=2, label=f'lr={lr}')\n",
    "\n",
    "ax1.scatter([0], [0], color='green', s=100, marker='*', zorder=5)\n",
    "ax1.set_xlabel('θ₁')\n",
    "ax1.set_ylabel('θ₂')\n",
    "ax1.set_title('Paths with Different Learning Rates')\n",
    "ax1.legend()\n",
    "ax1.set_xlim(-4, 4)\n",
    "ax1.set_ylim(-4, 4)\n",
    "\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Loss Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "**lr = 0.01:** Too slow — takes many steps to make progress  \n",
    "**lr = 0.1:** Just right — smooth convergence  \n",
    "**lr = 0.5:** Getting aggressive — some oscillation  \n",
    "**lr = 0.95:** Barely stable — oscillating around minimum  \n",
    "**lr = 1.1:** Diverging! — overshooting and exploding  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Ravine Problem\n",
    "\n",
    "Real loss surfaces aren't nice bowls. Let's see what happens with a narrow \"ravine\" shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ravine: Much steeper in y-direction than x-direction\n",
    "def ravine_loss(x, y):\n",
    "    return 0.5 * x**2 + 10 * y**2\n",
    "\n",
    "def ravine_gradient(x, y):\n",
    "    return np.array([x, 20*y])\n",
    "\n",
    "# Visualize the ravine\n",
    "Z_ravine = ravine_loss(X, Y)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contour(X, Y, Z_ravine, levels=20, cmap='viridis')\n",
    "plt.colorbar(label='Loss')\n",
    "plt.xlabel('θ₁')\n",
    "plt.ylabel('θ₂')\n",
    "plt.title('Ravine Loss Surface: Steep in Y, Flat in X')\n",
    "plt.scatter([0], [0], color='green', s=100, marker='*', label='Minimum')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run vanilla SGD on the ravine\n",
    "start_ravine = [-2.5, 0.5]\n",
    "path_sgd, losses_sgd = gradient_descent(start_ravine, ravine_gradient, ravine_loss, lr=0.05, n_steps=50)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contour(X, Y, Z_ravine, levels=20, cmap='viridis', alpha=0.5)\n",
    "plt.plot(path_sgd[:, 0], path_sgd[:, 1], 'o-', color='red', markersize=4, linewidth=1, label='SGD Path')\n",
    "plt.scatter([start_ravine[0]], [start_ravine[1]], color='red', s=100, marker='s', zorder=5)\n",
    "plt.scatter([0], [0], color='green', s=100, marker='*', zorder=5)\n",
    "plt.xlabel('θ₁')\n",
    "plt.ylabel('θ₂')\n",
    "plt.title('SGD on Ravine: Zigzag Problem!')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice the zigzag pattern! SGD oscillates across the narrow valley.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Momentum to the Rescue\n",
    "\n",
    "Momentum helps smooth out the oscillations by accumulating velocity from past gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_momentum(start, gradient_fn, loss_fn, lr=0.05, momentum=0.9, n_steps=50):\n",
    "    \"\"\"\n",
    "    SGD with momentum.\n",
    "    \"\"\"\n",
    "    position = np.array(start, dtype=float)\n",
    "    velocity = np.zeros(2)\n",
    "    path = [position.copy()]\n",
    "    losses = [loss_fn(position[0], position[1])]\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        grad = gradient_fn(position[0], position[1])\n",
    "        velocity = momentum * velocity - lr * grad  # Accumulate velocity\n",
    "        position = position + velocity\n",
    "        path.append(position.copy())\n",
    "        losses.append(loss_fn(position[0], position[1]))\n",
    "    \n",
    "    return np.array(path), np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SGD vs Momentum on the ravine\n",
    "path_momentum, losses_momentum = gradient_descent_momentum(start_ravine, ravine_gradient, ravine_loss, \n",
    "                                                           lr=0.05, momentum=0.9, n_steps=50)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Paths\n",
    "ax1 = axes[0]\n",
    "ax1.contour(X, Y, Z_ravine, levels=20, cmap='viridis', alpha=0.5)\n",
    "ax1.plot(path_sgd[:, 0], path_sgd[:, 1], 'o-', color='red', markersize=3, \n",
    "         linewidth=1, label='SGD', alpha=0.7)\n",
    "ax1.plot(path_momentum[:, 0], path_momentum[:, 1], 'o-', color='blue', markersize=3, \n",
    "         linewidth=1, label='SGD + Momentum')\n",
    "ax1.scatter([0], [0], color='green', s=100, marker='*', zorder=5)\n",
    "ax1.set_xlabel('θ₁')\n",
    "ax1.set_ylabel('θ₂')\n",
    "ax1.set_title('SGD vs Momentum on Ravine')\n",
    "ax1.legend()\n",
    "\n",
    "# Loss curves\n",
    "ax2 = axes[1]\n",
    "ax2.plot(losses_sgd, 'r-', linewidth=2, label='SGD')\n",
    "ax2.plot(losses_momentum, 'b-', linewidth=2, label='SGD + Momentum')\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Loss Over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"SGD final loss: {losses_sgd[-1]:.6f}\")\n",
    "print(f\"Momentum final loss: {losses_momentum[-1]:.6f}\")\n",
    "print(f\"\\nMomentum converges ~{losses_sgd[-1]/losses_momentum[-1]:.1f}x closer to the minimum!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Implementing Adam\n",
    "\n",
    "Adam combines momentum with adaptive learning rates per parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(start, gradient_fn, loss_fn, lr=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8, n_steps=50):\n",
    "    \"\"\"\n",
    "    Adam optimizer.\n",
    "    \"\"\"\n",
    "    position = np.array(start, dtype=float)\n",
    "    m = np.zeros(2)  # First moment (momentum)\n",
    "    v = np.zeros(2)  # Second moment (adaptive)\n",
    "    path = [position.copy()]\n",
    "    losses = [loss_fn(position[0], position[1])]\n",
    "    \n",
    "    for t in range(1, n_steps + 1):\n",
    "        grad = gradient_fn(position[0], position[1])\n",
    "        \n",
    "        # Update biased first moment estimate\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        # Update biased second moment estimate\n",
    "        v = beta2 * v + (1 - beta2) * grad**2\n",
    "        \n",
    "        # Bias correction\n",
    "        m_hat = m / (1 - beta1**t)\n",
    "        v_hat = v / (1 - beta2**t)\n",
    "        \n",
    "        # Update position\n",
    "        position = position - lr * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "        \n",
    "        path.append(position.copy())\n",
    "        losses.append(loss_fn(position[0], position[1]))\n",
    "    \n",
    "    return np.array(path), np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three optimizers on the ravine\n",
    "path_adam, losses_adam = adam(start_ravine, ravine_gradient, ravine_loss, lr=0.3, n_steps=50)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Paths\n",
    "ax1 = axes[0]\n",
    "ax1.contour(X, Y, Z_ravine, levels=20, cmap='viridis', alpha=0.5)\n",
    "ax1.plot(path_sgd[:, 0], path_sgd[:, 1], 'o-', color='red', markersize=3, \n",
    "         linewidth=1, label='SGD', alpha=0.6)\n",
    "ax1.plot(path_momentum[:, 0], path_momentum[:, 1], 'o-', color='blue', markersize=3, \n",
    "         linewidth=1, label='Momentum', alpha=0.6)\n",
    "ax1.plot(path_adam[:, 0], path_adam[:, 1], 'o-', color='purple', markersize=3, \n",
    "         linewidth=1.5, label='Adam')\n",
    "ax1.scatter([0], [0], color='green', s=100, marker='*', zorder=5)\n",
    "ax1.set_xlabel('θ₁')\n",
    "ax1.set_ylabel('θ₂')\n",
    "ax1.set_title('SGD vs Momentum vs Adam')\n",
    "ax1.legend()\n",
    "\n",
    "# Loss curves\n",
    "ax2 = axes[1]\n",
    "ax2.plot(losses_sgd, 'r-', linewidth=2, label='SGD')\n",
    "ax2.plot(losses_momentum, 'b-', linewidth=2, label='Momentum')\n",
    "ax2.plot(losses_adam, 'purple', linewidth=2, label='Adam')\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Loss Over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Final losses:\")\n",
    "print(f\"  SGD:      {losses_sgd[-1]:.6f}\")\n",
    "print(f\"  Momentum: {losses_momentum[-1]:.6f}\")\n",
    "print(f\"  Adam:     {losses_adam[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Regularization Effect\n",
    "\n",
    "Regularization adds a penalty for large weights, effectively changing the loss surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original loss vs L2 regularized loss\n",
    "lambda_l2 = 0.5\n",
    "\n",
    "def l2_regularized_loss(x, y):\n",
    "    return quadratic_loss(x, y) + lambda_l2 * (x**2 + y**2)\n",
    "\n",
    "# Visualize the effect\n",
    "Z_l2 = l2_regularized_loss(X, Y)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "c1 = ax1.contour(X, Y, Z, levels=15, cmap='viridis')\n",
    "ax1.set_title('Original Loss Surface')\n",
    "ax1.set_xlabel('θ₁')\n",
    "ax1.set_ylabel('θ₂')\n",
    "\n",
    "ax2 = axes[1]\n",
    "c2 = ax2.contour(X, Y, Z_l2, levels=15, cmap='viridis')\n",
    "ax2.set_title(f'With L2 Regularization (λ={lambda_l2})')\n",
    "ax2.set_xlabel('θ₁')\n",
    "ax2.set_ylabel('θ₂')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"L2 regularization makes the bowl steeper, encouraging smaller weights.\")\n",
    "print(\"This helps prevent overfitting by penalizing extreme parameter values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: TODO - Find the Optimal Learning Rate\n",
    "\n",
    "A common technique is to try several learning rates and pick the one that converges fastest without diverging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this experiment\n",
    "# Test learning rates: [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "# For each, record: final loss, number of steps to reach loss < 0.01, whether it diverged\n",
    "\n",
    "test_lrs = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "results = []\n",
    "\n",
    "for lr in test_lrs:\n",
    "    path, losses = gradient_descent(start, quadratic_gradient, quadratic_loss, lr=lr, n_steps=100)\n",
    "    \n",
    "    # Check for divergence\n",
    "    diverged = np.any(np.isnan(losses)) or np.any(losses > 1000)\n",
    "    \n",
    "    # Steps to reach low loss\n",
    "    steps_to_converge = None\n",
    "    for i, loss in enumerate(losses):\n",
    "        if loss < 0.01:\n",
    "            steps_to_converge = i\n",
    "            break\n",
    "    \n",
    "    results.append({\n",
    "        'lr': lr,\n",
    "        'final_loss': losses[-1] if not diverged else float('inf'),\n",
    "        'steps_to_0.01': steps_to_converge,\n",
    "        'diverged': diverged\n",
    "    })\n",
    "\n",
    "# TODO: Print results and identify the best learning rate\n",
    "print(\"Learning Rate Experiment Results:\")\n",
    "print(\"-\" * 50)\n",
    "for r in results:\n",
    "    status = \"DIVERGED\" if r['diverged'] else f\"loss={r['final_loss']:.6f}\"\n",
    "    steps = r['steps_to_0.01'] if r['steps_to_0.01'] else \"Never\"\n",
    "    print(f\"lr={r['lr']:<6} | {status:<20} | Steps to 0.01: {steps}\")\n",
    "\n",
    "# TODO: Which learning rate would you choose and why?\n",
    "# Write your answer below:\n",
    "\n",
    "# YOUR ANSWER:\n",
    "# Best LR: ___\n",
    "# Reason: ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Stakeholder Summary\n",
    "\n",
    "**TODO:** Write a 100-150 word summary explaining optimization to a non-technical colleague. Include:\n",
    "1. What gradient descent does in plain English\n",
    "2. Why learning rate matters\n",
    "3. When you would use Adam vs SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Summary:\n",
    "\n",
    "*Write your summary here...*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Gradient descent** is how models learn — iteratively adjusting parameters to reduce error\n",
    "2. **Learning rate** is critical: too high → diverge, too low → slow\n",
    "3. **Momentum** helps smooth oscillations on narrow valleys\n",
    "4. **Adam** is the default choice: combines momentum + adaptive learning rates\n",
    "5. **Regularization** changes the loss surface to prefer smaller weights\n",
    "\n",
    "### Next Steps\n",
    "- Explore the interactive playground to visualize different surfaces and optimizers\n",
    "- Complete the quiz to test your understanding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
