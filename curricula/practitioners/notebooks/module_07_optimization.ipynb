{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 7: Optimization and Gradient Descent\n",
        "\n",
        "**Goal:** Understand how gradient descent works, experiment with learning rates and optimizers.\n",
        "\n",
        "**Prerequisites:** Module 3 (Linear Regression), Module 4 (Logistic Regression)\n",
        "\n",
        "**Expected Runtime:** ~20 minutes\n",
        "\n",
        "**Outputs:**\n",
        "- Visualizations of gradient descent paths\n",
        "- Comparison of optimizers on different loss surfaces\n",
        "- Understanding of learning rate effects\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Plot style\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 11"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Understanding the Loss Surface\n",
        "\n",
        "Gradient descent tries to find the lowest point on a \"loss surface\" - a landscape where the height represents how bad the model's predictions are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define a simple quadratic loss surface (bowl shape)\n",
        "def quadratic_loss(x, y):\n",
        "    return x**2 + y**2\n",
        "\n",
        "def quadratic_gradient(x, y):\n",
        "    return np.array([2*x, 2*y])\n",
        "\n",
        "# Visualize the loss surface\n",
        "x = np.linspace(-3, 3, 100)\n",
        "y = np.linspace(-3, 3, 100)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = quadratic_loss(X, Y)\n",
        "\n",
        "fig = plt.figure(figsize=(14, 5))\n",
        "\n",
        "# 3D view\n",
        "ax1 = fig.add_subplot(121, projection='3d')\n",
        "ax1.plot_surface(X, Y, Z, cmap=cm.viridis, alpha=0.8)\n",
        "ax1.set_xlabel('Œ∏‚ÇÅ')\n",
        "ax1.set_ylabel('Œ∏‚ÇÇ')\n",
        "ax1.set_zlabel('Loss')\n",
        "ax1.set_title('3D View: Quadratic Bowl')\n",
        "\n",
        "# Contour view\n",
        "ax2 = fig.add_subplot(122)\n",
        "contour = ax2.contour(X, Y, Z, levels=15, cmap='viridis')\n",
        "ax2.scatter([0], [0], color='green', s=100, marker='*', label='Minimum')\n",
        "ax2.set_xlabel('Œ∏‚ÇÅ')\n",
        "ax2.set_ylabel('Œ∏‚ÇÇ')\n",
        "ax2.set_title('Contour View (Bird\\'s Eye)')\n",
        "ax2.legend()\n",
        "ax2.set_aspect('equal')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"The goal of gradient descent: Start somewhere on this surface and find the minimum (green star).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Implementing Gradient Descent from Scratch\n",
        "\n",
        "Let's implement vanilla gradient descent to see exactly what happens at each step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def gradient_descent(start, gradient_fn, loss_fn, lr=0.1, n_steps=50):\n",
        "    \"\"\"\n",
        "    Vanilla gradient descent.\n",
        "    \n",
        "    Args:\n",
        "        start: Starting position [x, y]\n",
        "        gradient_fn: Function that returns gradient at a point\n",
        "        loss_fn: Function that returns loss at a point\n",
        "        lr: Learning rate\n",
        "        n_steps: Number of steps to take\n",
        "    \n",
        "    Returns:\n",
        "        path: List of positions visited\n",
        "        losses: List of loss values\n",
        "    \"\"\"\n",
        "    position = np.array(start, dtype=float)\n",
        "    path = [position.copy()]\n",
        "    losses = [loss_fn(position[0], position[1])]\n",
        "    \n",
        "    for _ in range(n_steps):\n",
        "        grad = gradient_fn(position[0], position[1])\n",
        "        position = position - lr * grad  # The core update rule!\n",
        "        path.append(position.copy())\n",
        "        losses.append(loss_fn(position[0], position[1]))\n",
        "    \n",
        "    return np.array(path), np.array(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run gradient descent with a reasonable learning rate\n",
        "start = [-2.5, 2.5]\n",
        "path, losses = gradient_descent(start, quadratic_gradient, quadratic_loss, lr=0.1, n_steps=30)\n",
        "\n",
        "# Visualize the path\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Path on contour plot\n",
        "ax1 = axes[0]\n",
        "ax1.contour(X, Y, Z, levels=15, cmap='viridis', alpha=0.7)\n",
        "ax1.plot(path[:, 0], path[:, 1], 'o-', color='red', markersize=4, linewidth=1.5, label='GD Path')\n",
        "ax1.scatter([start[0]], [start[1]], color='red', s=100, marker='s', zorder=5, label='Start')\n",
        "ax1.scatter([0], [0], color='green', s=100, marker='*', zorder=5, label='Minimum')\n",
        "ax1.set_xlabel('Œ∏‚ÇÅ')\n",
        "ax1.set_ylabel('Œ∏‚ÇÇ')\n",
        "ax1.set_title(f'Gradient Descent Path (lr={0.1})')\n",
        "ax1.legend()\n",
        "ax1.set_aspect('equal')\n",
        "\n",
        "# Loss over time\n",
        "ax2 = axes[1]\n",
        "ax2.plot(losses, 'b-', linewidth=2)\n",
        "ax2.set_xlabel('Step')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.set_title('Loss Over Time')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Starting loss: {losses[0]:.4f}\")\n",
        "print(f\"Final loss: {losses[-1]:.4f}\")\n",
        "print(f\"Final position: ({path[-1, 0]:.4f}, {path[-1, 1]:.4f})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: The Learning Rate Effect\n",
        "\n",
        "Learning rate is the most important hyperparameter in gradient descent. Let's see what happens with different values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Experiment with different learning rates\n",
        "# Try: 0.01, 0.1, 0.5, 0.95, 1.1\n",
        "# Predict what will happen before running!\n",
        "\n",
        "learning_rates = [0.01, 0.1, 0.5, 0.95, 1.1]\n",
        "colors = ['blue', 'green', 'orange', 'red', 'purple']\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Path visualization\n",
        "ax1 = axes[0]\n",
        "ax1.contour(X, Y, Z, levels=15, cmap='viridis', alpha=0.5)\n",
        "\n",
        "# Loss curves\n",
        "ax2 = axes[1]\n",
        "\n",
        "for lr, color in zip(learning_rates, colors):\n",
        "    path, losses = gradient_descent(start, quadratic_gradient, quadratic_loss, lr=lr, n_steps=30)\n",
        "    \n",
        "    # Clip for visualization (some might explode)\n",
        "    path = np.clip(path, -5, 5)\n",
        "    losses = np.clip(losses, 0, 50)\n",
        "    \n",
        "    ax1.plot(path[:, 0], path[:, 1], 'o-', color=color, markersize=3, \n",
        "             linewidth=1, label=f'lr={lr}', alpha=0.7)\n",
        "    ax2.plot(losses, color=color, linewidth=2, label=f'lr={lr}')\n",
        "\n",
        "ax1.scatter([0], [0], color='green', s=100, marker='*', zorder=5)\n",
        "ax1.set_xlabel('Œ∏‚ÇÅ')\n",
        "ax1.set_ylabel('Œ∏‚ÇÇ')\n",
        "ax1.set_title('Paths with Different Learning Rates')\n",
        "ax1.legend()\n",
        "ax1.set_xlim(-4, 4)\n",
        "ax1.set_ylim(-4, 4)\n",
        "\n",
        "ax2.set_xlabel('Step')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.set_title('Loss Curves')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_ylim(0, 20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Observations\n",
        "\n",
        "**lr = 0.01:** Too slow - takes many steps to make progress  \n",
        "**lr = 0.1:** Just right - smooth convergence  \n",
        "**lr = 0.5:** Getting aggressive - some oscillation  \n",
        "**lr = 0.95:** Barely stable - oscillating around minimum  \n",
        "**lr = 1.1:** Diverging! - overshooting and exploding  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: The Ravine Problem\n",
        "\n",
        "Real loss surfaces aren't nice bowls. Let's see what happens with a narrow \"ravine\" shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ravine: Much steeper in y-direction than x-direction\n",
        "def ravine_loss(x, y):\n",
        "    return 0.5 * x**2 + 10 * y**2\n",
        "\n",
        "def ravine_gradient(x, y):\n",
        "    return np.array([x, 20*y])\n",
        "\n",
        "# Visualize the ravine\n",
        "Z_ravine = ravine_loss(X, Y)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.contour(X, Y, Z_ravine, levels=20, cmap='viridis')\n",
        "plt.colorbar(label='Loss')\n",
        "plt.xlabel('Œ∏‚ÇÅ')\n",
        "plt.ylabel('Œ∏‚ÇÇ')\n",
        "plt.title('Ravine Loss Surface: Steep in Y, Flat in X')\n",
        "plt.scatter([0], [0], color='green', s=100, marker='*', label='Minimum')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run vanilla SGD on the ravine\n",
        "start_ravine = [-2.5, 0.5]\n",
        "path_sgd, losses_sgd = gradient_descent(start_ravine, ravine_gradient, ravine_loss, lr=0.05, n_steps=50)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.contour(X, Y, Z_ravine, levels=20, cmap='viridis', alpha=0.5)\n",
        "plt.plot(path_sgd[:, 0], path_sgd[:, 1], 'o-', color='red', markersize=4, linewidth=1, label='SGD Path')\n",
        "plt.scatter([start_ravine[0]], [start_ravine[1]], color='red', s=100, marker='s', zorder=5)\n",
        "plt.scatter([0], [0], color='green', s=100, marker='*', zorder=5)\n",
        "plt.xlabel('Œ∏‚ÇÅ')\n",
        "plt.ylabel('Œ∏‚ÇÇ')\n",
        "plt.title('SGD on Ravine: Zigzag Problem!')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Notice the zigzag pattern! SGD oscillates across the narrow valley.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Momentum to the Rescue\n",
        "\n",
        "Momentum helps smooth out the oscillations by accumulating velocity from past gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def gradient_descent_momentum(start, gradient_fn, loss_fn, lr=0.05, momentum=0.9, n_steps=50):\n",
        "    \"\"\"\n",
        "    SGD with momentum.\n",
        "    \"\"\"\n",
        "    position = np.array(start, dtype=float)\n",
        "    velocity = np.zeros(2)\n",
        "    path = [position.copy()]\n",
        "    losses = [loss_fn(position[0], position[1])]\n",
        "    \n",
        "    for _ in range(n_steps):\n",
        "        grad = gradient_fn(position[0], position[1])\n",
        "        velocity = momentum * velocity - lr * grad  # Accumulate velocity\n",
        "        position = position + velocity\n",
        "        path.append(position.copy())\n",
        "        losses.append(loss_fn(position[0], position[1]))\n",
        "    \n",
        "    return np.array(path), np.array(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compare SGD vs Momentum on the ravine\n",
        "path_momentum, losses_momentum = gradient_descent_momentum(start_ravine, ravine_gradient, ravine_loss, \n",
        "                                                           lr=0.05, momentum=0.9, n_steps=50)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Paths\n",
        "ax1 = axes[0]\n",
        "ax1.contour(X, Y, Z_ravine, levels=20, cmap='viridis', alpha=0.5)\n",
        "ax1.plot(path_sgd[:, 0], path_sgd[:, 1], 'o-', color='red', markersize=3, \n",
        "         linewidth=1, label='SGD', alpha=0.7)\n",
        "ax1.plot(path_momentum[:, 0], path_momentum[:, 1], 'o-', color='blue', markersize=3, \n",
        "         linewidth=1, label='SGD + Momentum')\n",
        "ax1.scatter([0], [0], color='green', s=100, marker='*', zorder=5)\n",
        "ax1.set_xlabel('Œ∏‚ÇÅ')\n",
        "ax1.set_ylabel('Œ∏‚ÇÇ')\n",
        "ax1.set_title('SGD vs Momentum on Ravine')\n",
        "ax1.legend()\n",
        "\n",
        "# Loss curves\n",
        "ax2 = axes[1]\n",
        "ax2.plot(losses_sgd, 'r-', linewidth=2, label='SGD')\n",
        "ax2.plot(losses_momentum, 'b-', linewidth=2, label='SGD + Momentum')\n",
        "ax2.set_xlabel('Step')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.set_title('Loss Over Time')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_ylim(0, 5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"SGD final loss: {losses_sgd[-1]:.6f}\")\n",
        "print(f\"Momentum final loss: {losses_momentum[-1]:.6f}\")\n",
        "print(f\"\\nMomentum converges ~{losses_sgd[-1]/losses_momentum[-1]:.1f}x closer to the minimum!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Implementing Adam\n",
        "\n",
        "Adam combines momentum with adaptive learning rates per parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def adam(start, gradient_fn, loss_fn, lr=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8, n_steps=50):\n",
        "    \"\"\"\n",
        "    Adam optimizer.\n",
        "    \"\"\"\n",
        "    position = np.array(start, dtype=float)\n",
        "    m = np.zeros(2)  # First moment (momentum)\n",
        "    v = np.zeros(2)  # Second moment (adaptive)\n",
        "    path = [position.copy()]\n",
        "    losses = [loss_fn(position[0], position[1])]\n",
        "    \n",
        "    for t in range(1, n_steps + 1):\n",
        "        grad = gradient_fn(position[0], position[1])\n",
        "        \n",
        "        # Update biased first moment estimate\n",
        "        m = beta1 * m + (1 - beta1) * grad\n",
        "        # Update biased second moment estimate\n",
        "        v = beta2 * v + (1 - beta2) * grad**2\n",
        "        \n",
        "        # Bias correction\n",
        "        m_hat = m / (1 - beta1**t)\n",
        "        v_hat = v / (1 - beta2**t)\n",
        "        \n",
        "        # Update position\n",
        "        position = position - lr * m_hat / (np.sqrt(v_hat) + epsilon)\n",
        "        \n",
        "        path.append(position.copy())\n",
        "        losses.append(loss_fn(position[0], position[1]))\n",
        "    \n",
        "    return np.array(path), np.array(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compare all three optimizers on the ravine\n",
        "path_adam, losses_adam = adam(start_ravine, ravine_gradient, ravine_loss, lr=0.3, n_steps=50)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Paths\n",
        "ax1 = axes[0]\n",
        "ax1.contour(X, Y, Z_ravine, levels=20, cmap='viridis', alpha=0.5)\n",
        "ax1.plot(path_sgd[:, 0], path_sgd[:, 1], 'o-', color='red', markersize=3, \n",
        "         linewidth=1, label='SGD', alpha=0.6)\n",
        "ax1.plot(path_momentum[:, 0], path_momentum[:, 1], 'o-', color='blue', markersize=3, \n",
        "         linewidth=1, label='Momentum', alpha=0.6)\n",
        "ax1.plot(path_adam[:, 0], path_adam[:, 1], 'o-', color='purple', markersize=3, \n",
        "         linewidth=1.5, label='Adam')\n",
        "ax1.scatter([0], [0], color='green', s=100, marker='*', zorder=5)\n",
        "ax1.set_xlabel('Œ∏‚ÇÅ')\n",
        "ax1.set_ylabel('Œ∏‚ÇÇ')\n",
        "ax1.set_title('SGD vs Momentum vs Adam')\n",
        "ax1.legend()\n",
        "\n",
        "# Loss curves\n",
        "ax2 = axes[1]\n",
        "ax2.plot(losses_sgd, 'r-', linewidth=2, label='SGD')\n",
        "ax2.plot(losses_momentum, 'b-', linewidth=2, label='Momentum')\n",
        "ax2.plot(losses_adam, 'purple', linewidth=2, label='Adam')\n",
        "ax2.set_xlabel('Step')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.set_title('Loss Over Time')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_ylim(0, 5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Final losses:\")\n",
        "print(f\"  SGD:      {losses_sgd[-1]:.6f}\")\n",
        "print(f\"  Momentum: {losses_momentum[-1]:.6f}\")\n",
        "print(f\"  Adam:     {losses_adam[-1]:.6f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Regularization Effect\n",
        "\n",
        "Regularization adds a penalty for large weights, effectively changing the loss surface."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Original loss vs L2 regularized loss\n",
        "lambda_l2 = 0.5\n",
        "\n",
        "def l2_regularized_loss(x, y):\n",
        "    return quadratic_loss(x, y) + lambda_l2 * (x**2 + y**2)\n",
        "\n",
        "# Visualize the effect\n",
        "Z_l2 = l2_regularized_loss(X, Y)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "ax1 = axes[0]\n",
        "c1 = ax1.contour(X, Y, Z, levels=15, cmap='viridis')\n",
        "ax1.set_title('Original Loss Surface')\n",
        "ax1.set_xlabel('Œ∏‚ÇÅ')\n",
        "ax1.set_ylabel('Œ∏‚ÇÇ')\n",
        "\n",
        "ax2 = axes[1]\n",
        "c2 = ax2.contour(X, Y, Z_l2, levels=15, cmap='viridis')\n",
        "ax2.set_title(f'With L2 Regularization (Œª={lambda_l2})')\n",
        "ax2.set_xlabel('Œ∏‚ÇÅ')\n",
        "ax2.set_ylabel('Œ∏‚ÇÇ')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"L2 regularization makes the bowl steeper, encouraging smaller weights.\")\n",
        "print(\"This helps prevent overfitting by penalizing extreme parameter values.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: TODO - Find the Optimal Learning Rate\n",
        "\n",
        "A common technique is to try several learning rates and pick the one that converges fastest without diverging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Complete this experiment\n",
        "# Test learning rates: [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]\n",
        "# For each, record: final loss, number of steps to reach loss < 0.01, whether it diverged\n",
        "\n",
        "test_lrs = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]\n",
        "results = []\n",
        "\n",
        "for lr in test_lrs:\n",
        "    path, losses = gradient_descent(start, quadratic_gradient, quadratic_loss, lr=lr, n_steps=100)\n",
        "    \n",
        "    # Check for divergence\n",
        "    diverged = np.any(np.isnan(losses)) or np.any(losses > 1000)\n",
        "    \n",
        "    # Steps to reach low loss\n",
        "    steps_to_converge = None\n",
        "    for i, loss in enumerate(losses):\n",
        "        if loss < 0.01:\n",
        "            steps_to_converge = i\n",
        "            break\n",
        "    \n",
        "    results.append({\n",
        "        'lr': lr,\n",
        "        'final_loss': losses[-1] if not diverged else float('inf'),\n",
        "        'steps_to_0.01': steps_to_converge,\n",
        "        'diverged': diverged\n",
        "    })\n",
        "\n",
        "# TODO: Print results and identify the best learning rate\n",
        "print(\"Learning Rate Experiment Results:\")\n",
        "print(\"-\" * 50)\n",
        "for r in results:\n",
        "    status = \"DIVERGED\" if r['diverged'] else f\"loss={r['final_loss']:.6f}\"\n",
        "    steps = r['steps_to_0.01'] if r['steps_to_0.01'] else \"Never\"\n",
        "    print(f\"lr={r['lr']:<6} | {status:<20} | Steps to 0.01: {steps}\")\n",
        "\n",
        "# TODO: Which learning rate would you choose and why?\n",
        "# Write your answer below:\n",
        "\n",
        "# YOUR ANSWER:\n",
        "# Best LR: ___\n",
        "# Reason: ___"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: Connecting to StreamCart\n",
        "\n",
        "Let's apply what we learned about optimizers to our churn prediction model. When training a neural network or logistic regression on StreamCart data, these concepts matter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Applying optimizer concepts to real ML training\n",
        "# This code demonstrates how optimizer choices affect model training\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load StreamCart data\n",
        "DATA_URL = 'https://raw.githubusercontent.com/189investmentai/ml-foundations-interactive/main/shared/data/'\n",
        "try:\n",
        "    customers = pd.read_csv(DATA_URL + 'streamcart_customers.csv')\n",
        "except:\n",
        "    print(\"Using simulated data for demonstration\")\n",
        "    np.random.seed(42)\n",
        "    customers = pd.DataFrame({\n",
        "        'tenure_days': np.random.randint(1, 365, 1000),\n",
        "        'orders_total': np.random.randint(0, 50, 1000),\n",
        "        'total_spend': np.random.uniform(0, 5000, 1000),\n",
        "        'churn_30d': np.random.binomial(1, 0.2, 1000)\n",
        "    })\n",
        "\n",
        "# Prepare features\n",
        "feature_cols = ['tenure_days', 'orders_total', 'total_spend']\n",
        "available = [c for c in feature_cols if c in customers.columns]\n",
        "X = customers[available].fillna(0)\n",
        "y = customers['churn_30d']\n",
        "\n",
        "# Scale features (critical for gradient descent!)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Compare different learning rates with SGD\n",
        "print(\"=== Learning Rate Impact on StreamCart Churn Model ===\\n\")\n",
        "for lr in [0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
        "    # SGDClassifier uses gradient descent under the hood\n",
        "    model = SGDClassifier(\n",
        "        loss='log_loss',       # Logistic regression\n",
        "        learning_rate='constant',\n",
        "        eta0=lr,               # Learning rate\n",
        "        max_iter=100,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, model.predict(X_test))\n",
        "    print(f\"lr={lr:<6} ‚Üí Test Accuracy: {acc:.1%}\")\n",
        "\n",
        "print(\"\\nüí° Insight: Too high or too low learning rate hurts performance!\")\n",
        "print(\"   The best lr depends on feature scaling and data characteristics.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 10: Stakeholder Summary\n",
        "\n",
        "**TODO:** Write a 3-bullet summary (~100 words) explaining optimization to a non-technical colleague:\n",
        "‚Ä¢ **What gradient descent does:** [Plain English explanation]\n",
        "‚Ä¢ **Why learning rate matters:** [One sentence on what happens if too high/low]\n",
        "‚Ä¢ **When to use Adam vs SGD:** [Simple guidance]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Summary:\n",
        "\n",
        "*Write your summary here...*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Check\n",
        "\n",
        "Uncomment and run the asserts below to verify your optimizer experiments are correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# SELF-CHECK: Verify your optimization experiments\n",
        "# Run this cell after completing the TODOs above\n",
        "\n",
        "assert len(losses) > 1 and losses[-1] < losses[0], \"Loss should decrease over gradient descent steps\"\n",
        "assert len(results) > 0, \"Learning rate experiment results should exist\"\n",
        "converged = [r for r in results if not r.get('diverged', False)]\n",
        "assert len(converged) >= 2, \"At least 2 learning rates should converge\"\n",
        "best = min(converged, key=lambda r: r['final_loss'])\n",
        "assert best['final_loss'] < 0.5, f\"Best learning rate should reach low loss, got {best['final_loss']:.3f}\"\n",
        "print(f\"‚úÖ Self-check passed! {len(converged)}/{len(results)} learning rates converged\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Gradient descent** is how models learn - iteratively adjusting parameters to reduce error\n",
        "2. **Learning rate** is critical: too high ‚Üí diverge, too low ‚Üí slow\n",
        "3. **Momentum** helps smooth oscillations on narrow valleys\n",
        "4. **Adam** is the default choice: combines momentum + adaptive learning rates\n",
        "5. **Regularization** changes the loss surface to prefer smaller weights\n",
        "\n",
        "### Next Steps\n",
        "- Explore the interactive playground to visualize different surfaces and optimizers\n",
        "- Complete the quiz to test your understanding"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}