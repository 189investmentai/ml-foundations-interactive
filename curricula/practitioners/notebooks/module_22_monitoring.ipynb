{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 22: Monitoring - Keeping Models Healthy\n",
        "\n",
        "**Goal:** Learn how to detect model degradation, set up alerts, and know when to retrain.\n",
        "\n",
        "**Prerequisites:** Module 21 (MLOps)\n",
        "\n",
        "**Expected Runtime:** ~25 minutes\n",
        "\n",
        "**Outputs:**\n",
        "- Implemented drift detection\n",
        "- Built alerting system\n",
        "- Created monitoring dashboard\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8-whitegrid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Generate Data with Drift"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generate_data(n_samples: int, drift_factor: float = 0.0, concept_drift: float = 0.0):\n",
        "    \"\"\"\n",
        "    Generate data with optional drift.\n",
        "    \n",
        "    Args:\n",
        "        n_samples: Number of samples\n",
        "        drift_factor: Amount of data drift (0-1)\n",
        "        concept_drift: Amount of concept drift (0-1)\n",
        "    \"\"\"\n",
        "    # Base features\n",
        "    tenure = np.random.exponential(20, n_samples)\n",
        "    spend = np.random.normal(100, 30, n_samples)\n",
        "    support_tickets = np.random.poisson(2, n_samples)\n",
        "    engagement = np.random.beta(5, 2, n_samples) * 100\n",
        "    \n",
        "    # Apply data drift (shift distributions)\n",
        "    if drift_factor > 0:\n",
        "        tenure = tenure * (1 + drift_factor * 0.5)  # Older customers\n",
        "        spend = spend * (1 + drift_factor * 0.3)     # Higher spend\n",
        "        engagement = engagement * (1 - drift_factor * 0.2)  # Lower engagement\n",
        "    \n",
        "    # Base churn probability\n",
        "    churn_prob = 0.2 + 0.2 * (support_tickets > 3) - 0.1 * (engagement > 70)\n",
        "    \n",
        "    # Apply concept drift (change relationship)\n",
        "    if concept_drift > 0:\n",
        "        # The relationship changes - support tickets become less predictive\n",
        "        churn_prob = 0.2 + 0.1 * (support_tickets > 3) - 0.05 * (engagement > 70)\n",
        "        # New factor becomes important\n",
        "        churn_prob += concept_drift * 0.2 * (spend < 80)\n",
        "    \n",
        "    churn_prob = np.clip(churn_prob, 0.05, 0.95)\n",
        "    churned = (np.random.random(n_samples) < churn_prob).astype(int)\n",
        "    \n",
        "    return pd.DataFrame({\n",
        "        'tenure': tenure,\n",
        "        'spend': spend,\n",
        "        'support_tickets': support_tickets,\n",
        "        'engagement': engagement,\n",
        "        'churned': churned\n",
        "    })\n",
        "\n",
        "# Generate training data (no drift)\n",
        "train_data = generate_data(2000, drift_factor=0, concept_drift=0)\n",
        "print(f\"Training data: {len(train_data)} rows, churn rate: {train_data['churned'].mean():.1%}\")\n",
        "\n",
        "# Generate production data with drift\n",
        "prod_data_clean = generate_data(500, drift_factor=0, concept_drift=0)\n",
        "prod_data_drift = generate_data(500, drift_factor=0.5, concept_drift=0)\n",
        "prod_data_concept = generate_data(500, drift_factor=0, concept_drift=0.5)\n",
        "prod_data_both = generate_data(500, drift_factor=0.5, concept_drift=0.5)\n",
        "\n",
        "print(\"Production datasets generated with various drift levels.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Train Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare data\n",
        "features = ['tenure', 'spend', 'support_tickets', 'engagement']\n",
        "X_train = train_data[features]\n",
        "y_train = train_data['churned']\n",
        "\n",
        "# Train model\n",
        "model = GradientBoostingClassifier(n_estimators=100, max_depth=4, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Baseline metrics\n",
        "train_preds = model.predict(X_train)\n",
        "train_probs = model.predict_proba(X_train)[:, 1]\n",
        "\n",
        "baseline_metrics = {\n",
        "    'accuracy': accuracy_score(y_train, train_preds),\n",
        "    'auc': roc_auc_score(y_train, train_probs),\n",
        "    'pred_mean': train_probs.mean(),\n",
        "    'pred_std': train_probs.std()\n",
        "}\n",
        "\n",
        "print(\"=== Baseline Metrics ===\")\n",
        "for k, v in baseline_metrics.items():\n",
        "    print(f\"  {k}: {v:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Drift Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def calculate_psi(expected: np.ndarray, actual: np.ndarray, buckets: int = 10) -> float:\n",
        "    \"\"\"Calculate Population Stability Index.\"\"\"\n",
        "    # Create buckets based on expected distribution\n",
        "    bins = np.percentile(expected, np.linspace(0, 100, buckets + 1))\n",
        "    bins[0] = -np.inf\n",
        "    bins[-1] = np.inf\n",
        "    \n",
        "    # Calculate proportions\n",
        "    expected_counts = np.histogram(expected, bins)[0] / len(expected)\n",
        "    actual_counts = np.histogram(actual, bins)[0] / len(actual)\n",
        "    \n",
        "    # Avoid division by zero\n",
        "    expected_counts = np.clip(expected_counts, 0.0001, None)\n",
        "    actual_counts = np.clip(actual_counts, 0.0001, None)\n",
        "    \n",
        "    # Calculate PSI\n",
        "    psi = np.sum((actual_counts - expected_counts) * np.log(actual_counts / expected_counts))\n",
        "    return psi\n",
        "\n",
        "def detect_drift(reference: pd.DataFrame, current: pd.DataFrame, features: List[str]) -> Dict:\n",
        "    \"\"\"Detect drift across features.\"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    for feature in features:\n",
        "        # KS test\n",
        "        ks_stat, ks_pvalue = stats.ks_2samp(reference[feature], current[feature])\n",
        "        \n",
        "        # PSI\n",
        "        psi = calculate_psi(reference[feature].values, current[feature].values)\n",
        "        \n",
        "        results[feature] = {\n",
        "            'ks_statistic': ks_stat,\n",
        "            'ks_pvalue': ks_pvalue,\n",
        "            'psi': psi,\n",
        "            'drift_detected': psi > 0.1 or ks_pvalue < 0.05\n",
        "        }\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Test drift detection\n",
        "print(\"=== Drift Detection Results ===\")\n",
        "\n",
        "scenarios = [\n",
        "    (\"No Drift\", prod_data_clean),\n",
        "    (\"Data Drift\", prod_data_drift),\n",
        "    (\"Concept Drift\", prod_data_concept),\n",
        "    (\"Both Drifts\", prod_data_both)\n",
        "]\n",
        "\n",
        "for name, data in scenarios:\n",
        "    drift_results = detect_drift(train_data, data, features)\n",
        "    drifted_features = [f for f, r in drift_results.items() if r['drift_detected']]\n",
        "    avg_psi = np.mean([r['psi'] for r in drift_results.values()])\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Drifted features: {drifted_features if drifted_features else 'None'}\")\n",
        "    print(f\"  Average PSI: {avg_psi:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Performance Monitoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def monitor_performance(model, data: pd.DataFrame, features: List[str], baseline: Dict) -> Dict:\n",
        "    \"\"\"Monitor model performance on new data.\"\"\"\n",
        "    X = data[features]\n",
        "    y = data['churned']\n",
        "    \n",
        "    preds = model.predict(X)\n",
        "    probs = model.predict_proba(X)[:, 1]\n",
        "    \n",
        "    current_metrics = {\n",
        "        'accuracy': accuracy_score(y, preds),\n",
        "        'auc': roc_auc_score(y, probs),\n",
        "        'pred_mean': probs.mean(),\n",
        "        'pred_std': probs.std()\n",
        "    }\n",
        "    \n",
        "    # Compare to baseline\n",
        "    degradation = {\n",
        "        'accuracy_drop': baseline['accuracy'] - current_metrics['accuracy'],\n",
        "        'auc_drop': baseline['auc'] - current_metrics['auc'],\n",
        "        'pred_mean_shift': abs(current_metrics['pred_mean'] - baseline['pred_mean'])\n",
        "    }\n",
        "    \n",
        "    return {\n",
        "        'current': current_metrics,\n",
        "        'degradation': degradation,\n",
        "        'alert': degradation['accuracy_drop'] > 0.05 or degradation['auc_drop'] > 0.05\n",
        "    }\n",
        "\n",
        "# Monitor across scenarios\n",
        "print(\"=== Performance Monitoring ===\")\n",
        "\n",
        "for name, data in scenarios:\n",
        "    perf = monitor_performance(model, data, features, baseline_metrics)\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Current Accuracy: {perf['current']['accuracy']:.3f}\")\n",
        "    print(f\"  Accuracy Drop: {perf['degradation']['accuracy_drop']:.3f}\")\n",
        "    print(f\"  Alert: {'ðŸš¨ YES' if perf['alert'] else 'âœ“ No'}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Alerting System"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@dataclass\n",
        "class Alert:\n",
        "    level: str  # info, warning, critical\n",
        "    metric: str\n",
        "    message: str\n",
        "    value: float\n",
        "    threshold: float\n",
        "    timestamp: str\n",
        "\n",
        "class AlertingSystem:\n",
        "    \"\"\"Simple alerting system for model monitoring.\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Dict):\n",
        "        self.config = config\n",
        "        self.alerts: List[Alert] = []\n",
        "    \n",
        "    def check(self, metrics: Dict) -> List[Alert]:\n",
        "        \"\"\"Check metrics against thresholds.\"\"\"\n",
        "        new_alerts = []\n",
        "        timestamp = datetime.now().isoformat()\n",
        "        \n",
        "        # Check accuracy\n",
        "        if metrics.get('accuracy_drop', 0) > self.config['critical']['accuracy_drop']:\n",
        "            new_alerts.append(Alert(\n",
        "                level='critical',\n",
        "                metric='accuracy',\n",
        "                message=f\"Accuracy dropped by {metrics['accuracy_drop']:.1%}\",\n",
        "                value=metrics['accuracy_drop'],\n",
        "                threshold=self.config['critical']['accuracy_drop'],\n",
        "                timestamp=timestamp\n",
        "            ))\n",
        "        elif metrics.get('accuracy_drop', 0) > self.config['warning']['accuracy_drop']:\n",
        "            new_alerts.append(Alert(\n",
        "                level='warning',\n",
        "                metric='accuracy',\n",
        "                message=f\"Accuracy dropped by {metrics['accuracy_drop']:.1%}\",\n",
        "                value=metrics['accuracy_drop'],\n",
        "                threshold=self.config['warning']['accuracy_drop'],\n",
        "                timestamp=timestamp\n",
        "            ))\n",
        "        \n",
        "        # Check PSI\n",
        "        if metrics.get('psi', 0) > self.config['critical']['psi']:\n",
        "            new_alerts.append(Alert(\n",
        "                level='critical',\n",
        "                metric='psi',\n",
        "                message=f\"Significant drift detected (PSI: {metrics['psi']:.2f})\",\n",
        "                value=metrics['psi'],\n",
        "                threshold=self.config['critical']['psi'],\n",
        "                timestamp=timestamp\n",
        "            ))\n",
        "        elif metrics.get('psi', 0) > self.config['warning']['psi']:\n",
        "            new_alerts.append(Alert(\n",
        "                level='warning',\n",
        "                metric='psi',\n",
        "                message=f\"Moderate drift detected (PSI: {metrics['psi']:.2f})\",\n",
        "                value=metrics['psi'],\n",
        "                threshold=self.config['warning']['psi'],\n",
        "                timestamp=timestamp\n",
        "            ))\n",
        "        \n",
        "        self.alerts.extend(new_alerts)\n",
        "        return new_alerts\n",
        "\n",
        "# Configure alerting\n",
        "alert_config = {\n",
        "    'warning': {'accuracy_drop': 0.03, 'psi': 0.1},\n",
        "    'critical': {'accuracy_drop': 0.05, 'psi': 0.25}\n",
        "}\n",
        "\n",
        "alerting = AlertingSystem(alert_config)\n",
        "\n",
        "# Test alerting\n",
        "print(\"=== Alerting System ===\")\n",
        "\n",
        "for name, data in scenarios:\n",
        "    perf = monitor_performance(model, data, features, baseline_metrics)\n",
        "    drift = detect_drift(train_data, data, features)\n",
        "    avg_psi = np.mean([r['psi'] for r in drift.values()])\n",
        "    \n",
        "    metrics = {\n",
        "        'accuracy_drop': perf['degradation']['accuracy_drop'],\n",
        "        'psi': avg_psi\n",
        "    }\n",
        "    \n",
        "    alerts = alerting.check(metrics)\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    if alerts:\n",
        "        for alert in alerts:\n",
        "            icon = 'ðŸš¨' if alert.level == 'critical' else 'âš ï¸'\n",
        "            print(f\"  {icon} [{alert.level.upper()}] {alert.message}\")\n",
        "    else:\n",
        "        print(\"  âœ“ No alerts\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Retraining Decision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class RetrainingDecision:\n",
        "    \"\"\"Decide when to retrain the model.\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Dict):\n",
        "        self.config = config\n",
        "        self.last_retrain = datetime.now() - timedelta(days=30)\n",
        "    \n",
        "    def should_retrain(self, metrics: Dict) -> Tuple[bool, str]:\n",
        "        \"\"\"Decide if model should be retrained.\"\"\"\n",
        "        \n",
        "        # Check accuracy\n",
        "        if metrics.get('accuracy_drop', 0) > self.config['accuracy_threshold']:\n",
        "            return True, f\"Accuracy dropped by {metrics['accuracy_drop']:.1%}\"\n",
        "        \n",
        "        # Check drift\n",
        "        if metrics.get('psi', 0) > self.config['psi_threshold']:\n",
        "            return True, f\"Significant drift detected (PSI: {metrics['psi']:.2f})\"\n",
        "        \n",
        "        # Check time since last retrain\n",
        "        days_since = (datetime.now() - self.last_retrain).days\n",
        "        if days_since > self.config['max_days']:\n",
        "            return True, f\"Scheduled retrain ({days_since} days since last)\"\n",
        "        \n",
        "        return False, \"Model is healthy\"\n",
        "\n",
        "# Configure retraining\n",
        "retrain_config = {\n",
        "    'accuracy_threshold': 0.05,\n",
        "    'psi_threshold': 0.25,\n",
        "    'max_days': 30\n",
        "}\n",
        "\n",
        "retrain_decision = RetrainingDecision(retrain_config)\n",
        "\n",
        "# Test decisions\n",
        "print(\"=== Retraining Decisions ===\")\n",
        "\n",
        "for name, data in scenarios:\n",
        "    perf = monitor_performance(model, data, features, baseline_metrics)\n",
        "    drift = detect_drift(train_data, data, features)\n",
        "    avg_psi = np.mean([r['psi'] for r in drift.values()])\n",
        "    \n",
        "    metrics = {\n",
        "        'accuracy_drop': perf['degradation']['accuracy_drop'],\n",
        "        'psi': avg_psi\n",
        "    }\n",
        "    \n",
        "    should_retrain, reason = retrain_decision.should_retrain(metrics)\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Should retrain: {'YES' if should_retrain else 'No'}\")\n",
        "    print(f\"  Reason: {reason}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulate monitoring over time\n",
        "def simulate_production(model, baseline, days=30):\n",
        "    \"\"\"Simulate production monitoring over time.\"\"\"\n",
        "    metrics_over_time = []\n",
        "    \n",
        "    for day in range(days):\n",
        "        # Gradually increase drift\n",
        "        drift_factor = min(day / 30, 1.0) * 0.5\n",
        "        concept_drift = max(0, (day - 15) / 30) * 0.5\n",
        "        \n",
        "        # Generate daily data\n",
        "        daily_data = generate_data(200, drift_factor=drift_factor, concept_drift=concept_drift)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        X = daily_data[features]\n",
        "        y = daily_data['churned']\n",
        "        preds = model.predict(X)\n",
        "        probs = model.predict_proba(X)[:, 1]\n",
        "        \n",
        "        accuracy = accuracy_score(y, preds)\n",
        "        drift_results = detect_drift(train_data, daily_data, features)\n",
        "        avg_psi = np.mean([r['psi'] for r in drift_results.values()])\n",
        "        \n",
        "        metrics_over_time.append({\n",
        "            'day': day,\n",
        "            'accuracy': accuracy,\n",
        "            'psi': avg_psi,\n",
        "            'pred_mean': probs.mean()\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(metrics_over_time)\n",
        "\n",
        "# Run simulation\n",
        "metrics_df = simulate_production(model, baseline_metrics, days=30)\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
        "\n",
        "# Accuracy over time\n",
        "axes[0].plot(metrics_df['day'], metrics_df['accuracy'], 'b-', linewidth=2)\n",
        "axes[0].axhline(y=baseline_metrics['accuracy'], color='g', linestyle='--', label='Baseline')\n",
        "axes[0].axhline(y=baseline_metrics['accuracy'] - 0.05, color='r', linestyle='--', label='Critical threshold')\n",
        "axes[0].fill_between(metrics_df['day'], 0, metrics_df['accuracy'], \n",
        "                     where=metrics_df['accuracy'] < baseline_metrics['accuracy'] - 0.05, \n",
        "                     alpha=0.3, color='red')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Model Accuracy Over Time')\n",
        "axes[0].legend()\n",
        "axes[0].set_ylim(0.5, 1.0)\n",
        "\n",
        "# PSI over time\n",
        "colors = ['green' if psi < 0.1 else 'orange' if psi < 0.25 else 'red' for psi in metrics_df['psi']]\n",
        "axes[1].bar(metrics_df['day'], metrics_df['psi'], color=colors)\n",
        "axes[1].axhline(y=0.1, color='orange', linestyle='--', label='Warning')\n",
        "axes[1].axhline(y=0.25, color='red', linestyle='--', label='Critical')\n",
        "axes[1].set_ylabel('PSI')\n",
        "axes[1].set_title('Feature Drift (PSI) Over Time')\n",
        "axes[1].legend()\n",
        "\n",
        "# Prediction distribution\n",
        "axes[2].plot(metrics_df['day'], metrics_df['pred_mean'], 'purple', linewidth=2)\n",
        "axes[2].axhline(y=baseline_metrics['pred_mean'], color='g', linestyle='--', label='Baseline')\n",
        "axes[2].set_xlabel('Day')\n",
        "axes[2].set_ylabel('Mean Prediction')\n",
        "axes[2].set_title('Prediction Distribution Shift')\n",
        "axes[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: TODO - Build Your Monitoring Dashboard\n",
        "\n",
        "Extend the monitoring system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Add these to the monitoring system:\n",
        "# 1. Latency monitoring\n",
        "# 2. Error rate tracking\n",
        "# 3. Custom business metrics\n",
        "# 4. Automated report generation\n",
        "\n",
        "print(\"Extend the monitoring system with additional metrics!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Check\n",
        "\n",
        "Run the cell below to verify your monitoring dashboard components are correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# SELF-CHECK: Verify your monitoring setup\n",
        "\n",
        "assert callable(calculate_psi), \"calculate_psi function should exist\"\n",
        "assert callable(detect_drift), \"detect_drift function should exist\"\n",
        "assert hasattr(alerting, 'alerts'), \"AlertingSystem should track alerts\"\n",
        "assert hasattr(retrain_decision, 'should_retrain'), \"RetrainingDecision should have decision method\"\n",
        "print(f\"âœ… Self-check passed! Monitoring system configured with {len(alerting.alerts)} alerts triggered\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Stakeholder Summary\n",
        "\n",
        "### TODO: Write a 3-bullet summary (~100 words) for the PM\n",
        "\n",
        "Template:\n",
        "â€¢ **What is drift:** When real-world data changes vs training data. Like a weather model trained on summer data failing in winter.\n",
        "â€¢ **How we catch it:** Monitor PSI (data shift) and accuracy. Alert at PSI > 0.1 (warning) or > 0.25 (retrain trigger).\n",
        "â€¢ **Business impact:** Without monitoring, degradation is silent. By the time retention offers stop working, we've already lost customers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Types of drift:** Data drift, concept drift, label drift\n",
        "2. **Detection methods:** KS test, PSI, accuracy monitoring\n",
        "3. **Alerting:** Set appropriate thresholds, avoid fatigue\n",
        "4. **Retraining:** Trigger on drift, accuracy drop, or schedule\n",
        "5. **Monitor everything:** Inputs, outputs, performance, business metrics\n",
        "\n",
        "### Congratulations!\n",
        "\n",
        "You've completed the full ML curriculum! You now understand:\n",
        "- ML fundamentals and problem framing\n",
        "- Supervised learning algorithms\n",
        "- Feature engineering and evaluation\n",
        "- Deep learning and transformers\n",
        "- LLMs and agentic AI\n",
        "- MLOps and production monitoring\n",
        "\n",
        "### Next Steps\n",
        "- Complete the Capstone Project\n",
        "- Apply these skills to real problems\n",
        "- Keep learning and practicing!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}