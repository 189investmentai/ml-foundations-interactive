{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 21: MLOps - From Notebook to Production\n",
        "\n",
        "**Goal:** Learn how to move ML models from experimentation to production with versioning, reproducibility, and deployment practices.\n",
        "\n",
        "**Prerequisites:** Modules 1-20\n",
        "\n",
        "**Expected Runtime:** ~25 minutes\n",
        "\n",
        "**Outputs:**\n",
        "- Implemented versioning for models\n",
        "- Built experiment tracking\n",
        "- Created a training pipeline\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any, Optional\n",
        "from dataclasses import dataclass, asdict\n",
        "import joblib\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Model Versioning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@dataclass\n",
        "class ModelMetadata:\n",
        "    \"\"\"Metadata for a trained model.\"\"\"\n",
        "    version: str\n",
        "    name: str\n",
        "    trained_at: str\n",
        "    data_version: str\n",
        "    git_commit: str\n",
        "    config: Dict[str, Any]\n",
        "    metrics: Dict[str, float]\n",
        "    features: list\n",
        "    threshold: Optional[float] = None\n",
        "\n",
        "class ModelRegistry:\n",
        "    \"\"\"Simple model registry for versioning.\"\"\"\n",
        "    \n",
        "    def __init__(self, base_path: str = \"models\"):\n",
        "        self.base_path = base_path\n",
        "        os.makedirs(base_path, exist_ok=True)\n",
        "        self.registry_file = os.path.join(base_path, \"registry.json\")\n",
        "        self._load_registry()\n",
        "    \n",
        "    def _load_registry(self):\n",
        "        if os.path.exists(self.registry_file):\n",
        "            with open(self.registry_file) as f:\n",
        "                self.registry = json.load(f)\n",
        "        else:\n",
        "            self.registry = {\"models\": []}\n",
        "    \n",
        "    def _save_registry(self):\n",
        "        with open(self.registry_file, \"w\") as f:\n",
        "            json.dump(self.registry, f, indent=2)\n",
        "    \n",
        "    def register(self, model, metadata: ModelMetadata) -> str:\n",
        "        \"\"\"Register a new model version.\"\"\"\n",
        "        model_path = os.path.join(self.base_path, f\"{metadata.name}_{metadata.version}.joblib\")\n",
        "        meta_path = os.path.join(self.base_path, f\"{metadata.name}_{metadata.version}.json\")\n",
        "        \n",
        "        # Save model\n",
        "        joblib.dump(model, model_path)\n",
        "        \n",
        "        # Save metadata\n",
        "        with open(meta_path, \"w\") as f:\n",
        "            json.dump(asdict(metadata), f, indent=2)\n",
        "        \n",
        "        # Update registry\n",
        "        self.registry[\"models\"].append({\n",
        "            \"name\": metadata.name,\n",
        "            \"version\": metadata.version,\n",
        "            \"path\": model_path,\n",
        "            \"registered_at\": datetime.now().isoformat()\n",
        "        })\n",
        "        self._save_registry()\n",
        "        \n",
        "        return model_path\n",
        "    \n",
        "    def get_latest(self, name: str):\n",
        "        \"\"\"Get the latest version of a model.\"\"\"\n",
        "        versions = [m for m in self.registry[\"models\"] if m[\"name\"] == name]\n",
        "        if not versions:\n",
        "            return None\n",
        "        latest = sorted(versions, key=lambda x: x[\"version\"])[-1]\n",
        "        return joblib.load(latest[\"path\"])\n",
        "    \n",
        "    def list_versions(self, name: str = None):\n",
        "        \"\"\"List all registered versions.\"\"\"\n",
        "        models = self.registry[\"models\"]\n",
        "        if name:\n",
        "            models = [m for m in models if m[\"name\"] == name]\n",
        "        return models\n",
        "\n",
        "# Create registry\n",
        "registry = ModelRegistry(\"./demo_models\")\n",
        "print(\"Model registry initialized!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Experiment Tracking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@dataclass\n",
        "class Experiment:\n",
        "    \"\"\"Record of a training experiment.\"\"\"\n",
        "    name: str\n",
        "    timestamp: str\n",
        "    params: Dict[str, Any]\n",
        "    metrics: Dict[str, float]\n",
        "    data_hash: str\n",
        "    notes: str = \"\"\n",
        "\n",
        "class ExperimentTracker:\n",
        "    \"\"\"Simple experiment tracking.\"\"\"\n",
        "    \n",
        "    def __init__(self, log_path: str = \"experiments\"):\n",
        "        self.log_path = log_path\n",
        "        os.makedirs(log_path, exist_ok=True)\n",
        "        self.experiments = []\n",
        "    \n",
        "    def log(self, experiment: Experiment):\n",
        "        \"\"\"Log an experiment.\"\"\"\n",
        "        self.experiments.append(experiment)\n",
        "        \n",
        "        # Save to file\n",
        "        filename = f\"{experiment.name}_{experiment.timestamp.replace(':', '-')}.json\"\n",
        "        filepath = os.path.join(self.log_path, filename)\n",
        "        with open(filepath, \"w\") as f:\n",
        "            json.dump(asdict(experiment), f, indent=2)\n",
        "        \n",
        "        return filepath\n",
        "    \n",
        "    def compare(self, metric: str = \"auc\"):\n",
        "        \"\"\"Compare experiments by metric.\"\"\"\n",
        "        if not self.experiments:\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        data = []\n",
        "        for exp in self.experiments:\n",
        "            row = {\"name\": exp.name, \"timestamp\": exp.timestamp}\n",
        "            row.update(exp.params)\n",
        "            row.update(exp.metrics)\n",
        "            data.append(row)\n",
        "        \n",
        "        df = pd.DataFrame(data)\n",
        "        if metric in df.columns:\n",
        "            df = df.sort_values(metric, ascending=False)\n",
        "        return df\n",
        "\n",
        "def hash_data(df: pd.DataFrame) -> str:\n",
        "    \"\"\"Create a hash of dataframe for versioning.\"\"\"\n",
        "    return hashlib.md5(pd.util.hash_pandas_object(df).values).hexdigest()[:8]\n",
        "\n",
        "# Create tracker\n",
        "tracker = ExperimentTracker(\"./demo_experiments\")\n",
        "print(\"Experiment tracker initialized!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate sample data\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'tenure': np.random.exponential(20, n_samples),\n",
        "    'spend': np.random.normal(100, 30, n_samples),\n",
        "    'support_tickets': np.random.poisson(2, n_samples),\n",
        "    'engagement': np.random.beta(5, 2, n_samples) * 100\n",
        "})\n",
        "\n",
        "# Create target with realistic relationship\n",
        "churn_prob = 0.2 + 0.2 * (data['support_tickets'] > 3) - 0.1 * (data['engagement'] > 70)\n",
        "data['churned'] = (np.random.random(n_samples) < churn_prob).astype(int)\n",
        "\n",
        "print(f\"Sample data: {len(data)} rows, churn rate: {data['churned'].mean():.1%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class TrainingPipeline:\n",
        "    \"\"\"End-to-end training pipeline.\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Dict):\n",
        "        self.config = config\n",
        "        self.data = None\n",
        "        self.model = None\n",
        "        self.metrics = {}\n",
        "    \n",
        "    def load_data(self, df: pd.DataFrame):\n",
        "        \"\"\"Step 1: Load and validate data.\"\"\"\n",
        "        print(\"Step 1: Loading data...\")\n",
        "        self.data = df.copy()\n",
        "        self.data_hash = hash_data(df)\n",
        "        print(f\"  Loaded {len(df)} rows, hash: {self.data_hash}\")\n",
        "        return self\n",
        "    \n",
        "    def preprocess(self):\n",
        "        \"\"\"Step 2: Preprocess features.\"\"\"\n",
        "        print(\"Step 2: Preprocessing...\")\n",
        "        features = self.config[\"features\"]\n",
        "        target = self.config[\"target\"]\n",
        "        \n",
        "        self.X = self.data[features]\n",
        "        self.y = self.data[target]\n",
        "        \n",
        "        # Split\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            self.X, self.y, \n",
        "            test_size=self.config[\"test_size\"],\n",
        "            random_state=self.config[\"random_state\"]\n",
        "        )\n",
        "        print(f\"  Train: {len(self.X_train)}, Test: {len(self.X_test)}\")\n",
        "        return self\n",
        "    \n",
        "    def train(self):\n",
        "        \"\"\"Step 3: Train model.\"\"\"\n",
        "        print(\"Step 3: Training model...\")\n",
        "        model_config = self.config[\"model\"]\n",
        "        \n",
        "        self.model = GradientBoostingClassifier(\n",
        "            n_estimators=model_config[\"n_estimators\"],\n",
        "            max_depth=model_config[\"max_depth\"],\n",
        "            random_state=self.config[\"random_state\"]\n",
        "        )\n",
        "        self.model.fit(self.X_train, self.y_train)\n",
        "        print(\"  Model trained!\")\n",
        "        return self\n",
        "    \n",
        "    def evaluate(self):\n",
        "        \"\"\"Step 4: Evaluate model.\"\"\"\n",
        "        print(\"Step 4: Evaluating...\")\n",
        "        y_prob = self.model.predict_proba(self.X_test)[:, 1]\n",
        "        y_pred = (y_prob >= self.config[\"threshold\"]).astype(int)\n",
        "        \n",
        "        self.metrics = {\n",
        "            \"auc\": roc_auc_score(self.y_test, y_prob),\n",
        "            \"precision\": precision_score(self.y_test, y_pred),\n",
        "            \"recall\": recall_score(self.y_test, y_pred)\n",
        "        }\n",
        "        print(f\"  AUC: {self.metrics['auc']:.3f}\")\n",
        "        print(f\"  Precision: {self.metrics['precision']:.3f}\")\n",
        "        print(f\"  Recall: {self.metrics['recall']:.3f}\")\n",
        "        return self\n",
        "    \n",
        "    def register(self, registry: ModelRegistry, version: str):\n",
        "        \"\"\"Step 5: Register model.\"\"\"\n",
        "        print(\"Step 5: Registering model...\")\n",
        "        \n",
        "        metadata = ModelMetadata(\n",
        "            version=version,\n",
        "            name=\"churn_model\",\n",
        "            trained_at=datetime.now().isoformat(),\n",
        "            data_version=self.data_hash,\n",
        "            git_commit=\"demo\",\n",
        "            config=self.config,\n",
        "            metrics=self.metrics,\n",
        "            features=self.config[\"features\"],\n",
        "            threshold=self.config[\"threshold\"]\n",
        "        )\n",
        "        \n",
        "        path = registry.register(self.model, metadata)\n",
        "        print(f\"  Registered as {version} at {path}\")\n",
        "        return self\n",
        "    \n",
        "    def run(self, df: pd.DataFrame, registry: ModelRegistry, version: str):\n",
        "        \"\"\"Run full pipeline.\"\"\"\n",
        "        return (self\n",
        "                .load_data(df)\n",
        "                .preprocess()\n",
        "                .train()\n",
        "                .evaluate()\n",
        "                .register(registry, version))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define configuration\n",
        "config = {\n",
        "    \"features\": [\"tenure\", \"spend\", \"support_tickets\", \"engagement\"],\n",
        "    \"target\": \"churned\",\n",
        "    \"test_size\": 0.2,\n",
        "    \"random_state\": 42,\n",
        "    \"threshold\": 0.5,\n",
        "    \"model\": {\n",
        "        \"n_estimators\": 100,\n",
        "        \"max_depth\": 4\n",
        "    }\n",
        "}\n",
        "\n",
        "# Run pipeline\n",
        "print(\"=== Running Training Pipeline ===\")\n",
        "pipeline = TrainingPipeline(config)\n",
        "pipeline.run(data, registry, \"v1.0.0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Running Multiple Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run experiments with different configs\n",
        "experiments_to_run = [\n",
        "    {\"name\": \"baseline\", \"n_estimators\": 50, \"max_depth\": 3},\n",
        "    {\"name\": \"deeper\", \"n_estimators\": 100, \"max_depth\": 5},\n",
        "    {\"name\": \"more_trees\", \"n_estimators\": 200, \"max_depth\": 4},\n",
        "]\n",
        "\n",
        "print(\"=== Running Experiments ===\")\n",
        "for exp_config in experiments_to_run:\n",
        "    print(f\"\\nExperiment: {exp_config['name']}\")\n",
        "    \n",
        "    # Update config\n",
        "    config[\"model\"][\"n_estimators\"] = exp_config[\"n_estimators\"]\n",
        "    config[\"model\"][\"max_depth\"] = exp_config[\"max_depth\"]\n",
        "    \n",
        "    # Run pipeline\n",
        "    pipeline = TrainingPipeline(config)\n",
        "    pipeline.load_data(data).preprocess().train().evaluate()\n",
        "    \n",
        "    # Log experiment\n",
        "    experiment = Experiment(\n",
        "        name=exp_config[\"name\"],\n",
        "        timestamp=datetime.now().isoformat(),\n",
        "        params=exp_config,\n",
        "        metrics=pipeline.metrics,\n",
        "        data_hash=pipeline.data_hash\n",
        "    )\n",
        "    tracker.log(experiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compare experiments\n",
        "print(\"=== Experiment Comparison ===\")\n",
        "comparison = tracker.compare(\"auc\")\n",
        "print(comparison[[\"name\", \"n_estimators\", \"max_depth\", \"auc\", \"precision\", \"recall\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Model Serving (Batch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def batch_predict(model_path: str, data: pd.DataFrame, features: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Run batch predictions.\n",
        "    \n",
        "    Args:\n",
        "        model_path: Path to saved model\n",
        "        data: Input data\n",
        "        features: Feature columns to use\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with predictions\n",
        "    \"\"\"\n",
        "    # Load model\n",
        "    model = joblib.load(model_path)\n",
        "    \n",
        "    # Generate predictions\n",
        "    X = data[features]\n",
        "    probabilities = model.predict_proba(X)[:, 1]\n",
        "    \n",
        "    # Create results\n",
        "    results = data[[\"tenure\", \"spend\"]].copy()  # Include some identifying columns\n",
        "    results[\"churn_probability\"] = probabilities\n",
        "    results[\"predicted_at\"] = datetime.now().isoformat()\n",
        "    results[\"model_version\"] = model_path.split(\"_\")[-1].replace(\".joblib\", \"\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Test batch prediction\n",
        "print(\"=== Batch Prediction ===\")\n",
        "model_versions = registry.list_versions(\"churn_model\")\n",
        "if model_versions:\n",
        "    latest = model_versions[-1]\n",
        "    predictions = batch_predict(\n",
        "        latest[\"path\"], \n",
        "        data.head(10), \n",
        "        config[\"features\"]\n",
        "    )\n",
        "    print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Reproducibility Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def check_reproducibility(config: Dict, data: pd.DataFrame, n_runs: int = 3) -> bool:\n",
        "    \"\"\"\n",
        "    Verify that training is reproducible with the same config.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for i in range(n_runs):\n",
        "        pipeline = TrainingPipeline(config)\n",
        "        pipeline.load_data(data).preprocess().train().evaluate()\n",
        "        results.append(pipeline.metrics[\"auc\"])\n",
        "    \n",
        "    # Check if all results are identical\n",
        "    is_reproducible = len(set(results)) == 1\n",
        "    \n",
        "    print(f\"Reproducibility check ({n_runs} runs):\")\n",
        "    print(f\"  Results: {results}\")\n",
        "    print(f\"  Reproducible: {is_reproducible}\")\n",
        "    \n",
        "    return is_reproducible\n",
        "\n",
        "# Run check\n",
        "check_reproducibility(config, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: TODO - Create Your Pipeline\n",
        "\n",
        "Extend the pipeline with additional features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Add these capabilities to the pipeline:\n",
        "# 1. Feature validation (check for missing values, expected ranges)\n",
        "# 2. Model validation (minimum performance thresholds)\n",
        "# 3. Automated rollback if new model is worse\n",
        "\n",
        "print(\"Extend the pipeline with validation and rollback!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Check\n",
        "\n",
        "Run the cell below to verify your pipeline components are correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# SELF-CHECK: Verify your MLOps pipeline\n",
        "\n",
        "assert 'pipeline' in dir(), \"Pipeline should be defined\"\n",
        "assert hasattr(pipeline, 'metrics'), \"Pipeline should have metrics\"\n",
        "assert len(registry.list_versions(\"churn_model\")) >= 1, \"Should have registered at least one model\"\n",
        "assert len(tracker.experiments) >= 1, \"Should have logged at least one experiment\"\n",
        "print(f\"✅ Self-check passed! {len(registry.list_versions('churn_model'))} model versions, {len(tracker.experiments)} experiments\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Stakeholder Summary\n",
        "\n",
        "### TODO: Write a 3-bullet summary (~100 words) for the VP of Engineering\n",
        "\n",
        "Template:\n",
        "• **Why MLOps:** Without it, we can't [reproduce results / debug production issues / compare experiments]. Version control for ML.\n",
        "• **Reproducibility:** Same code + data + config = same model. Set random seeds, track data versions, log all hyperparameters.\n",
        "• **Minimum viable setup:** Start with [experiment tracking + model registry + config files]. Prevents \"it worked on my laptop\" syndrome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Version everything**: Code, data, models, config\n",
        "2. **Track experiments**: Parameters, metrics, artifacts\n",
        "3. **Use pipelines**: Modular, reproducible, testable\n",
        "4. **Register models**: With metadata for traceability\n",
        "5. **Set random seeds**: For reproducibility\n",
        "\n",
        "### Next Steps\n",
        "- Explore the interactive playground\n",
        "- Complete the quiz\n",
        "- Move to Module 22: Monitoring"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}