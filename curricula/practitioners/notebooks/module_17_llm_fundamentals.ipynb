{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 17: LLM Fundamentals\n",
        "\n",
        "**Goal:** Learn effective prompting techniques, understand RAG vs fine-tuning, and evaluate LLM outputs.\n",
        "\n",
        "**Prerequisites:** Module 14 (Retrieval), Module 16 (Transformers)\n",
        "\n",
        "**Expected Runtime:** ~25 minutes\n",
        "\n",
        "**Outputs:**\n",
        "- Compared prompting techniques\n",
        "- Built a simple RAG pipeline\n",
        "- Evaluated response quality\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Prompt Templates\n",
        "\n",
        "Different prompting styles for different needs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Task: Classify support tickets\n",
        "ticket = \"I can't log into my account after resetting my password\"\n",
        "\n",
        "# Zero-shot prompt\n",
        "zero_shot = f\"\"\"Classify this support ticket as: billing, technical, or general.\n",
        "\n",
        "Ticket: \"{ticket}\"\n",
        "\n",
        "Category:\"\"\"\n",
        "\n",
        "# Few-shot prompt\n",
        "few_shot = f\"\"\"Classify support tickets into categories.\n",
        "\n",
        "Examples:\n",
        "Ticket: \"Why was I charged twice this month?\"\n",
        "Category: billing\n",
        "\n",
        "Ticket: \"The app keeps crashing on startup\"\n",
        "Category: technical\n",
        "\n",
        "Ticket: \"What are your business hours?\"\n",
        "Category: general\n",
        "\n",
        "Now classify this ticket:\n",
        "Ticket: \"{ticket}\"\n",
        "Category:\"\"\"\n",
        "\n",
        "# Chain-of-thought prompt\n",
        "cot_prompt = f\"\"\"Classify this support ticket. Think step by step.\n",
        "\n",
        "Ticket: \"{ticket}\"\n",
        "\n",
        "Step 1: What is the customer's main issue?\n",
        "Step 2: Which category (billing, technical, general) best describes this?\n",
        "Step 3: Final classification.\n",
        "\n",
        "Analysis:\"\"\"\n",
        "\n",
        "print(\"=== Zero-Shot Prompt ===\")\n",
        "print(zero_shot)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"\\n=== Few-Shot Prompt ===\")\n",
        "print(few_shot[:300] + \"...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Simulating LLM Responses\n",
        "\n",
        "In production, you'd call an API. Here we'll simulate responses to understand the patterns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulated responses (what an LLM might return)\n",
        "responses = {\n",
        "    'zero_shot': 'technical',\n",
        "    'few_shot': 'technical',\n",
        "    'cot': '''Step 1: The customer is having trouble logging in after resetting their password. This is an account access issue.\n",
        "\n",
        "Step 2: Login and password issues are related to the technical functionality of the system, not billing or general inquiries.\n",
        "\n",
        "Step 3: technical'''\n",
        "}\n",
        "\n",
        "print(\"=== Simulated Responses ===\")\n",
        "for style, response in responses.items():\n",
        "    print(f\"\\n{style.upper()}:\")\n",
        "    print(response[:200] + (\"...\" if len(response) > 200 else \"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Building a Simple RAG System"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Knowledge base (support documentation)\n",
        "knowledge_base = [\n",
        "    {\n",
        "        \"title\": \"Password Reset Guide\",\n",
        "        \"content\": \"Password resets are processed within 5 minutes. If you can't log in after reset, clear your browser cache and cookies. Try incognito mode. If issues persist, contact support for manual account unlock.\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Billing FAQ\",\n",
        "        \"content\": \"Charges appear within 24 hours. Double charges may occur due to payment retries. Request refunds within 30 days. View billing history in Account Settings.\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Shipping Information\",\n",
        "        \"content\": \"Standard shipping takes 5-7 business days. Express is 2-3 days. Track orders in Order History. Free shipping on orders over $50.\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Account Security\",\n",
        "        \"content\": \"Enable two-factor authentication in Security Settings. Use strong passwords with 12+ characters. We never ask for passwords via email.\"\n",
        "    },\n",
        "]\n",
        "\n",
        "# Create document texts\n",
        "docs = [f\"{d['title']}: {d['content']}\" for d in knowledge_base]\n",
        "\n",
        "# Build retriever using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "doc_vectors = vectorizer.fit_transform(docs)\n",
        "\n",
        "def retrieve(query, k=2):\n",
        "    \"\"\"Retrieve top-k relevant documents.\"\"\"\n",
        "    query_vector = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_vector, doc_vectors).flatten()\n",
        "    top_k_idx = similarities.argsort()[-k:][::-1]\n",
        "    \n",
        "    results = []\n",
        "    for idx in top_k_idx:\n",
        "        results.append({\n",
        "            'title': knowledge_base[idx]['title'],\n",
        "            'content': knowledge_base[idx]['content'],\n",
        "            'score': similarities[idx]\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# Test retrieval\n",
        "query = \"I can't log in after password reset\"\n",
        "retrieved = retrieve(query, k=2)\n",
        "\n",
        "print(f\"Query: '{query}'\\n\")\n",
        "print(\"Retrieved Documents:\")\n",
        "for doc in retrieved:\n",
        "    print(f\"\\n[{doc['score']:.3f}] {doc['title']}\")\n",
        "    print(f\"   {doc['content'][:100]}...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_rag_prompt(query, retrieved_docs):\n",
        "    \"\"\"Build a RAG-augmented prompt.\"\"\"\n",
        "    context = \"\\n\\n\".join([f\"- {d['title']}: {d['content']}\" for d in retrieved_docs])\n",
        "    \n",
        "    prompt = f\"\"\"You are a helpful support agent. Answer the customer's question using ONLY the information provided below.\n",
        "\n",
        "RETRIEVED CONTEXT:\n",
        "{context}\n",
        "\n",
        "CUSTOMER QUESTION: {query}\n",
        "\n",
        "If the answer is not in the context, say \"I don't have that information.\"\n",
        "\n",
        "RESPONSE:\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "# Build RAG prompt\n",
        "rag_prompt = build_rag_prompt(query, retrieved)\n",
        "print(\"=== RAG Prompt ===\")\n",
        "print(rag_prompt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Evaluating LLM Outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulated responses to evaluate\n",
        "test_cases = [\n",
        "    {\n",
        "        'query': 'How long does shipping take?',\n",
        "        'context': 'Standard shipping takes 5-7 business days. Express is 2-3 days.',\n",
        "        'response': 'Standard shipping takes 5-7 business days, and express shipping delivers in 2-3 days.',\n",
        "        'expected': 'Standard shipping: 5-7 days, Express: 2-3 days'\n",
        "    },\n",
        "    {\n",
        "        'query': 'How long does shipping take?',\n",
        "        'context': 'Standard shipping takes 5-7 business days. Express is 2-3 days.',\n",
        "        'response': 'Shipping usually takes 1-2 weeks, but premium members get it in 24 hours.',  # Hallucinated!\n",
        "        'expected': 'Standard shipping: 5-7 days, Express: 2-3 days'\n",
        "    },\n",
        "    {\n",
        "        'query': 'What is the refund policy?',\n",
        "        'context': 'Request refunds within 30 days.',\n",
        "        'response': 'You can request a refund within 30 days of purchase.',\n",
        "        'expected': 'Refunds available within 30 days'\n",
        "    },\n",
        "]\n",
        "\n",
        "def check_faithfulness(response, context):\n",
        "    \"\"\"Simple faithfulness check: does response contain claims not in context?\"\"\"\n",
        "    # In production, use more sophisticated NLI models\n",
        "    response_words = set(response.lower().split())\n",
        "    context_words = set(context.lower().split())\n",
        "    \n",
        "    # Check for numbers in response not in context (common hallucination)\n",
        "    import re\n",
        "    response_numbers = set(re.findall(r'\\d+', response))\n",
        "    context_numbers = set(re.findall(r'\\d+', context))\n",
        "    \n",
        "    hallucinated_numbers = response_numbers - context_numbers\n",
        "    \n",
        "    return len(hallucinated_numbers) == 0, hallucinated_numbers\n",
        "\n",
        "print(\"=== Faithfulness Evaluation ===\")\n",
        "for i, case in enumerate(test_cases):\n",
        "    faithful, issues = check_faithfulness(case['response'], case['context'])\n",
        "    \n",
        "    print(f\"\\nCase {i+1}: {'✓ Faithful' if faithful else '✗ Hallucination Detected'}\")\n",
        "    print(f\"  Query: {case['query']}\")\n",
        "    print(f\"  Response: {case['response'][:60]}...\")\n",
        "    if not faithful:\n",
        "        print(f\"  ⚠️ Numbers not in context: {issues}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: LLM-as-Judge Pattern"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_judge_prompt(query, response, context=None):\n",
        "    \"\"\"Create a prompt for LLM-as-judge evaluation.\"\"\"\n",
        "    \n",
        "    context_section = f\"\\nCONTEXT PROVIDED:\\n{context}\\n\" if context else \"\"\n",
        "    \n",
        "    prompt = f\"\"\"Evaluate this response on a scale of 1-5 for each criterion.\n",
        "\n",
        "QUERY: {query}\n",
        "{context_section}\n",
        "RESPONSE: {response}\n",
        "\n",
        "Rate the response:\n",
        "1. RELEVANCE (1-5): Does it answer the question?\n",
        "2. ACCURACY (1-5): Is the information correct?\n",
        "3. HELPFULNESS (1-5): Is it actionable and useful?\n",
        "4. FAITHFULNESS (1-5): Does it stick to provided context? (if context given)\n",
        "\n",
        "Provide scores in JSON format:\n",
        "{{\n",
        "    \"relevance\": <score>,\n",
        "    \"accuracy\": <score>,\n",
        "    \"helpfulness\": <score>,\n",
        "    \"faithfulness\": <score>,\n",
        "    \"reasoning\": \"<brief explanation>\"\n",
        "}}\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "# Example judge prompt\n",
        "judge_prompt = create_judge_prompt(\n",
        "    query=\"How do I reset my password?\",\n",
        "    response=\"You can reset your password by clicking 'Forgot Password' on the login page. A reset link will be sent to your email.\",\n",
        "    context=\"Password resets are processed within 5 minutes. Click 'Forgot Password' on login page.\"\n",
        ")\n",
        "\n",
        "print(\"=== LLM-as-Judge Prompt ===\")\n",
        "print(judge_prompt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: RAG vs Fine-Tuning Decision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Decision framework\n",
        "scenarios = [\n",
        "    {\n",
        "        'scenario': 'Company FAQ that changes monthly',\n",
        "        'recommendation': 'RAG',\n",
        "        'reason': 'Information changes frequently; RAG allows instant updates without retraining'\n",
        "    },\n",
        "    {\n",
        "        'scenario': 'Consistent brand voice across all responses',\n",
        "        'recommendation': 'Fine-tune',\n",
        "        'reason': 'Style/tone is a learned behavior, not retrievable information'\n",
        "    },\n",
        "    {\n",
        "        'scenario': 'Customer order status lookup',\n",
        "        'recommendation': 'RAG',\n",
        "        'reason': 'Real-time data that must come from database, not training'\n",
        "    },\n",
        "    {\n",
        "        'scenario': 'Technical domain expertise (medical, legal)',\n",
        "        'recommendation': 'Fine-tune + RAG',\n",
        "        'reason': 'Need specialized knowledge (fine-tune) AND current facts (RAG)'\n",
        "    },\n",
        "    {\n",
        "        'scenario': 'Specific output format (JSON, structured)',\n",
        "        'recommendation': 'Fine-tune or few-shot',\n",
        "        'reason': 'Format consistency is behavioral; fine-tuning or examples work well'\n",
        "    },\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(scenarios)\n",
        "print(\"=== RAG vs Fine-Tuning Decision Guide ===\")\n",
        "print(df.to_string(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: TODO - Build a Complete RAG Flow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Implement a full RAG pipeline\n",
        "def rag_answer(query, knowledge_base_docs=None, top_k=2):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline:\n",
        "    1. Retrieve relevant documents\n",
        "    2. Build augmented prompt\n",
        "    3. (In production) Call LLM API\n",
        "    4. Return response\n",
        "    \"\"\"\n",
        "    # Step 1: Retrieve\n",
        "    retrieved = retrieve(query, k=top_k)\n",
        "    \n",
        "    # Step 2: Build prompt\n",
        "    prompt = build_rag_prompt(query, retrieved)\n",
        "    \n",
        "    # Step 3: Call LLM (simulated)\n",
        "    # In production:\n",
        "    # response = openai.ChatCompletion.create(\n",
        "    #     model=\"gpt-4\",\n",
        "    #     messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    # )\n",
        "    \n",
        "    # Simulated response\n",
        "    simulated_response = \"Based on the information provided, here's my answer...\"\n",
        "    \n",
        "    return {\n",
        "        'query': query,\n",
        "        'retrieved_docs': retrieved,\n",
        "        'prompt': prompt,\n",
        "        'response': simulated_response\n",
        "    }\n",
        "\n",
        "# Test the pipeline\n",
        "result = rag_answer(\"How do I get a refund?\")\n",
        "print(\"=== RAG Pipeline Result ===\")\n",
        "print(f\"Query: {result['query']}\")\n",
        "print(f\"\\nRetrieved: {[d['title'] for d in result['retrieved_docs']]}\")\n",
        "print(f\"\\nPrompt length: {len(result['prompt'])} chars\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Check\n",
        "\n",
        "Uncomment and run the asserts below to verify your RAG pipeline components work correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# SELF-CHECK: Verify your RAG pipeline\n",
        "assert callable(retrieve), \"retrieve function should exist\"\n",
        "results = retrieve(\"password reset\", k=2)\n",
        "assert len(results) == 2, \"retrieve should return k results\"\n",
        "assert all('title' in r and 'score' in r for r in results), \"Results should have title and score\"\n",
        "assert callable(build_rag_prompt), \"build_rag_prompt function should exist\"\n",
        "print(f\"✅ Self-check passed! RAG pipeline retrieved {len(results)} documents\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Stakeholder Summary\n",
        "\n",
        "### TODO: Write a 3-bullet summary (~100 words) for the PM\n",
        "\n",
        "Template:\n",
        "• **Three approaches:** Prompting (fast, no training), RAG (retrieves facts at query time), Fine-tuning (trains model on your data).\n",
        "• **When to use each:** RAG for [current/changing facts]; Fine-tuning for [consistent style/domain expertise]; Prompting for [quick experiments].\n",
        "• **Evaluation:** Check faithfulness (does it stick to context?), relevance (does it answer the question?), and [specific business metrics]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Summary:\n",
        "\n",
        "*Write your explanation here...*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Prompt engineering:** Zero-shot, few-shot, and CoT for different needs\n",
        "2. **RAG:** Retrieve context to ground responses in facts\n",
        "3. **Fine-tuning:** For style, tone, and specialized behavior\n",
        "4. **Evaluation:** Faithfulness, relevance, accuracy; use LLM-as-judge\n",
        "5. **Hallucination:** Address with RAG and \"use only provided info\"\n",
        "\n",
        "### Next Steps\n",
        "- Explore the interactive playground\n",
        "- Complete the quiz\n",
        "- Move to Module 18: Tool Calling"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}