{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 14: Retrieval\n",
        "\n",
        "**Goal:** Build a retrieval system that finds relevant documents using semantic search.\n",
        "\n",
        "**Prerequisites:** Module 12 (Embeddings)\n",
        "\n",
        "**Expected Runtime:** ~25 minutes\n",
        "\n",
        "**Outputs:**\n",
        "- Keyword vs semantic search comparison\n",
        "- Retrieval evaluation metrics\n",
        "- Hybrid search implementation\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "plt.rcParams['figure.figsize'] = (12, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Create Document Corpus\n",
        "\n",
        "Sample support articles for retrieval."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sample support knowledge base\n",
        "documents = [\n",
        "    {\"id\": 1, \"title\": \"Refund Policy\", \n",
        "     \"content\": \"We offer full refunds within 30 days of purchase. To request a refund, go to Order History and select the item.\"},\n",
        "    {\"id\": 2, \"title\": \"How to Cancel Subscription\", \n",
        "     \"content\": \"Cancel your subscription from Account Settings. Click Manage Subscription then Cancel. You'll retain access until the end of your billing period.\"},\n",
        "    {\"id\": 3, \"title\": \"Password Reset Guide\", \n",
        "     \"content\": \"Reset your password by clicking Forgot Password on the login page. We'll send a reset link to your email.\"},\n",
        "    {\"id\": 4, \"title\": \"Change Login Credentials\", \n",
        "     \"content\": \"Update your email or password in Account Settings. For security, you'll need to confirm your current password.\"},\n",
        "    {\"id\": 5, \"title\": \"Shipping and Delivery\", \n",
        "     \"content\": \"Standard shipping takes 5-7 business days. Express shipping delivers in 2-3 days. Track your order in Order History.\"},\n",
        "    {\"id\": 6, \"title\": \"Return an Item\", \n",
        "     \"content\": \"Start a return from Order History. Select the item, choose Return, and print the prepaid shipping label. Money back within 5-10 days.\"},\n",
        "    {\"id\": 7, \"title\": \"Two-Factor Authentication\", \n",
        "     \"content\": \"Enable 2FA in Security Settings for extra protection. Use an authenticator app or SMS verification.\"},\n",
        "    {\"id\": 8, \"title\": \"Payment Methods\", \n",
        "     \"content\": \"We accept credit cards, debit cards, and PayPal. Add or update payment methods in Billing Settings.\"},\n",
        "    {\"id\": 9, \"title\": \"Contact Support\", \n",
        "     \"content\": \"Reach our team via live chat, email, or phone. Business hours: Monday-Friday 9AM-6PM EST.\"},\n",
        "    {\"id\": 10, \"title\": \"Membership Benefits\", \n",
        "     \"content\": \"Premium members get free shipping, exclusive discounts, and early access to sales. Upgrade in Subscription Settings.\"},\n",
        "]\n",
        "\n",
        "# Create test queries with ground truth\n",
        "test_queries = [\n",
        "    {\"query\": \"how to get my money back\", \"relevant\": [1, 6]},\n",
        "    {\"query\": \"reset password\", \"relevant\": [3, 4]},\n",
        "    {\"query\": \"end my subscription\", \"relevant\": [2]},\n",
        "    {\"query\": \"shipping time\", \"relevant\": [5]},\n",
        "    {\"query\": \"secure my account\", \"relevant\": [7, 4]},\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(documents)\n",
        "df['text'] = df['title'] + ' ' + df['content']\n",
        "\n",
        "print(f\"Corpus: {len(df)} documents\")\n",
        "df[['id', 'title']].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Keyword Search (BM25-style)\n",
        "\n",
        "Simple TF-IDF based keyword matching."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build TF-IDF index for keyword search\n",
        "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
        "tfidf_matrix = tfidf.fit_transform(df['text'])\n",
        "\n",
        "def keyword_search(query, k=5):\n",
        "    \"\"\"Search using TF-IDF similarity (proxy for BM25).\"\"\"\n",
        "    query_vec = tfidf.transform([query])\n",
        "    scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
        "    top_k_idx = scores.argsort()[::-1][:k]\n",
        "    return [(df.iloc[i]['id'], scores[i]) for i in top_k_idx]\n",
        "\n",
        "# Test keyword search\n",
        "query = \"reset password\"\n",
        "results = keyword_search(query, k=5)\n",
        "\n",
        "print(f\"Query: '{query}'\\n\")\n",
        "print(\"Keyword Search Results:\")\n",
        "for doc_id, score in results:\n",
        "    title = df[df['id'] == doc_id]['title'].values[0]\n",
        "    print(f\"  [{score:.3f}] {doc_id}. {title}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Semantic Search (Embeddings)\n",
        "\n",
        "Using TF-IDF as simple embeddings (in production, use sentence-transformers)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Semantic embeddings - two options:\n",
        "\n",
        "# OPTION 1: Production-quality with sentence-transformers\n",
        "# Install: pip install sentence-transformers\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# model = SentenceTransformer('all-MiniLM-L6-v2')  # ~80MB, 384 dims\n",
        "# semantic_embeddings = model.encode(df['text'].tolist())\n",
        "\n",
        "# OPTION 2: Demo-friendly with TF-IDF + SVD (no extra install)\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "svd = TruncatedSVD(n_components=50, random_state=42)\n",
        "semantic_embeddings = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "print(\"Using TF-IDF + SVD for semantic embeddings (demo)\")\n",
        "print(\"For production, install sentence-transformers:\")\n",
        "print(\"  pip install sentence-transformers\")\n",
        "print(\"  model = SentenceTransformer('all-MiniLM-L6-v2')\")\n",
        "print(\"  embeddings = model.encode(texts)\")\n",
        "\n",
        "def semantic_search(query, k=5):\n",
        "    \"\"\"Search using semantic embeddings.\"\"\"\n",
        "    query_vec = tfidf.transform([query])\n",
        "    query_emb = svd.transform(query_vec)\n",
        "    scores = cosine_similarity(query_emb, semantic_embeddings).flatten()\n",
        "    top_k_idx = scores.argsort()[::-1][:k]\n",
        "    return [(df.iloc[i]['id'], scores[i]) for i in top_k_idx]\n",
        "\n",
        "# Test semantic search\n",
        "query = \"how to get my money back\"  # Uses \"money back\" instead of \"refund\"\n",
        "print(f\"Query: '{query}'\\n\")\n",
        "\n",
        "print(\"Keyword Search:\")\n",
        "for doc_id, score in keyword_search(query, k=3):\n",
        "    title = df[df['id'] == doc_id]['title'].values[0]\n",
        "    print(f\"  [{score:.3f}] {doc_id}. {title}\")\n",
        "\n",
        "print(\"\\nSemantic Search:\")\n",
        "for doc_id, score in semantic_search(query, k=3):\n",
        "    title = df[df['id'] == doc_id]['title'].values[0]\n",
        "    print(f\"  [{score:.3f}] {doc_id}. {title}\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Semantic search finds 'Refund Policy' even without the word 'refund' in the query!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Hybrid Search\n",
        "\n",
        "Combine keyword and semantic for best of both."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def hybrid_search(query, k=5, alpha=0.5):\n",
        "    \"\"\"Combine keyword and semantic search.\n",
        "    \n",
        "    alpha: weight for keyword (1-alpha for semantic)\n",
        "    \"\"\"\n",
        "    # Get keyword scores\n",
        "    query_vec = tfidf.transform([query])\n",
        "    keyword_scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
        "    \n",
        "    # Get semantic scores\n",
        "    query_emb = svd.transform(query_vec)\n",
        "    semantic_scores = cosine_similarity(query_emb, semantic_embeddings).flatten()\n",
        "    \n",
        "    # Normalize to [0, 1]\n",
        "    if keyword_scores.max() > 0:\n",
        "        keyword_scores = keyword_scores / keyword_scores.max()\n",
        "    if semantic_scores.max() > 0:\n",
        "        semantic_scores = semantic_scores / semantic_scores.max()\n",
        "    \n",
        "    # Combine\n",
        "    combined = alpha * keyword_scores + (1 - alpha) * semantic_scores\n",
        "    top_k_idx = combined.argsort()[::-1][:k]\n",
        "    \n",
        "    return [(df.iloc[i]['id'], combined[i], keyword_scores[i], semantic_scores[i]) \n",
        "            for i in top_k_idx]\n",
        "\n",
        "# Compare all methods\n",
        "query = \"end my membership\"  # Neither \"cancel\" nor \"subscription\" appear\n",
        "\n",
        "print(f\"Query: '{query}'\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for method, results in [\n",
        "    (\"Keyword\", keyword_search(query, k=3)),\n",
        "    (\"Semantic\", semantic_search(query, k=3)),\n",
        "    (\"Hybrid\", [(r[0], r[1]) for r in hybrid_search(query, k=3, alpha=0.3)])\n",
        "]:\n",
        "    print(f\"\\n{method} Search:\")\n",
        "    for doc_id, score in results:\n",
        "        title = df[df['id'] == doc_id]['title'].values[0]\n",
        "        print(f\"  [{score:.3f}] {doc_id}. {title}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Evaluation Metrics\n",
        "\n",
        "Measure retrieval quality with Precision, Recall, and MRR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def precision_at_k(retrieved, relevant, k):\n",
        "    \"\"\"What fraction of top K are relevant?\"\"\"\n",
        "    top_k = [r[0] for r in retrieved[:k]]\n",
        "    hits = len(set(top_k) & set(relevant))\n",
        "    return hits / k\n",
        "\n",
        "def recall_at_k(retrieved, relevant, k):\n",
        "    \"\"\"What fraction of relevant are in top K?\"\"\"\n",
        "    top_k = [r[0] for r in retrieved[:k]]\n",
        "    hits = len(set(top_k) & set(relevant))\n",
        "    return hits / len(relevant) if relevant else 0\n",
        "\n",
        "def mrr(retrieved, relevant):\n",
        "    \"\"\"Mean Reciprocal Rank - where is the first relevant result?\"\"\"\n",
        "    for i, (doc_id, _) in enumerate(retrieved):\n",
        "        if doc_id in relevant:\n",
        "            return 1.0 / (i + 1)\n",
        "    return 0\n",
        "\n",
        "# Evaluate all methods on test queries\n",
        "results_table = []\n",
        "\n",
        "for test in test_queries:\n",
        "    query = test['query']\n",
        "    relevant = test['relevant']\n",
        "    k = 5\n",
        "    \n",
        "    for method_name, search_fn in [\n",
        "        ('Keyword', keyword_search),\n",
        "        ('Semantic', semantic_search),\n",
        "        ('Hybrid', lambda q, k: [(r[0], r[1]) for r in hybrid_search(q, k, alpha=0.3)])\n",
        "    ]:\n",
        "        retrieved = search_fn(query, k)\n",
        "        \n",
        "        results_table.append({\n",
        "            'Query': query[:25] + '...' if len(query) > 25 else query,\n",
        "            'Method': method_name,\n",
        "            'P@5': precision_at_k(retrieved, relevant, k),\n",
        "            'R@5': recall_at_k(retrieved, relevant, k),\n",
        "            'MRR': mrr(retrieved, relevant)\n",
        "        })\n",
        "\n",
        "results_df = pd.DataFrame(results_table)\n",
        "print(\"=== Evaluation Results ===\")\n",
        "print(results_df.to_string(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Aggregate by method\n",
        "summary = results_df.groupby('Method')[['P@5', 'R@5', 'MRR']].mean().round(3)\n",
        "print(\"\\n=== Average Metrics by Method ===\")\n",
        "print(summary)\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "x = np.arange(3)\n",
        "width = 0.25\n",
        "\n",
        "methods = ['Keyword', 'Semantic', 'Hybrid']\n",
        "colors = ['#ef4444', '#22c55e', '#8b5cf6']\n",
        "\n",
        "for i, method in enumerate(methods):\n",
        "    values = summary.loc[method].values\n",
        "    ax.bar(x + i * width, values, width, label=method, color=colors[i])\n",
        "\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Retrieval Method Comparison')\n",
        "ax.set_xticks(x + width)\n",
        "ax.set_xticklabels(['Precision@5', 'Recall@5', 'MRR'])\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 1)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Chunking for RAG\n",
        "\n",
        "How to split documents for retrieval-augmented generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example long document\n",
        "long_doc = \"\"\"\n",
        "# Complete Guide to Account Management\n",
        "\n",
        "## Password and Security\n",
        "Your account security is our top priority. We recommend using a strong password \n",
        "with at least 12 characters, including uppercase, lowercase, numbers, and symbols.\n",
        "Enable two-factor authentication for additional protection.\n",
        "\n",
        "## Subscription Management\n",
        "You can upgrade, downgrade, or cancel your subscription at any time from the \n",
        "Account Settings page. Premium members enjoy benefits like free shipping and \n",
        "exclusive discounts. Changes take effect at the start of your next billing cycle.\n",
        "\n",
        "## Billing and Payments\n",
        "We accept all major credit cards, debit cards, and PayPal. View your billing \n",
        "history and update payment methods in Billing Settings. For refunds, please \n",
        "contact support within 30 days of purchase.\n",
        "\n",
        "## Contact Us\n",
        "Our support team is available Monday through Friday, 9 AM to 6 PM EST. \n",
        "Reach us via live chat, email at support@example.com, or call 1-800-EXAMPLE.\n",
        "\"\"\"\n",
        "\n",
        "def chunk_by_paragraph(text, overlap_sentences=1):\n",
        "    \"\"\"Split text into chunks by paragraph with overlap.\"\"\"\n",
        "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "    chunks = []\n",
        "    \n",
        "    for i, para in enumerate(paragraphs):\n",
        "        chunk = para\n",
        "        \n",
        "        # Add overlap from previous paragraph\n",
        "        if i > 0 and overlap_sentences > 0:\n",
        "            prev_sentences = paragraphs[i-1].split('. ')[-overlap_sentences:]\n",
        "            chunk = '. '.join(prev_sentences) + '... ' + chunk\n",
        "        \n",
        "        chunks.append({\n",
        "            'chunk_id': i,\n",
        "            'text': chunk,\n",
        "            'length': len(chunk.split())\n",
        "        })\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_by_paragraph(long_doc)\n",
        "\n",
        "print(f\"Document split into {len(chunks)} chunks:\\n\")\n",
        "for chunk in chunks:\n",
        "    print(f\"Chunk {chunk['chunk_id']} ({chunk['length']} words):\")\n",
        "    print(f\"  {chunk['text'][:80]}...\")\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: TODO - Implement Reranking\n",
        "\n",
        "Add a second-stage reranker for better precision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Implement a simple reranking function\n",
        "# In production, use a cross-encoder model like sentence-transformers/ms-marco-MiniLM-L-6-v2\n",
        "\n",
        "def rerank(query, candidates, top_k=3):\n",
        "    \"\"\"\n",
        "    Rerank candidates based on query-document relevance.\n",
        "    \n",
        "    This simple implementation uses word overlap scoring.\n",
        "    For production: use a cross-encoder that takes (query, doc) pairs.\n",
        "    \n",
        "    Args:\n",
        "        query: search query\n",
        "        candidates: list of (doc_id, initial_score) from retrieval\n",
        "        top_k: number of results to return\n",
        "    \n",
        "    Returns:\n",
        "        reranked list of (doc_id, score)\n",
        "    \"\"\"\n",
        "    query_words = set(query.lower().split())\n",
        "    scored = []\n",
        "    \n",
        "    for doc_id, initial_score in candidates:\n",
        "        # Get document text\n",
        "        doc_text = df[df['id'] == doc_id]['text'].values[0].lower()\n",
        "        doc_words = set(doc_text.split())\n",
        "        \n",
        "        # Compute word overlap as simple relevance boost\n",
        "        overlap = len(query_words & doc_words)\n",
        "        \n",
        "        # Combine initial score with overlap boost\n",
        "        rerank_score = initial_score * 0.7 + (overlap / max(len(query_words), 1)) * 0.3\n",
        "        scored.append((doc_id, rerank_score))\n",
        "    \n",
        "    # Sort by new score\n",
        "    scored.sort(key=lambda x: x[1], reverse=True)\n",
        "    return scored[:top_k]\n",
        "\n",
        "# Test reranking\n",
        "query = \"how to secure my account\"\n",
        "initial_results = semantic_search(query, k=10)\n",
        "reranked = rerank(query, initial_results, top_k=3)\n",
        "\n",
        "print(f\"Query: '{query}'\")\n",
        "print(\"\\nInitial Top 5:\")\n",
        "for doc_id, score in initial_results[:5]:\n",
        "    print(f\"  [{score:.3f}] {df[df['id']==doc_id]['title'].values[0]}\")\n",
        "\n",
        "print(\"\\nAfter Reranking:\")\n",
        "for doc_id, score in reranked:\n",
        "    print(f\"  [{score:.3f}] {df[df['id']==doc_id]['title'].values[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Check\n",
        "\n",
        "Run the cell below to verify your retrieval system and evaluation metrics are correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Check\n",
        "\n",
        "Uncomment and run the asserts below to verify your retrieval system works correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# SELF-CHECK: Verify your retrieval system\n",
        "results = semantic_search(\"password reset\", k=3)\n",
        "assert len(results) == 3, \"semantic_search should return k results\"\n",
        "assert all(0 <= score <= 1 for _, score in results), \"Similarity scores should be in [0, 1]\"\n",
        "reranked_test = rerank(\"password reset\", results, top_k=3)\n",
        "assert len(reranked_test) == 3, \"rerank should return top_k results\"\n",
        "print(f\"âœ… Self-check passed! Retrieved {len(results)} results, reranked to {len(reranked_test)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Stakeholder Summary\n",
        "\n",
        "### TODO: Write a 3-bullet summary (~100 words) for the PM\n",
        "\n",
        "Template:\n",
        "â€¢ **Keyword vs Semantic:** Keyword search matches exact words; semantic search understands meaning (e.g., \"money back\" finds \"refund\").\n",
        "â€¢ **Quality metrics:** We measure Precision@K (accuracy of top results), Recall@K (coverage), and MRR (how fast we find relevant docs).\n",
        "â€¢ **Recommendation:** Use hybrid search (alpha=____) to combine keyword precision with semantic understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Summary:\n",
        "\n",
        "*Write your explanation here...*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Keyword search** is fast and precise for exact matches\n",
        "2. **Semantic search** understands meaning and synonyms\n",
        "3. **Hybrid search** combines both for best results\n",
        "4. **Evaluation metrics:** Precision@K, Recall@K, MRR\n",
        "5. **Chunking matters** for RAG quality\n",
        "\n",
        "### Next Steps\n",
        "- Explore the interactive playground\n",
        "- Complete the quiz\n",
        "- Try sentence-transformers for production-quality semantic search"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}