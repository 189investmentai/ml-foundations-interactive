{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 4: Logistic Regression for Churn Prediction\n",
        "\n",
        "**Goal:** Build a logistic regression model to predict customer churn, then optimize the classification threshold for business impact.\n",
        "\n",
        "**Prerequisites:** Module 3 (Linear Regression)\n",
        "\n",
        "**Expected Runtime:** ~45 minutes\n",
        "\n",
        "**Outputs:**\n",
        "- Fitted logistic regression with probability interpretation\n",
        "- Precision/recall/F1 tradeoff analysis\n",
        "- Cost-optimized classification threshold\n",
        "- Stakeholder summary of churn model results\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ“ Libraries loaded\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Explore Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DATA_URL = 'https://raw.githubusercontent.com/189investmentai/ml-foundations-interactive/main/shared/data/'\n",
        "\n",
        "customers = pd.read_csv(DATA_URL + 'streamcart_customers.csv')\n",
        "print(f\"Loaded {len(customers)} customers\")\n",
        "customers.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check the target: churn_30d\n",
        "churn_rate = customers['churn_30d'].mean()\n",
        "print(f\"\\nChurn rate: {churn_rate:.1%}\")\n",
        "print(f\"Churned: {customers['churn_30d'].sum()}\")\n",
        "print(f\"Retained: {(1 - customers['churn_30d']).sum()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Self-Check: Is this imbalanced?\n",
        "\n",
        "If the churn rate is below 20%, we have class imbalance. Keep this in mind when evaluating metrics!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Select features for churn prediction\n",
        "feature_cols = ['tenure_days', 'orders_total', 'total_spend', 'support_tickets_total', 'avg_order_value']\n",
        "\n",
        "# Check if columns exist, create if needed\n",
        "if 'tenure_days' not in customers.columns:\n",
        "    customers['tenure_days'] = (pd.to_datetime('2024-01-01') - pd.to_datetime(customers['signup_date'])).dt.days\n",
        "if 'avg_order_value' not in customers.columns:\n",
        "    customers['avg_order_value'] = customers['total_spend'] / customers['orders_total'].replace(0, 1)\n",
        "\n",
        "# Filter to available columns\n",
        "available_features = [c for c in feature_cols if c in customers.columns]\n",
        "print(f\"Using features: {available_features}\")\n",
        "\n",
        "X = customers[available_features].fillna(0)\n",
        "y = customers['churn_30d']\n",
        "\n",
        "print(f\"\\nFeature matrix shape: {X.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {len(X_train)} customers ({y_train.mean():.1%} churn)\")\n",
        "print(f\"Test set: {len(X_test)} customers ({y_test.mean():.1%} churn)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why stratify?** This ensures the churn rate is the same in train and test sets. Important for imbalanced data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Baseline: Always Predict Majority Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# What if we always predict \"no churn\"?\n",
        "baseline_preds = np.zeros(len(y_test))\n",
        "\n",
        "print(\"Baseline (always predict 'no churn'):\")\n",
        "print(f\"  Accuracy: {accuracy_score(y_test, baseline_preds):.1%}\")\n",
        "print(f\"  Precision: {precision_score(y_test, baseline_preds, zero_division=0):.1%}\")\n",
        "print(f\"  Recall: {recall_score(y_test, baseline_preds):.1%}\")\n",
        "print(f\"  F1: {f1_score(y_test, baseline_preds):.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Self-Check: The Accuracy Trap\n",
        "\n",
        "Notice how baseline accuracy might be high (if churn rate is low), but recall is 0% - we catch zero churners!\n",
        "\n",
        "**Lesson:** Don't trust accuracy alone for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Fit Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fit logistic regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"âœ“ Model trained\")\n",
        "print(f\"\\nIntercept: {model.intercept_[0]:.4f}\")\n",
        "print(\"\\nCoefficients:\")\n",
        "for name, coef in zip(available_features, model.coef_[0]):\n",
        "    odds_ratio = np.exp(coef)\n",
        "    print(f\"  {name}: {coef:.4f} (odds ratio: {odds_ratio:.2f})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpreting Coefficients\n",
        "\n",
        "- **Positive coefficient:** Increases probability of churn\n",
        "- **Negative coefficient:** Decreases probability of churn\n",
        "- **Odds ratio:** Each unit increase in the feature multiplies the odds by this factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Get Probabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get probability of churn (class 1)\n",
        "probabilities = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Probability distribution:\")\n",
        "print(f\"  Min: {probabilities.min():.3f}\")\n",
        "print(f\"  Max: {probabilities.max():.3f}\")\n",
        "print(f\"  Mean: {probabilities.mean():.3f}\")\n",
        "print(f\"  Median: {np.median(probabilities):.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize probability distributions by actual class\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "ax.hist(probabilities[y_test == 0], bins=30, alpha=0.6, label='Actual: Retained', color='#3b82f6')\n",
        "ax.hist(probabilities[y_test == 1], bins=30, alpha=0.6, label='Actual: Churned', color='#ef4444')\n",
        "ax.axvline(x=0.5, color='#f59e0b', linestyle='--', linewidth=2, label='Threshold = 0.5')\n",
        "\n",
        "ax.set_xlabel('Predicted Probability of Churn')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_title('Probability Distributions by Actual Class')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What to look for:**\n",
        "- Good model: Blue (retained) peaks LEFT, Red (churned) peaks RIGHT\n",
        "- Perfect model: No overlap between distributions\n",
        "- Poor model: Distributions overlap completely"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate with Default Threshold (0.5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Default predictions (threshold = 0.5)\n",
        "default_preds = model.predict(X_test)\n",
        "\n",
        "print(\"Default threshold (0.5):\")\n",
        "print(f\"  Accuracy: {accuracy_score(y_test, default_preds):.1%}\")\n",
        "print(f\"  Precision: {precision_score(y_test, default_preds):.1%}\")\n",
        "print(f\"  Recall: {recall_score(y_test, default_preds):.1%}\")\n",
        "print(f\"  F1: {f1_score(y_test, default_preds):.3f}\")\n",
        "print(f\"  AUC: {roc_auc_score(y_test, probabilities):.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, default_preds)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(f\"                 Predicted\")\n",
        "print(f\"                 No    Yes\")\n",
        "print(f\"Actual No      {tn:4d}  {fp:4d}  (True Neg / False Pos)\")\n",
        "print(f\"Actual Yes     {fn:4d}  {tp:4d}  (False Neg / True Pos)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. TODO: Find the Optimal Threshold\n",
        "\n",
        "The business context:\n",
        "- **Cost of False Positive:** $50 (wasted retention offer)\n",
        "- **Cost of False Negative:** $200 (lost customer lifetime value)\n",
        "\n",
        "Your task: Find the threshold that minimizes total cost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cost calculation function\n",
        "def calculate_total_cost(y_true, y_pred, fp_cost=50, fn_cost=200):\n",
        "    \"\"\"\n",
        "    Calculate total business cost from predictions.\n",
        "    \n",
        "    Args:\n",
        "        y_true: Actual labels\n",
        "        y_pred: Predicted labels\n",
        "        fp_cost: Cost per false positive ($50 = wasted retention offer)\n",
        "        fn_cost: Cost per false negative ($200 = lost customer)\n",
        "    \n",
        "    Returns:\n",
        "        Total cost\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    \n",
        "    # Total cost = FP cost + FN cost\n",
        "    total_cost = fp * fp_cost + fn * fn_cost\n",
        "    \n",
        "    return total_cost\n",
        "\n",
        "# Test the function\n",
        "test_cost = calculate_total_cost(y_test, default_preds)\n",
        "print(f\"Test: Cost at threshold 0.5 = ${test_cost:,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sweep thresholds to find cost-optimal one\n",
        "thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "results = []\n",
        "\n",
        "for thresh in thresholds:\n",
        "    preds = (probabilities >= thresh).astype(int)\n",
        "    \n",
        "    # Calculate metrics for this threshold\n",
        "    prec = precision_score(y_test, preds, zero_division=0)\n",
        "    rec = recall_score(y_test, preds)\n",
        "    f1 = f1_score(y_test, preds)\n",
        "    cost = calculate_total_cost(y_test, preds, fp_cost=50, fn_cost=200)\n",
        "    \n",
        "    results.append({\n",
        "        'threshold': thresh,\n",
        "        'precision': prec,\n",
        "        'recall': rec,\n",
        "        'f1': f1,\n",
        "        'cost': cost\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Find and print the optimal threshold\n",
        "optimal_idx = results_df['cost'].idxmin()\n",
        "optimal_row = results_df.loc[optimal_idx]\n",
        "\n",
        "print(f\"=== Optimal Threshold: {optimal_row['threshold']:.2f} ===\")\n",
        "print(f\"At this threshold:\")\n",
        "print(f\"  Precision: {optimal_row['precision']:.1%}\")\n",
        "print(f\"  Recall: {optimal_row['recall']:.1%}\")\n",
        "print(f\"  F1: {optimal_row['f1']:.3f}\")\n",
        "print(f\"  Total Cost: ${optimal_row['cost']:,.0f}\")\n",
        "\n",
        "# Compare to default\n",
        "default_cost = calculate_total_cost(y_test, default_preds)\n",
        "print(f\"\\nðŸ’° Savings vs default (0.5): ${default_cost - optimal_row['cost']:,.0f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Self-Check: Threshold Intuition\n",
        "\n",
        "- Is the optimal threshold above or below 0.5?\n",
        "- Why does this make sense given that FN cost > FP cost?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualize the Tradeoff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Precision-Recall tradeoff\n",
        "ax1 = axes[0]\n",
        "ax1.plot(results_df['threshold'], results_df['precision'], 'b-', label='Precision', linewidth=2)\n",
        "ax1.plot(results_df['threshold'], results_df['recall'], 'r-', label='Recall', linewidth=2)\n",
        "ax1.plot(results_df['threshold'], results_df['f1'], 'g--', label='F1', linewidth=2)\n",
        "ax1.axvline(x=0.5, color='gray', linestyle=':', alpha=0.5)\n",
        "ax1.set_xlabel('Threshold')\n",
        "ax1.set_ylabel('Score')\n",
        "ax1.set_title('Precision-Recall Tradeoff')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Cost curve\n",
        "ax2 = axes[1]\n",
        "ax2.plot(results_df['threshold'], results_df['cost'], 'purple', linewidth=2)\n",
        "min_cost_thresh = results_df.loc[results_df['cost'].idxmin(), 'threshold']\n",
        "ax2.axvline(x=min_cost_thresh, color='green', linestyle='--', label=f'Optimal: {min_cost_thresh:.2f}')\n",
        "ax2.set_xlabel('Threshold')\n",
        "ax2.set_ylabel('Total Cost ($)')\n",
        "ax2.set_title('Business Cost by Threshold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. ROC Curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ROC Curve\n",
        "fpr, tpr, roc_thresholds = roc_curve(y_test, probabilities)\n",
        "auc = roc_auc_score(y_test, probabilities)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'Model (AUC = {auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random (AUC = 0.5)')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate (Recall)')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nAUC Interpretation:\")\n",
        "print(f\"  Random guessing: 0.5\")\n",
        "print(f\"  Perfect model: 1.0\")\n",
        "print(f\"  Our model: {auc:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Final Evaluation with Optimal Threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply optimal threshold\n",
        "optimal_threshold = results_df.loc[results_df['cost'].idxmin(), 'threshold']\n",
        "final_preds = (probabilities >= optimal_threshold).astype(int)\n",
        "\n",
        "print(f\"Final Model Performance (threshold = {optimal_threshold:.2f})\")\n",
        "print(\"=\" * 50)\n",
        "print(classification_report(y_test, final_preds, target_names=['Retained', 'Churned']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Business impact comparison\n",
        "default_cost = calculate_total_cost(y_test, default_preds)\n",
        "optimal_cost = calculate_total_cost(y_test, final_preds)\n",
        "baseline_cost = calculate_total_cost(y_test, baseline_preds)\n",
        "\n",
        "print(\"\\nBusiness Impact Comparison:\")\n",
        "print(f\"  Baseline (no model): ${baseline_cost:,.0f}\")\n",
        "print(f\"  Default threshold (0.5): ${default_cost:,.0f}\")\n",
        "print(f\"  Optimal threshold ({optimal_threshold:.2f}): ${optimal_cost:,.0f}\")\n",
        "print(f\"\\n  Savings vs baseline: ${baseline_cost - optimal_cost:,.0f}\")\n",
        "print(f\"  Savings vs default: ${default_cost - optimal_cost:,.0f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Stakeholder Summary\n",
        "\n",
        "### TODO: Write a 3-bullet summary (~100 words) for the retention team\n",
        "\n",
        "Template:\n",
        "â€¢ **What it does:** A model that predicts churn risk for each customer, using threshold ____ to balance costs.\n",
        "â€¢ **Performance:** Of customers we flag, about ___% actually churn (precision). We catch ___% of all churners (recall).\n",
        "â€¢ **Recommendation:** [How should the team use these predictions? What's the expected cost savings?]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Summary:**\n",
        "\n",
        "_[Write your summary here]_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# SELF-CHECK: Verify your threshold optimization is correct\n",
        "# Run this after completing the cost optimization section\n",
        "\n",
        "# Cost parameters (should match what you used above)\n",
        "FP_COST = 50   # Cost of false positive (wasted retention offer)\n",
        "FN_COST = 200  # Cost of false negative (lost customer)\n",
        "\n",
        "# Check that the cost function works\n",
        "assert calculate_total_cost(y_test, default_preds) > 0, \"Cost function should return positive value\"\n",
        "\n",
        "# Check that you found an optimal threshold\n",
        "assert 'optimal_threshold' in dir(), \"Should have found optimal_threshold\"\n",
        "assert optimal_threshold != 0.5, \"Optimal threshold should differ from default 0.5 (given FN costs 4x FP)\"\n",
        "\n",
        "# Check that optimal threshold is lower (since FN cost > FP cost)\n",
        "assert optimal_threshold < 0.5, \"When FN costs more than FP, optimal threshold should be below 0.5\"\n",
        "\n",
        "# Check cost savings\n",
        "optimal_cost = calculate_total_cost(y_test, final_preds, FP_COST, FN_COST)\n",
        "default_cost_check = calculate_total_cost(y_test, default_preds, FP_COST, FN_COST)\n",
        "assert optimal_cost <= default_cost_check, \"Optimal threshold should reduce or maintain costs\"\n",
        "\n",
        "print(\"âœ… Self-check passed!\")\n",
        "print(f\"   Optimal threshold: {optimal_threshold:.2f}\")\n",
        "print(f\"   Default cost (0.5): ${default_cost_check:,}\")\n",
        "print(f\"   Optimal cost: ${optimal_cost:,}\")\n",
        "print(f\"   Savings: ${default_cost_check - optimal_cost:,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Self-Assessment Checklist\n",
        "\n",
        "- [ ] I understand why accuracy is misleading for imbalanced classes\n",
        "- [ ] I can explain the precision-recall tradeoff\n",
        "- [ ] I found the optimal threshold using business costs\n",
        "- [ ] I can interpret logistic regression coefficients as odds ratios\n",
        "- [ ] I wrote a clear stakeholder summary\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. **Debug Drill:** Fix a classification model with threshold issues\n",
        "2. **Module 5:** Decision Trees - see how non-linear boundaries work"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}