{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution: The Diverging Neural Network\n",
    "\n",
    "This notebook provides the complete solution to the debug drill.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with features on different scales\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, random_state=42)\n",
    "\n",
    "# Make features on vastly different scales\n",
    "X[:, 0] *= 1000\n",
    "X[:, 1] *= 100\n",
    "X[:, 2] *= 0.01\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Feature scales:\")\n",
    "print(f\"  Feature 0: {X[:, 0].min():.0f} to {X[:, 0].max():.0f}\")\n",
    "print(f\"  Feature 1: {X[:, 1].min():.0f} to {X[:, 1].max():.0f}\")\n",
    "print(f\"  Feature 2: {X[:, 2].min():.4f} to {X[:, 2].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== BROKEN CODE =====\n",
    "model_broken = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    learning_rate_init=1.0,  # BUG: Too high!\n",
    "    max_iter=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_broken.fit(X_train, y_train)  # BUG: Not scaled!\n",
    "\n",
    "print(\"=== Broken Model ===\")\n",
    "print(f\"Final loss: {model_broken.loss_:.4f}\")\n",
    "print(f\"Test accuracy: {model_broken.score(X_test, y_test):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FIXED CODE =====\n",
    "\n",
    "# Fix 1: Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fix 2: Use a reasonable learning rate\n",
    "model_fixed = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    learning_rate_init=0.001,  # Fixed: reasonable LR\n",
    "    max_iter=200,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "model_fixed.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"=== Fixed Model ===\")\n",
    "print(f\"Final loss: {model_fixed.loss_:.4f}\")\n",
    "print(f\"Train accuracy: {model_fixed.score(X_train_scaled, y_train):.1%}\")\n",
    "print(f\"Test accuracy: {model_fixed.score(X_test_scaled, y_test):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare loss curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "if hasattr(model_broken, 'loss_curve_') and len(model_broken.loss_curve_) > 0:\n",
    "    axes[0].plot(model_broken.loss_curve_, 'r-', linewidth=2)\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('BROKEN: Unscaled + High LR')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(model_fixed.loss_curve_, 'g-', linewidth=2)\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('FIXED: Scaled + Reasonable LR')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Summary\n",
    "\n",
    "**Bug 1: No Feature Scaling**\n",
    "- Problem: Features on different scales (0.01 to 1000) cause unstable gradients\n",
    "- Fix: Apply `StandardScaler` before training\n",
    "\n",
    "**Bug 2: Learning Rate Too High**\n",
    "- Problem: LR=1.0 causes overshooting and divergence\n",
    "- Fix: Use LR=0.001 (typical default)\n",
    "\n",
    "**Result:**\n",
    "- Loss decreased smoothly\n",
    "- Test accuracy improved from ~50% to ~85%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
