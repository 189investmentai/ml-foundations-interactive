{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution: Debug Drill 06 - The Runaway Booster\n",
    "\n",
    "This is the solution notebook for the boosting overfitting drill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGBOOST = True\n",
    "except ImportError:\n",
    "    HAS_XGBOOST = False\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "DATA_URL = 'https://raw.githubusercontent.com/189investmentai/ml-foundations-interactive/main/shared/data/'\n",
    "customers = pd.read_csv(DATA_URL + 'streamcart_customers.csv')\n",
    "\n",
    "if 'tenure_days' not in customers.columns:\n",
    "    customers['tenure_days'] = (pd.to_datetime('2024-01-01') - pd.to_datetime(customers['signup_date'])).dt.days\n",
    "if 'avg_order_value' not in customers.columns:\n",
    "    customers['avg_order_value'] = customers['total_spend'] / customers['orders_total'].replace(0, 1)\n",
    "\n",
    "feature_cols = ['tenure_days', 'orders_total', 'total_spend', 'support_tickets_total', 'avg_order_value']\n",
    "available_features = [c for c in feature_cols if c in customers.columns]\n",
    "\n",
    "X = customers[available_features].fillna(0)\n",
    "y = customers['churn_30d']\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find where validation accuracy peaks\n",
    "n_estimators_list = [10, 25, 50, 100, 200, 300, 400, 500]\n",
    "results = []\n",
    "\n",
    "for n in n_estimators_list:\n",
    "    if HAS_XGBOOST:\n",
    "        model = XGBClassifier(n_estimators=n, max_depth=10, learning_rate=0.5,\n",
    "                              random_state=42, verbosity=0)\n",
    "    else:\n",
    "        model = GradientBoostingClassifier(n_estimators=n, max_depth=10, learning_rate=0.5,\n",
    "                                           random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    results.append({\n",
    "        'n_estimators': n,\n",
    "        'val_acc': accuracy_score(y_val, model.predict(X_val))\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "optimal_idx = results_df['val_acc'].idxmax()\n",
    "optimal_n = results_df.loc[optimal_idx, 'n_estimators']\n",
    "print(f\"Optimal n_estimators: {optimal_n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Fixed gradient boosting\n",
    "if HAS_XGBOOST:\n",
    "    gb_fixed = XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.1,\n",
    "        early_stopping_rounds=20,\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "    gb_fixed.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    stopped_at = gb_fixed.best_iteration\n",
    "else:\n",
    "    gb_fixed = GradientBoostingClassifier(\n",
    "        n_estimators=int(optimal_n),\n",
    "        max_depth=4,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    gb_fixed.fit(X_train, y_train)\n",
    "    stopped_at = int(optimal_n)\n",
    "\n",
    "train_acc_fixed = accuracy_score(y_train, gb_fixed.predict(X_train))\n",
    "val_acc_fixed = accuracy_score(y_val, gb_fixed.predict(X_val))\n",
    "test_acc_fixed = accuracy_score(y_test, gb_fixed.predict(X_test))\n",
    "\n",
    "print(\"=== Fixed Gradient Boosting ===\")\n",
    "print(f\"  Stopped at round: {stopped_at}\")\n",
    "print(f\"  max_depth: 4\")\n",
    "print(f\"  learning_rate: 0.1\")\n",
    "print(f\"\\n  Train Accuracy: {train_acc_fixed:.1%}\")\n",
    "print(f\"  Val Accuracy:   {val_acc_fixed:.1%}\")\n",
    "print(f\"  Test Accuracy:  {test_acc_fixed:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to overfit model\n",
    "if HAS_XGBOOST:\n",
    "    gb_overfit = XGBClassifier(n_estimators=500, max_depth=10, learning_rate=0.5,\n",
    "                               random_state=42, verbosity=0)\n",
    "else:\n",
    "    gb_overfit = GradientBoostingClassifier(n_estimators=500, max_depth=10, learning_rate=0.5,\n",
    "                                            random_state=42)\n",
    "gb_overfit.fit(X_train, y_train)\n",
    "\n",
    "train_acc = accuracy_score(y_train, gb_overfit.predict(X_train))\n",
    "test_acc = accuracy_score(y_test, gb_overfit.predict(X_test))\n",
    "\n",
    "print(\"\\n=== Comparison ===\")\n",
    "print(f\"                    Overfit Model   Fixed Model\")\n",
    "print(f\"  Rounds:           {500:>12}    {stopped_at:>12}\")\n",
    "print(f\"  max_depth:        {10:>12}    {4:>12}\")\n",
    "print(f\"  learning_rate:    {0.5:>12}    {0.1:>12}\")\n",
    "print(f\"  Train Acc:        {train_acc:>12.1%}    {train_acc_fixed:>12.1%}\")\n",
    "print(f\"  Test Acc:         {test_acc:>12.1%}    {test_acc_fixed:>12.1%}\")\n",
    "print(f\"\\n  Test improvement: +{test_acc_fixed - test_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Postmortem\n",
    "\n",
    "### What happened:\n",
    "- The model had high train accuracy but poor test accuracy\n",
    "- The model performed terribly in production after deployment\n",
    "\n",
    "### Root cause:\n",
    "Three configuration issues:\n",
    "1. No early stopping — model overtrained for 500 rounds\n",
    "2. Learning rate too high (0.5) — each tree overfit quickly\n",
    "3. Trees too deep (10) — individual trees were too complex\n",
    "\n",
    "### How to prevent:\n",
    "- ALWAYS use early stopping with gradient boosting\n",
    "- Use learning_rate 0.01-0.1 (not 0.3+)\n",
    "- Use max_depth 3-6 (not 8+)\n",
    "- Monitor validation loss during training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
