{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution: The Unscaled Lasso\n",
    "\n",
    "This is the answer key for `drill_11_unscaled_lasso.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with features on different scales\n",
    "n = 500\n",
    "\n",
    "tenure_days = np.random.uniform(30, 1000, n)\n",
    "monthly_spend = np.random.uniform(20, 200, n)\n",
    "orders = np.random.poisson(5, n)\n",
    "is_premium = np.random.binomial(1, 0.3, n)\n",
    "\n",
    "true_coefs = {\n",
    "    'tenure_days': 0.5,\n",
    "    'monthly_spend': 2.0,\n",
    "    'orders': 30.0,\n",
    "    'is_premium': 100.0\n",
    "}\n",
    "\n",
    "ltv = (\n",
    "    true_coefs['tenure_days'] * tenure_days +\n",
    "    true_coefs['monthly_spend'] * monthly_spend +\n",
    "    true_coefs['orders'] * orders +\n",
    "    true_coefs['is_premium'] * is_premium +\n",
    "    np.random.normal(0, 50, n)\n",
    ")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'tenure_days': tenure_days,\n",
    "    'monthly_spend': monthly_spend,\n",
    "    'orders': orders,\n",
    "    'is_premium': is_premium,\n",
    "    'ltv': ltv\n",
    "})\n",
    "\n",
    "feature_cols = ['tenure_days', 'monthly_spend', 'orders', 'is_premium']\n",
    "X = df[feature_cols]\n",
    "y = df['ltv']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUGGY: Lasso without scaling\n",
    "lasso_unscaled = Lasso(alpha=1.0)\n",
    "lasso_unscaled.fit(X_train, y_train)\n",
    "\n",
    "print(\"=== Unscaled Lasso (WRONG) ===\")\n",
    "for name, coef in zip(feature_cols, lasso_unscaled.coef_):\n",
    "    status = \"ZEROED\" if abs(coef) < 0.01 else f\"{coef:.3f}\"\n",
    "    print(f\"  {name:<15}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION: Scale Before Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Scale the features (fit on train only!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 2: Fit Lasso on scaled data\n",
    "lasso_scaled = LassoCV(cv=5)  # Use CV to find best alpha\n",
    "lasso_scaled.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"=== Fixed Lasso Results (Scaled) ===\")\n",
    "print(f\"\\nOptimal alpha: {lasso_scaled.alpha_:.4f}\")\n",
    "print(\"\\nCoefficients:\")\n",
    "for name, coef in zip(feature_cols, lasso_scaled.coef_):\n",
    "    status = \"ZEROED\" if abs(coef) < 0.01 else f\"{coef:.3f}\"\n",
    "    print(f\"  {name:<15}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance\n",
    "r2_unscaled = r2_score(y_test, lasso_unscaled.predict(X_test))\n",
    "r2_scaled = r2_score(y_test, lasso_scaled.predict(X_test_scaled))\n",
    "\n",
    "print(\"=== Performance Comparison ===\")\n",
    "print(f\"\\nUnscaled Lasso:\")\n",
    "print(f\"  R²: {r2_unscaled:.3f}\")\n",
    "print(f\"  Features selected: {(np.abs(lasso_unscaled.coef_) > 0.01).sum()}\")\n",
    "print(f\"\\nScaled Lasso:\")\n",
    "print(f\"  R²: {r2_scaled:.3f}\")\n",
    "print(f\"  Features selected: {(np.abs(lasso_scaled.coef_) > 0.01).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-check\n",
    "tenure_idx = feature_cols.index('tenure_days')\n",
    "assert abs(lasso_scaled.coef_[tenure_idx]) > 0.01, \"tenure_days should not be zeroed after scaling!\"\n",
    "assert r2_scaled > r2_unscaled, \"Scaled Lasso should have better R²\"\n",
    "\n",
    "print(\"✓ Lasso fixed!\")\n",
    "print(f\"✓ tenure_days now has coefficient: {lasso_scaled.coef_[tenure_idx]:.3f}\")\n",
    "print(f\"✓ R² improved: {r2_unscaled:.3f} → {r2_scaled:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Postmortem\n",
    "\n",
    "### What happened:\n",
    "- Lasso wrongly eliminated `tenure_days` as \"unimportant\" despite it being a strong predictor.\n",
    "\n",
    "### Root cause:\n",
    "- Lasso penalizes the sum of |coefficients|. When features are on different scales, features with large ranges (like tenure_days: 30-1000) get small coefficients, which Lasso then zeros out. The coefficient size doesn't reflect feature importance when scales differ.\n",
    "\n",
    "### How to prevent:\n",
    "- **Always scale before Lasso/Ridge/ElasticNet.** Use StandardScaler to make features comparable.\n",
    "- **Interpret coefficients on scaled data.** After scaling, larger |coefficient| = more important.\n",
    "- **Decision trees don't need scaling** — they're scale-invariant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
