{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution: Debug Drill 07 - The Exploding Loss\n",
    "\n",
    "This is the solution notebook for the diverging gradient descent drill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "def gradient_fn(x, y):\n",
    "    return np.array([2*x, 2*y])\n",
    "\n",
    "def gradient_descent(start, lr, n_steps=50):\n",
    "    position = np.array(start, dtype=float)\n",
    "    path = [position.copy()]\n",
    "    losses = [loss_fn(position[0], position[1])]\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        grad = gradient_fn(position[0], position[1])\n",
    "        position = position - lr * grad\n",
    "        path.append(position.copy())\n",
    "        losses.append(loss_fn(position[0], position[1]))\n",
    "    \n",
    "    return np.array(path), np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original divergent training\n",
    "START_POSITION = [-2.0, 2.0]\n",
    "LEARNING_RATE = 1.2\n",
    "\n",
    "path_bad, losses_bad = gradient_descent(START_POSITION, LEARNING_RATE, n_steps=20)\n",
    "\n",
    "print(\"=== Original (Divergent) ===\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "for i in range(min(5, len(losses_bad))):\n",
    "    if np.isfinite(losses_bad[i]):\n",
    "        print(f\"  Step {i}: {losses_bad[i]:.2f}\")\n",
    "    else:\n",
    "        print(f\"  Step {i}: EXPLODED!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Fixed learning rate\n",
    "LEARNING_RATE_FIXED = 0.1  # Safe learning rate\n",
    "\n",
    "path_fixed, losses_fixed = gradient_descent(START_POSITION, LEARNING_RATE_FIXED, n_steps=50)\n",
    "\n",
    "print(f\"=== Fixed Training (lr={LEARNING_RATE_FIXED}) ===\")\n",
    "print(f\"Starting loss: {losses_fixed[0]:.4f}\")\n",
    "print(f\"Final loss: {losses_fixed[-1]:.6f}\")\n",
    "print(f\"Final position: ({path_fixed[-1, 0]:.4f}, {path_fixed[-1, 1]:.4f})\")\n",
    "print(f\"\\nConverged: {losses_fixed[-1] < 0.01}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = loss_fn(X, Y)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.5)\n",
    "ax1.plot(np.clip(path_bad[:10], -5, 5)[:, 0], np.clip(path_bad[:10], -5, 5)[:, 1], \n",
    "         'r-o', markersize=4, linewidth=1, label=f'Divergent (lr={LEARNING_RATE})', alpha=0.7)\n",
    "ax1.plot(path_fixed[:, 0], path_fixed[:, 1], \n",
    "         'b-o', markersize=4, linewidth=1, label=f'Converged (lr={LEARNING_RATE_FIXED})')\n",
    "ax1.scatter([0], [0], color='green', s=100, marker='*', zorder=5)\n",
    "ax1.set_xlabel('θ₁')\n",
    "ax1.set_ylabel('θ₂')\n",
    "ax1.set_title('Gradient Descent Paths')\n",
    "ax1.legend()\n",
    "ax1.set_xlim(-5, 5)\n",
    "ax1.set_ylim(-5, 5)\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.plot(np.clip(losses_bad, 0, 50), 'r-', linewidth=2, label=f'lr={LEARNING_RATE} (diverges)')\n",
    "ax2.plot(losses_fixed, 'b-', linewidth=2, label=f'lr={LEARNING_RATE_FIXED} (converges)')\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Loss Over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Postmortem\n",
    "\n",
    "### What happened:\n",
    "- Loss increased instead of decreased, eventually going to infinity\n",
    "- Training completely failed to converge\n",
    "\n",
    "### Root cause:\n",
    "- Learning rate (1.2) was too high\n",
    "- For a quadratic loss, lr must be < 1.0 to converge\n",
    "- With lr > 1, updates overshoot and amplify the error\n",
    "\n",
    "### How to prevent:\n",
    "- Start with small learning rate (0.001-0.1)\n",
    "- If loss explodes, reduce lr by 10x\n",
    "- Use learning rate schedulers for stability\n",
    "- Monitor loss during early training iterations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
