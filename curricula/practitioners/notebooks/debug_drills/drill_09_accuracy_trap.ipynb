{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Drill: The Accuracy Trap\n",
    "\n",
    "**Scenario:**\n",
    "A colleague built a fraud detection model. They're celebrating.\n",
    "\n",
    "\"99.5% accuracy!\" they announce. \"Best model I've ever built!\"\n",
    "\n",
    "But the fraud team is angry: \"We're catching fewer fraudsters than before!\"\n",
    "\n",
    "**Your Task:**\n",
    "1. Run the model and verify the high accuracy\n",
    "2. Investigate why the fraud team is upset\n",
    "3. Find the right metric and fix the threshold\n",
    "4. Write a 3-bullet postmortem\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score\n",
    ")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate imbalanced fraud data (0.5% fraud rate)\n",
    "n_samples = 10000\n",
    "fraud_rate = 0.005  # Very rare!\n",
    "\n",
    "# Features\n",
    "amount = np.random.exponential(100, n_samples)\n",
    "hour = np.random.randint(0, 24, n_samples)\n",
    "is_foreign = np.random.binomial(1, 0.1, n_samples)\n",
    "customer_age_days = np.random.uniform(1, 1000, n_samples)\n",
    "\n",
    "# Generate fraud labels (fraudsters tend to: high amount, foreign, night time, new customers)\n",
    "fraud_prob = 1 / (1 + np.exp(-(\n",
    "    -7 +\n",
    "    0.01 * amount +\n",
    "    2 * is_foreign +\n",
    "    0.1 * ((hour < 6) | (hour > 22)).astype(int) -\n",
    "    0.003 * customer_age_days\n",
    ")))\n",
    "fraud = (np.random.random(n_samples) < fraud_prob).astype(int)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'amount': amount,\n",
    "    'hour': hour,\n",
    "    'is_foreign': is_foreign,\n",
    "    'customer_age_days': customer_age_days,\n",
    "    'fraud': fraud\n",
    "})\n",
    "\n",
    "print(f\"Dataset: {len(df)} transactions\")\n",
    "print(f\"Fraud rate: {df['fraud'].mean():.2%} ({df['fraud'].sum()} fraudulent)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== COLLEAGUE'S CODE =====\n",
    "\n",
    "X = df[['amount', 'hour', 'is_foreign', 'customer_age_days']]\n",
    "y = df['fraud']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Using default threshold of 0.5\n",
    "y_pred = model.predict(X_test)  # <-- BUG: Default threshold for imbalanced data!\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"=== Colleague's Report ===\")\n",
    "print(f\"\\nðŸŽ‰ Accuracy: {accuracy:.1%}\")\n",
    "print(\"\\nâœ… Deploying to production...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Your Investigation\n",
    "\n",
    "The fraud team says the model is useless. Let's find out why.\n",
    "\n",
    "### Step 1: The naive baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we predicted NO FRAUD for everyone?\n",
    "naive_pred = np.zeros_like(y_test)\n",
    "naive_accuracy = accuracy_score(y_test, naive_pred)\n",
    "\n",
    "print(\"=== The Accuracy Trap ===\")\n",
    "print(f\"\\nColleague's model accuracy: {accuracy:.1%}\")\n",
    "print(f\"Naive 'no fraud' accuracy:  {naive_accuracy:.1%}\")\n",
    "print(f\"\\nâŒ Predicting 'no fraud' for EVERYONE gets {naive_accuracy:.1%} accuracy!\")\n",
    "print(\"   Accuracy is USELESS when fraud is rare.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(f\"\\n              Predicted\")\n",
    "print(f\"              No Fraud    Fraud\")\n",
    "print(f\"Actual No    TN={tn:<6}  FP={fp}\")\n",
    "print(f\"Actual Yes   FN={fn:<6}  TP={tp}\")\n",
    "\n",
    "print(f\"\\nðŸš¨ PROBLEM: We're catching only {tp} out of {tp + fn} fraudsters!\")\n",
    "print(f\"   Recall = {tp}/{tp + fn} = {recall_score(y_test, y_pred):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Look at the right metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probabilities instead of predictions\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"=== Metrics That Actually Matter ===\")\n",
    "print(f\"\\nPrecision: {precision_score(y_test, y_pred):.1%}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred):.1%} â† This is the problem!\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred):.3f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc_score(y_test, y_prob):.3f}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ The model has learned something (AUC > 0.5), but the threshold is wrong!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: TODO - Find the right threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business context:\n",
    "# - Missing fraud (FN) costs $1000 (chargebacks + reputation)\n",
    "# - False alarm (FP) costs $10 (manual review)\n",
    "\n",
    "COST_FN = 1000\n",
    "COST_FP = 10\n",
    "\n",
    "# Theoretical optimal threshold\n",
    "theoretical_optimal = COST_FP / (COST_FP + COST_FN)\n",
    "print(f\"Theoretical optimal threshold: {theoretical_optimal:.3f}\")\n",
    "print(f\"Colleague's threshold: 0.5\")\n",
    "print(f\"\\nâŒ 0.5 is WAY too high when FN costs 100x more than FP!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sweep thresholds and find the cost-optimal one\n",
    "\n",
    "# Uncomment and complete:\n",
    "\n",
    "# thresholds = np.arange(0.001, 0.5, 0.005)\n",
    "# results = []\n",
    "# \n",
    "# for thresh in thresholds:\n",
    "#     preds = (y_prob >= thresh).astype(int)\n",
    "#     cm_t = confusion_matrix(y_test, preds)\n",
    "#     tn_t, fp_t, fn_t, tp_t = cm_t.ravel()\n",
    "#     \n",
    "#     cost = fn_t * COST_FN + fp_t * COST_FP\n",
    "#     recall = recall_score(y_test, preds)\n",
    "#     precision = precision_score(y_test, preds, zero_division=0)\n",
    "#     \n",
    "#     results.append({\n",
    "#         'threshold': thresh,\n",
    "#         'cost': cost,\n",
    "#         'recall': recall,\n",
    "#         'precision': precision,\n",
    "#         'fn': fn_t,\n",
    "#         'fp': fp_t\n",
    "#     })\n",
    "# \n",
    "# results_df = pd.DataFrame(results)\n",
    "# optimal_idx = results_df['cost'].idxmin()\n",
    "# optimal_threshold = results_df.loc[optimal_idx, 'threshold']\n",
    "# \n",
    "# print(f\"=== Cost-Optimal Threshold: {optimal_threshold:.3f} ===\")\n",
    "# print(f\"Recall at optimal: {results_df.loc[optimal_idx, 'recall']:.1%}\")\n",
    "# print(f\"Precision at optimal: {results_df.loc[optimal_idx, 'precision']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare original vs optimized\n",
    "\n",
    "# Uncomment:\n",
    "\n",
    "# y_pred_fixed = (y_prob >= optimal_threshold).astype(int)\n",
    "# \n",
    "# original_cost = fn * COST_FN + fp * COST_FP\n",
    "# optimal_cost = results_df.loc[optimal_idx, 'cost']\n",
    "# \n",
    "# print(\"=== Comparison ===\")\n",
    "# print(f\"\\n                    Original (0.5)    Optimized ({optimal_threshold:.3f})\")\n",
    "# print(f\"  Recall:           {recall_score(y_test, y_pred):>10.1%}      {recall_score(y_test, y_pred_fixed):>10.1%}\")\n",
    "# print(f\"  Precision:        {precision_score(y_test, y_pred):>10.1%}      {precision_score(y_test, y_pred_fixed):>10.1%}\")\n",
    "# print(f\"  Total Cost:       ${original_cost:>10,}      ${optimal_cost:>10,}\")\n",
    "# print(f\"\\n  ðŸ’° Savings: ${original_cost - optimal_cost:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SELF-CHECK\n",
    "# ============================================\n",
    "\n",
    "# Uncomment:\n",
    "\n",
    "# assert optimal_threshold < 0.5, \"Optimal threshold should be below 0.5 for costly FN\"\n",
    "# assert recall_score(y_test, y_pred_fixed) > recall_score(y_test, y_pred), \"Should improve recall\"\n",
    "# assert optimal_cost < original_cost, \"Should reduce total cost\"\n",
    "# \n",
    "# print(\"âœ“ Accuracy trap escaped!\")\n",
    "# print(f\"âœ“ Threshold: 0.5 â†’ {optimal_threshold:.3f}\")\n",
    "# print(f\"âœ“ Recall: {recall_score(y_test, y_pred):.1%} â†’ {recall_score(y_test, y_pred_fixed):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Write your postmortem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postmortem = \"\"\"\n",
    "## Postmortem: The Accuracy Trap\n",
    "\n",
    "### What happened:\n",
    "- (Your answer: Why was 99.5% accuracy misleading?)\n",
    "\n",
    "### Root cause:\n",
    "- (Your answer: What's wrong with using accuracy on imbalanced data?)\n",
    "\n",
    "### How to prevent:\n",
    "- (Your answer: What metrics should we use for rare-event classification?)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(postmortem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Drill Complete!\n",
    "\n",
    "**Key lessons:**\n",
    "\n",
    "1. **Accuracy is useless for imbalanced data.** Predicting the majority class gets high accuracy.\n",
    "\n",
    "2. **Use recall when missing positives is costly** (fraud, disease, churn).\n",
    "\n",
    "3. **Use precision when false alarms are costly** (spam, recommendations).\n",
    "\n",
    "4. **The default threshold (0.5) is rarely optimal.** Calculate based on business costs.\n",
    "\n",
    "---\n",
    "\n",
    "## Metric Selection Guide\n",
    "\n",
    "| If... | Use... | Why |\n",
    "|-------|--------|-----|\n",
    "| Classes are balanced | Accuracy, F1 | Simple and informative |\n",
    "| Missing positives is costly | Recall, PR-AUC | Catch as many as possible |\n",
    "| False alarms are costly | Precision | Be confident when you flag |\n",
    "| Ranking matters | ROC-AUC | Threshold-independent |\n",
    "| Class is very rare | PR-AUC | ROC-AUC can be misleading |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
