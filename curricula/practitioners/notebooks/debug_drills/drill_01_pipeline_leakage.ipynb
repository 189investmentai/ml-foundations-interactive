{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Drill: The Too-Good Model\n",
    "\n",
    "**Scenario:**\n",
    "A colleague built a churn prediction pipeline. They're excited because the model achieves 97% AUC on the test set.\n",
    "\n",
    "\"This is the best model we've ever built!\" they say. \"We should ship it immediately!\"\n",
    "\n",
    "**Your Task:**\n",
    "1. Run the pipeline end-to-end\n",
    "2. Find the bug (hint: it's NOT in the model training step)\n",
    "3. Fix it\n",
    "4. Write a 3-bullet postmortem\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STAGE 1: DATA LOADING =====\n",
    "DATA_URL = 'https://raw.githubusercontent.com/189investmentai/ml-foundations-interactive/main/shared/data/'\n",
    "\n",
    "df = pd.read_csv(DATA_URL + 'streamcart_customers.csv')\n",
    "print(f\"Loaded {len(df):,} customers\")\n",
    "print(f\"Churn rate: {df['churned'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STAGE 2: FEATURE ENGINEERING =====\n",
    "# Colleague's feature engineering code\n",
    "\n",
    "def engineer_features(data):\n",
    "    \"\"\"Create features for churn prediction.\"\"\"\n",
    "    df_feat = data.copy()\n",
    "    \n",
    "    # Behavioral features\n",
    "    df_feat['orders_per_month'] = df_feat['orders_total'] / (df_feat['tenure_months'] + 1)\n",
    "    df_feat['spend_per_order'] = df_feat['total_spend'] / (df_feat['orders_total'] + 1)\n",
    "    df_feat['login_intensity'] = df_feat['logins_last_30d'] / 30\n",
    "    \n",
    "    # Engagement ratio\n",
    "    df_feat['engagement_ratio'] = df_feat['logins_per_month_avg'] / (df_feat['orders_per_month'] + 0.1)\n",
    "    \n",
    "    # Recency features\n",
    "    df_feat['days_since_last_order'] = df_feat['days_since_last_purchase']\n",
    "    \n",
    "    # Support interaction\n",
    "    df_feat['tickets_per_tenure'] = df_feat['support_tickets_total'] / (df_feat['tenure_months'] + 1)\n",
    "    \n",
    "    return df_feat\n",
    "\n",
    "df_engineered = engineer_features(df)\n",
    "print(f\"Engineered {len(df_engineered.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STAGE 3: PREPROCESSING =====\n",
    "# Colleague's preprocessing code (CONTAINS BUG)\n",
    "\n",
    "feature_cols = [\n",
    "    'tenure_months', 'orders_per_month', 'spend_per_order', \n",
    "    'login_intensity', 'engagement_ratio', 'days_since_last_order',\n",
    "    'tickets_per_tenure', 'avg_order_value'\n",
    "]\n",
    "\n",
    "X = df_engineered[feature_cols].fillna(0)\n",
    "y = df_engineered['churned']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # <-- Colleague scales ALL data here\n",
    "\n",
    "print(f\"Features shape: {X_scaled.shape}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STAGE 4: TRAIN/TEST SPLIT =====\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_train):,}\")\n",
    "print(f\"Test size: {len(X_test):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STAGE 5: MODEL TRAINING =====\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STAGE 6: EVALUATION =====\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Test AUC: {auc:.3f}\")\n",
    "print(f\"\\nðŸŽ‰ Colleague: 'This is amazing! 97% AUC! Ship it!'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Your Investigation\n",
    "\n",
    "The AUC is suspiciously high. Something is wrong.\n",
    "\n",
    "### Step 1: Which pipeline stage contains the bug?\n",
    "\n",
    "Remember the ML pipeline stages:\n",
    "1. Problem Framing\n",
    "2. Data Collection  \n",
    "3. Feature Engineering\n",
    "4. Preprocessing\n",
    "5. Train/Test Split\n",
    "6. Model Training\n",
    "7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Investigate each stage\n",
    "# Look at the code above. What happens BEFORE the train/test split?\n",
    "\n",
    "# Hint 1: Look at the order of operations\n",
    "print(\"Order of operations:\")\n",
    "print(\"1. Load data\")\n",
    "print(\"2. Engineer features\")\n",
    "print(\"3. Scale features with StandardScaler\")\n",
    "print(\"4. Split into train/test\")\n",
    "print(\"5. Train model\")\n",
    "print(\"6. Evaluate\")\n",
    "\n",
    "print(\"\\nðŸ” What's wrong with this order?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 2: What does StandardScaler.fit_transform() learn from the data?\n",
    "print(\"StandardScaler learns:\")\n",
    "print(f\"  Mean of each feature: {scaler.mean_[:3]}...\")\n",
    "print(f\"  Std of each feature: {scaler.scale_[:3]}...\")\n",
    "\n",
    "print(\"\\nðŸ¤” Question: When scaler.fit_transform(X) was called, what data was it using?\")\n",
    "print(\"   Answer: ALL the data â€” including what would become the test set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your diagnosis:\n",
    "\n",
    "diagnosis = \"\"\"\n",
    "YOUR DIAGNOSIS HERE:\n",
    "\n",
    "The bug is in stage: _______________\n",
    "\n",
    "The problem is: _______________\n",
    "\n",
    "This is called: _______________\n",
    "\n",
    "Why it causes inflated AUC: _______________\n",
    "\n",
    "\"\"\"\n",
    "print(diagnosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Fix the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the correct pipeline order\n",
    "\n",
    "# The key insight: Split FIRST, then fit scaler only on training data\n",
    "\n",
    "# Uncomment and complete:\n",
    "\n",
    "# # Correct order:\n",
    "# # 1. Split raw features FIRST\n",
    "# X_raw = df_engineered[feature_cols].fillna(0)\n",
    "# y = df_engineered['churned']\n",
    "#\n",
    "# X_train_raw, X_test_raw, y_train_fixed, y_test_fixed = train_test_split(\n",
    "#     X_raw, y, test_size=0.2, random_state=42, stratify=y\n",
    "# )\n",
    "#\n",
    "# # 2. Fit scaler ONLY on training data\n",
    "# scaler_fixed = StandardScaler()\n",
    "# X_train_fixed = scaler_fixed.fit_transform(X_train_raw)  # fit + transform on train\n",
    "# X_test_fixed = scaler_fixed.transform(X_test_raw)        # transform only on test\n",
    "#\n",
    "# # 3. Train model\n",
    "# model_fixed = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "# model_fixed.fit(X_train_fixed, y_train_fixed)\n",
    "#\n",
    "# # 4. Evaluate\n",
    "# y_pred_proba_fixed = model_fixed.predict_proba(X_test_fixed)[:, 1]\n",
    "# auc_fixed = roc_auc_score(y_test_fixed, y_pred_proba_fixed)\n",
    "#\n",
    "# print(f\"Fixed Test AUC: {auc_fixed:.3f}\")\n",
    "# print(f\"\\nDifference: {auc - auc_fixed:.3f} points lower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SELF-CHECK: Did you fix it?\n",
    "# ============================================\n",
    "\n",
    "# Uncomment after fixing:\n",
    "#\n",
    "# # The fixed AUC should be lower (more realistic)\n",
    "# assert auc_fixed < auc, \"Fixed AUC should be lower than the leaky AUC!\"\n",
    "# \n",
    "# # The difference should be noticeable\n",
    "# assert auc - auc_fixed > 0.01, \"Should see at least 1% drop in AUC\"\n",
    "#\n",
    "# print(\"âœ“ Pipeline fixed!\")\n",
    "# print(f\"âœ“ Buggy AUC: {auc:.3f} (inflated by preprocessing leakage)\")\n",
    "# print(f\"âœ“ Fixed AUC: {auc_fixed:.3f} (realistic estimate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Understand why this matters\n",
    "\n",
    "Preprocessing leakage is subtle because:\n",
    "1. The code runs without errors\n",
    "2. The model trains successfully  \n",
    "3. The metrics look great\n",
    "4. **But production performance will be worse**\n",
    "\n",
    "In production:\n",
    "- You can only use statistics from past data\n",
    "- The scaler must be fit on training data and saved\n",
    "- New data gets transformed using the saved scaler\n",
    "\n",
    "When you fit the scaler on ALL data (including test), you're:\n",
    "- Using future information (test set statistics)\n",
    "- Making the test set \"easier\" to predict\n",
    "- Getting an optimistic estimate of production performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Write your postmortem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postmortem = \"\"\"\n",
    "## Postmortem: The Too-Good Model\n",
    "\n",
    "### What happened:\n",
    "- (Your answer: What was the symptom that indicated a problem?)\n",
    "\n",
    "### Root cause:\n",
    "- (Your answer: Which pipeline stage had the bug? What exactly was wrong?)\n",
    "\n",
    "### How to prevent:\n",
    "- (Your answer: What checks would catch this in future pipelines?)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(postmortem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Drill Complete!\n",
    "\n",
    "**Key lessons:**\n",
    "\n",
    "1. **Pipeline order matters.** Split before preprocessing, not after.\n",
    "\n",
    "2. **Preprocessing leakage is silent.** No errors, no warnings â€” just inflated metrics.\n",
    "\n",
    "3. **The bug was in Stage 3-4 boundary.** The model training (Stage 5) was fine. Most ML bugs are at stage boundaries, not in the model itself.\n",
    "\n",
    "4. **\"Too good to be true\" is a red flag.** Domain-appropriate AUC for churn is typically 0.70-0.85. Getting 0.97 should trigger investigation.\n",
    "\n",
    "---\n",
    "\n",
    "## Bonus: Other preprocessing leakage patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: Imputation leakage\n",
    "print(\"WRONG:\")\n",
    "print(\"  df['feature'] = df['feature'].fillna(df['feature'].mean())  # Uses all data\")\n",
    "print(\"  X_train, X_test = train_test_split(...)\")\n",
    "\n",
    "print(\"\\nRIGHT:\")\n",
    "print(\"  X_train, X_test = train_test_split(...)  # Split first\")\n",
    "print(\"  train_mean = X_train['feature'].mean()  # Calculate on train only\")\n",
    "print(\"  X_train['feature'] = X_train['feature'].fillna(train_mean)\")\n",
    "print(\"  X_test['feature'] = X_test['feature'].fillna(train_mean)  # Use train mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 2: Encoding leakage\n",
    "print(\"WRONG:\")\n",
    "print(\"  encoder.fit(df['category'])  # Learns all categories\")\n",
    "print(\"  X_train, X_test = train_test_split(...)\")\n",
    "\n",
    "print(\"\\nRIGHT:\")\n",
    "print(\"  X_train, X_test = train_test_split(...)  # Split first\")\n",
    "print(\"  encoder.fit(X_train['category'])  # Learn from train only\")\n",
    "print(\"  # Handle unknown categories in test with 'handle_unknown'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 3: Feature selection leakage\n",
    "print(\"WRONG:\")\n",
    "print(\"  # Select features using correlation with target on ALL data\")\n",
    "print(\"  correlations = df[features].corrwith(df['target'])\")\n",
    "print(\"  best_features = correlations.nlargest(10).index\")\n",
    "print(\"  X_train, X_test = train_test_split(df[best_features], ...)\")\n",
    "\n",
    "print(\"\\nRIGHT:\")\n",
    "print(\"  # Split first, select features using train only\")\n",
    "print(\"  X_train, X_test = train_test_split(...)\")\n",
    "print(\"  correlations = X_train[features].corrwith(y_train)\")\n",
    "print(\"  best_features = correlations.nlargest(10).index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Universal Rule\n",
    "\n",
    "**Anything that \"learns\" from data must learn only from training data.**\n",
    "\n",
    "This includes:\n",
    "- Scalers (mean, std)\n",
    "- Imputers (fill values)\n",
    "- Encoders (category mappings)\n",
    "- Feature selectors (importance scores)\n",
    "- Dimensionality reducers (PCA components)\n",
    "\n",
    "**The fix is always the same:** Split first, fit on train, transform both."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
