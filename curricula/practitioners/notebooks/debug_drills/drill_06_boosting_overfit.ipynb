{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Drill: The Runaway Booster\n",
    "\n",
    "**Scenario:**\n",
    "A colleague trained a gradient boosting model for churn prediction. They followed \"best practices\" from a blog.\n",
    "\n",
    "\"I set n_estimators really high (1000) and the training loss keeps going down!\" they say proudly.\n",
    "\n",
    "But after deployment, the model performs terribly.\n",
    "\n",
    "**Your Task:**\n",
    "1. Run the model and diagnose the problem\n",
    "2. Identify why the boosting overfit\n",
    "3. Fix it with proper configuration\n",
    "4. Write a 3-bullet postmortem\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try to use XGBoost if available (faster, better early stopping)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGBOOST = True\n",
    "    print(\"âœ“ Using XGBoost\")\n",
    "except ImportError:\n",
    "    HAS_XGBOOST = False\n",
    "    print(\"âš  XGBoost not available, using sklearn GradientBoosting\")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "DATA_URL = 'https://raw.githubusercontent.com/189investmentai/ml-foundations-interactive/main/shared/data/'\n",
    "\n",
    "customers = pd.read_csv(DATA_URL + 'streamcart_customers.csv')\n",
    "print(f\"Loaded {len(customers)} customers\")\n",
    "\n",
    "# Prepare features\n",
    "if 'tenure_days' not in customers.columns:\n",
    "    customers['tenure_days'] = (pd.to_datetime('2024-01-01') - pd.to_datetime(customers['signup_date'])).dt.days\n",
    "if 'avg_order_value' not in customers.columns:\n",
    "    customers['avg_order_value'] = customers['total_spend'] / customers['orders_total'].replace(0, 1)\n",
    "\n",
    "feature_cols = ['tenure_days', 'orders_total', 'total_spend', 'support_tickets_total', 'avg_order_value']\n",
    "available_features = [c for c in feature_cols if c in customers.columns]\n",
    "\n",
    "X = customers[available_features].fillna(0)\n",
    "y = customers['churn_30d']\n",
    "\n",
    "# Train-validation-test split (need validation for early stopping)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== COLLEAGUE'S CODE (CONTAINS BUGS) =====\n",
    "\n",
    "# \"I read that more trees is always better!\"\n",
    "# \"And higher learning rate makes training faster!\"\n",
    "# \"Deep trees capture complex patterns!\"\n",
    "\n",
    "if HAS_XGBOOST:\n",
    "    gb_overfit = XGBClassifier(\n",
    "        n_estimators=500,     # <-- BUG: Too many without early stopping\n",
    "        max_depth=10,         # <-- BUG: Too deep for boosting\n",
    "        learning_rate=0.5,    # <-- BUG: Too aggressive\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "else:\n",
    "    gb_overfit = GradientBoostingClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.5,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "gb_overfit.fit(X_train, y_train)\n",
    "\n",
    "train_acc = accuracy_score(y_train, gb_overfit.predict(X_train))\n",
    "val_acc = accuracy_score(y_val, gb_overfit.predict(X_val))\n",
    "test_acc = accuracy_score(y_test, gb_overfit.predict(X_test))\n",
    "\n",
    "print(\"=== Colleague's Gradient Boosting ===\")\n",
    "print(f\"  n_estimators: 500\")\n",
    "print(f\"  max_depth: 10\")\n",
    "print(f\"  learning_rate: 0.5\")\n",
    "print(f\"\\n  Train Accuracy: {train_acc:.1%}\")\n",
    "print(f\"  Val Accuracy:   {val_acc:.1%}\")\n",
    "print(f\"  Test Accuracy:  {test_acc:.1%}\")\n",
    "print(f\"\\n  Train-Test Gap: {train_acc - test_acc:.1%} â† OVERFITTING!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Your Investigation\n",
    "\n",
    "The model memorized the training data. Let's understand why.\n",
    "\n",
    "### Step 1: Understand what went wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three deadly sins of boosting:\n",
    "print(\"âŒ The Three Deadly Sins of Boosting:\")\n",
    "print()\n",
    "print(\"1. No early stopping:\")\n",
    "print(\"   - Boosting can perfectly fit training data given enough rounds\")\n",
    "print(\"   - Without stopping, it will overfit\")\n",
    "print()\n",
    "print(\"2. High learning rate (0.5):\")\n",
    "print(\"   - Each tree makes large corrections\")\n",
    "print(\"   - Model fits noise quickly\")\n",
    "print(f\"   - Colleague used: 0.5 (recommended: 0.01-0.1)\")\n",
    "print()\n",
    "print(\"3. Deep trees (max_depth=10):\")\n",
    "print(\"   - Each tree is complex enough to overfit on its own\")\n",
    "print(\"   - Boosting works best with shallow 'weak' learners\")\n",
    "print(f\"   - Colleague used: 10 (recommended: 3-6)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how accuracy changes with number of rounds\n",
    "# (This simulates what happens during training)\n",
    "\n",
    "n_estimators_list = [10, 25, 50, 100, 200, 300, 400, 500]\n",
    "results = []\n",
    "\n",
    "for n in n_estimators_list:\n",
    "    if HAS_XGBOOST:\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=n, max_depth=10, learning_rate=0.5,\n",
    "            random_state=42, verbosity=0\n",
    "        )\n",
    "    else:\n",
    "        model = GradientBoostingClassifier(\n",
    "            n_estimators=n, max_depth=10, learning_rate=0.5,\n",
    "            random_state=42\n",
    "        )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    results.append({\n",
    "        'n_estimators': n,\n",
    "        'train_acc': accuracy_score(y_train, model.predict(X_train)),\n",
    "        'val_acc': accuracy_score(y_val, model.predict(X_val)),\n",
    "        'test_acc': accuracy_score(y_test, model.predict(X_test))\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df['n_estimators'], results_df['train_acc'], 'b-o', label='Train', linewidth=2)\n",
    "plt.plot(results_df['n_estimators'], results_df['val_acc'], 'g-s', label='Validation', linewidth=2)\n",
    "plt.plot(results_df['n_estimators'], results_df['test_acc'], 'r-^', label='Test', linewidth=2)\n",
    "plt.xlabel('Number of Boosting Rounds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Overfitting: Train Accuracy Keeps Rising, Test Plateaus/Drops')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ” Key observation: Validation accuracy peaks then drops!\")\n",
    "print(\"   This is classic overfitting. Early stopping would have caught this.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where would early stopping have stopped?\n",
    "optimal_idx = results_df['val_acc'].idxmax()\n",
    "optimal_n = results_df.loc[optimal_idx, 'n_estimators']\n",
    "\n",
    "print(f\"Early stopping would have stopped at: {optimal_n} rounds\")\n",
    "print(f\"Colleague trained for: 500 rounds\")\n",
    "print(f\"\\nOvertraining by: {500 - optimal_n} rounds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Fix the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a properly configured gradient boosting model\n",
    "\n",
    "# Uncomment and complete:\n",
    "\n",
    "# if HAS_XGBOOST:\n",
    "#     gb_fixed = XGBClassifier(\n",
    "#         n_estimators=500,        # Still high, but early stopping will save us\n",
    "#         max_depth=4,             # FIX: Shallow trees (3-6 is typical)\n",
    "#         learning_rate=0.1,       # FIX: Smaller steps (0.01-0.1 is typical)\n",
    "#         early_stopping_rounds=20, # FIX: Stop if no improvement for 20 rounds\n",
    "#         random_state=42,\n",
    "#         verbosity=0\n",
    "#     )\n",
    "#     gb_fixed.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "#     stopped_at = gb_fixed.best_iteration\n",
    "# else:\n",
    "#     # sklearn GradientBoosting doesn't have built-in early stopping\n",
    "#     # So we use the optimal n we found above\n",
    "#     gb_fixed = GradientBoostingClassifier(\n",
    "#         n_estimators=optimal_n,  # Use the optimal found earlier\n",
    "#         max_depth=4,\n",
    "#         learning_rate=0.1,\n",
    "#         random_state=42\n",
    "#     )\n",
    "#     gb_fixed.fit(X_train, y_train)\n",
    "#     stopped_at = optimal_n\n",
    "# \n",
    "# train_acc_fixed = accuracy_score(y_train, gb_fixed.predict(X_train))\n",
    "# val_acc_fixed = accuracy_score(y_val, gb_fixed.predict(X_val))\n",
    "# test_acc_fixed = accuracy_score(y_test, gb_fixed.predict(X_test))\n",
    "# \n",
    "# print(\"=== Fixed Gradient Boosting ===\")\n",
    "# print(f\"  Stopped at round: {stopped_at}\")\n",
    "# print(f\"  max_depth: 4\")\n",
    "# print(f\"  learning_rate: 0.1\")\n",
    "# print(f\"\\n  Train Accuracy: {train_acc_fixed:.1%}\")\n",
    "# print(f\"  Val Accuracy:   {val_acc_fixed:.1%}\")\n",
    "# print(f\"  Test Accuracy:  {test_acc_fixed:.1%}\")\n",
    "# print(f\"  Train-Test Gap: {train_acc_fixed - test_acc_fixed:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare the models\n",
    "\n",
    "# Uncomment:\n",
    "\n",
    "# print(\"\\n=== Comparison ===\")\n",
    "# print(f\"                    Overfit Model   Fixed Model\")\n",
    "# print(f\"  Rounds:           {500:>12}    {stopped_at:>12}\")\n",
    "# print(f\"  max_depth:        {10:>12}    {4:>12}\")\n",
    "# print(f\"  learning_rate:    {0.5:>12}    {0.1:>12}\")\n",
    "# print(f\"  Train Acc:        {train_acc:>12.1%}    {train_acc_fixed:>12.1%}\")\n",
    "# print(f\"  Test Acc:         {test_acc:>12.1%}    {test_acc_fixed:>12.1%}\")\n",
    "# print(f\"  Gap:              {train_acc - test_acc:>12.1%}    {train_acc_fixed - test_acc_fixed:>12.1%}\")\n",
    "# print(f\"\\n  Test improvement: +{test_acc_fixed - test_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SELF-CHECK: Did you fix the overfitting?\n",
    "# ============================================\n",
    "\n",
    "# Uncomment:\n",
    "\n",
    "# assert (train_acc_fixed - test_acc_fixed) < 0.15, \"Train-test gap should be reduced\"\n",
    "# assert test_acc_fixed >= test_acc - 0.02, \"Test accuracy should not drop significantly\"\n",
    "# assert stopped_at < 500, \"Early stopping should have stopped before 500 rounds\"\n",
    "# \n",
    "# print(\"âœ“ Boosting overfitting fixed!\")\n",
    "# print(f\"âœ“ Stopped at {stopped_at} rounds instead of 500\")\n",
    "# print(f\"âœ“ Train-test gap reduced from {train_acc - test_acc:.1%} to {train_acc_fixed - test_acc_fixed:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Write your postmortem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postmortem = \"\"\"\n",
    "## Postmortem: The Runaway Booster\n",
    "\n",
    "### What happened:\n",
    "- (Your answer: What symptoms indicated overfitting?)\n",
    "\n",
    "### Root cause:\n",
    "- (Your answer: Which three configuration issues caused overfitting?)\n",
    "\n",
    "### How to prevent:\n",
    "- (Your answer: What should ALWAYS be used with gradient boosting?)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(postmortem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Drill Complete!\n",
    "\n",
    "**Key lessons:**\n",
    "\n",
    "1. **Always use early stopping with boosting.** It's the single most important regularization technique.\n",
    "\n",
    "2. **Keep trees shallow (depth 3-6).** Boosting works by combining many weak learners, not one strong one.\n",
    "\n",
    "3. **Use small learning rates (0.01-0.1).** Slower learning with more rounds generalizes better.\n",
    "\n",
    "4. **Monitor validation loss.** When it stops improving, stop training.\n",
    "\n",
    "---\n",
    "\n",
    "## Boosting Configuration Cheatsheet\n",
    "\n",
    "| Parameter | Dangerous | Safe | Notes |\n",
    "|-----------|-----------|------|-------|\n",
    "| n_estimators | 1000+ without early stopping | 100-500 with early stopping | More is fine IF using early stopping |\n",
    "| learning_rate | 0.3+ | 0.01-0.1 | Lower is safer, needs more rounds |\n",
    "| max_depth | 8+ | 3-6 | Deep trees overfit quickly |\n",
    "| early_stopping | Off | On (10-50 rounds) | ALWAYS use with boosting |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
