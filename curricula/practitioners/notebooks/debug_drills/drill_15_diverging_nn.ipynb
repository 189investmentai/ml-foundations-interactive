{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Drill: The Diverging Neural Network\n",
    "\n",
    "**Scenario:**\n",
    "A colleague is training a neural network for customer churn prediction.\n",
    "\n",
    "\"The loss keeps going up instead of down!\" they say, frustrated.\n",
    "\n",
    "The model is diverging during training, producing NaN or exploding loss values.\n",
    "\n",
    "**Your Task:**\n",
    "1. Identify why the neural network is diverging\n",
    "2. Fix the training issues\n",
    "3. Get the model to converge properly\n",
    "4. Write a 3-bullet postmortem\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate customer churn data\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, \n",
    "    n_features=20, \n",
    "    n_informative=10,\n",
    "    n_redundant=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Make features on different scales (realistic)\n",
    "X[:, 0] *= 1000    # Revenue: ~1000s\n",
    "X[:, 1] *= 100     # Orders: ~100s\n",
    "X[:, 2] *= 0.01    # Rate: ~0.01\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Dataset: {len(X_train)} train, {len(X_test)} test\")\n",
    "print(f\"Feature scales:\")\n",
    "print(f\"  Feature 0 (Revenue): {X[:, 0].min():.0f} to {X[:, 0].max():.0f}\")\n",
    "print(f\"  Feature 1 (Orders):  {X[:, 1].min():.0f} to {X[:, 1].max():.0f}\")\n",
    "print(f\"  Feature 2 (Rate):    {X[:, 2].min():.4f} to {X[:, 2].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== COLLEAGUE'S CODE (BUGS: No scaling, high learning rate) =====\n",
    "\n",
    "# BUG 1: No feature scaling!\n",
    "# BUG 2: Learning rate too high!\n",
    "\n",
    "model_broken = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    learning_rate_init=1.0,      # <-- BUG: WAY too high!\n",
    "    max_iter=100,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Try to train (will fail or produce bad results)\n",
    "try:\n",
    "    model_broken.fit(X_train, y_train)  # <-- NOT SCALED!\n",
    "    train_acc = model_broken.score(X_train, y_train)\n",
    "    test_acc = model_broken.score(X_test, y_test)\n",
    "    final_loss = model_broken.loss_\n",
    "    \n",
    "    print(\"=== Colleague's Results ===\")\n",
    "    print(f\"Final loss: {final_loss:.4f}\")\n",
    "    print(f\"Train accuracy: {train_acc:.1%}\")\n",
    "    print(f\"Test accuracy: {test_acc:.1%}\")\n",
    "    \n",
    "    if final_loss > 1.0 or test_acc < 0.6:\n",
    "        print(\"\\nâŒ Model appears to have diverged or failed to learn!\")\n",
    "except Exception as e:\n",
    "    print(f\"Training failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss curve (if training didn't crash)\n",
    "if hasattr(model_broken, 'loss_curve_') and len(model_broken.loss_curve_) > 0:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(model_broken.loss_curve_, 'r-', linewidth=2)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curve (Diverging or Unstable)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ðŸ” Notice: Loss may be erratic, increasing, or stuck at a high value\")\n",
    "else:\n",
    "    print(\"No loss curve available (training may have failed early)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Your Investigation\n",
    "\n",
    "### Step 1: Identify the issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Bug Analysis ===\")\n",
    "print()\n",
    "print(\"âŒ BUG 1: No feature scaling\")\n",
    "print(\"   Neural networks are sensitive to feature scales\")\n",
    "print(\"   Features range from 0.01 to 1000 - gradients will be unstable\")\n",
    "print()\n",
    "print(\"âŒ BUG 2: Learning rate = 1.0 (WAY too high)\")\n",
    "print(\"   Typical learning rates: 0.001 to 0.01\")\n",
    "print(\"   High LR causes overshooting â†’ divergence\")\n",
    "print()\n",
    "print(\"Combined effect: Exploding gradients â†’ NaN or stuck loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: TODO - Fix the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix the training issues\n",
    "\n",
    "# Uncomment and complete:\n",
    "\n",
    "# # Fix 1: Scale the features\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "# \n",
    "# # Fix 2: Use a reasonable learning rate\n",
    "# model_fixed = MLPClassifier(\n",
    "#     hidden_layer_sizes=(64, 32),\n",
    "#     learning_rate_init=0.001,   # Fixed: reasonable LR\n",
    "#     max_iter=200,\n",
    "#     random_state=42,\n",
    "#     early_stopping=True,        # Added: prevent overfitting\n",
    "#     validation_fraction=0.1\n",
    "# )\n",
    "# \n",
    "# model_fixed.fit(X_train_scaled, y_train)  # Train on SCALED data\n",
    "# \n",
    "# print(\"=== Fixed Model Results ===\")\n",
    "# print(f\"Final loss: {model_fixed.loss_:.4f}\")\n",
    "# print(f\"Train accuracy: {model_fixed.score(X_train_scaled, y_train):.1%}\")\n",
    "# print(f\"Test accuracy: {model_fixed.score(X_test_scaled, y_test):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare loss curves\n",
    "\n",
    "# Uncomment:\n",
    "\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "# \n",
    "# # Broken model\n",
    "# if hasattr(model_broken, 'loss_curve_') and len(model_broken.loss_curve_) > 0:\n",
    "#     axes[0].plot(model_broken.loss_curve_, 'r-', linewidth=2)\n",
    "# axes[0].set_xlabel('Iteration')\n",
    "# axes[0].set_ylabel('Loss')\n",
    "# axes[0].set_title('BROKEN: Unscaled + High LR')\n",
    "# axes[0].grid(True, alpha=0.3)\n",
    "# \n",
    "# # Fixed model\n",
    "# axes[1].plot(model_fixed.loss_curve_, 'g-', linewidth=2)\n",
    "# axes[1].set_xlabel('Iteration')\n",
    "# axes[1].set_ylabel('Loss')\n",
    "# axes[1].set_title('FIXED: Scaled + Reasonable LR')\n",
    "# axes[1].grid(True, alpha=0.3)\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# \n",
    "# print(\"\\nâœ“ Fixed model shows smooth, decreasing loss!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SELF-CHECK\n",
    "# ============================================\n",
    "\n",
    "# Uncomment:\n",
    "\n",
    "# test_acc_fixed = model_fixed.score(X_test_scaled, y_test)\n",
    "# assert test_acc_fixed > 0.7, f\"Fixed model should achieve >70% accuracy, got {test_acc_fixed:.1%}\"\n",
    "# assert model_fixed.loss_ < 0.5, f\"Loss should be low, got {model_fixed.loss_:.3f}\"\n",
    "# \n",
    "# print(\"âœ“ Neural network fixed!\")\n",
    "# print(f\"âœ“ Test accuracy: {test_acc_fixed:.1%}\")\n",
    "# print(f\"âœ“ Final loss: {model_fixed.loss_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Write your postmortem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postmortem = \"\"\"\n",
    "## Postmortem: The Diverging Neural Network\n",
    "\n",
    "### What happened:\n",
    "- (Your answer: What symptoms indicated the model was diverging?)\n",
    "\n",
    "### Root cause:\n",
    "- (Your answer: What two issues caused the divergence?)\n",
    "\n",
    "### How to prevent:\n",
    "- (Your answer: What preprocessing and hyperparameters are essential for NNs?)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(postmortem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Drill Complete!\n",
    "\n",
    "**Key lessons:**\n",
    "\n",
    "1. **Always scale features for neural networks.** They're sensitive to input magnitudes.\n",
    "\n",
    "2. **Start with small learning rates.** 0.001 is a safe default.\n",
    "\n",
    "3. **Watch the loss curve.** Increasing or erratic loss = divergence.\n",
    "\n",
    "4. **Use early stopping.** Prevents overfitting and saves time.\n",
    "\n",
    "---\n",
    "\n",
    "## Neural Network Checklist\n",
    "\n",
    "| Check | Why |\n",
    "|-------|-----|\n",
    "| Scale features (StandardScaler) | NNs are scale-sensitive |\n",
    "| Small learning rate (0.001) | Prevents divergence |\n",
    "| Early stopping | Prevents overfitting |\n",
    "| Monitor loss curve | Detect problems early |\n",
    "| Check train/test gap | Detect overfitting |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
