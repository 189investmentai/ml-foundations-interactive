{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Drill: The Memorizing Tree\n",
    "\n",
    "**Scenario:**\n",
    "A colleague built a decision tree for churn prediction. They're confused by the results.\n",
    "\n",
    "\"The model gets 99% accuracy on training data!\" they say. \"But in production it's only 65%. What's going on?\"\n",
    "\n",
    "**Your Task:**\n",
    "1. Run the model and diagnose the problem\n",
    "2. Identify the overfitting symptoms\n",
    "3. Fix the tree configuration\n",
    "4. Write a 3-bullet postmortem\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "DATA_URL = 'https://raw.githubusercontent.com/189investmentai/ml-foundations-interactive/main/shared/data/'\n",
    "\n",
    "customers = pd.read_csv(DATA_URL + 'streamcart_customers.csv')\n",
    "print(f\"Loaded {len(customers)} customers\")\n",
    "\n",
    "# Prepare features\n",
    "if 'tenure_days' not in customers.columns:\n",
    "    customers['tenure_days'] = (pd.to_datetime('2024-01-01') - pd.to_datetime(customers['signup_date'])).dt.days\n",
    "if 'avg_order_value' not in customers.columns:\n",
    "    customers['avg_order_value'] = customers['total_spend'] / customers['orders_total'].replace(0, 1)\n",
    "\n",
    "feature_cols = ['tenure_days', 'orders_total', 'total_spend', 'support_tickets_total', 'avg_order_value']\n",
    "available_features = [c for c in feature_cols if c in customers.columns]\n",
    "\n",
    "X = customers[available_features].fillna(0)\n",
    "y = customers['churn_30d']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== COLLEAGUE'S CODE (CONTAINS BUG) =====\n",
    "\n",
    "# \"I want the most accurate model possible!\"\n",
    "tree_overfit = DecisionTreeClassifier(\n",
    "    max_depth=20,         # <-- BUG: Way too deep!\n",
    "    min_samples_leaf=1,   # <-- BUG: Allows single-sample leaves\n",
    "    min_samples_split=2,  # <-- BUG: Splits even with 2 samples\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tree_overfit.fit(X_train, y_train)\n",
    "\n",
    "train_acc = accuracy_score(y_train, tree_overfit.predict(X_train))\n",
    "test_acc = accuracy_score(y_test, tree_overfit.predict(X_test))\n",
    "\n",
    "print(\"=== Colleague's Tree ===\")\n",
    "print(f\"  Max Depth Setting: 20\")\n",
    "print(f\"  Actual Depth: {tree_overfit.get_depth()}\")\n",
    "print(f\"  Number of Leaves: {tree_overfit.get_n_leaves()}\")\n",
    "print(f\"\\n  Train Accuracy: {train_acc:.1%}\")\n",
    "print(f\"  Test Accuracy:  {test_acc:.1%}\")\n",
    "print(f\"  Gap: {train_acc - test_acc:.1%} â† HUGE OVERFIT!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Your Investigation\n",
    "\n",
    "The 30%+ gap between train and test accuracy is a classic overfitting symptom.\n",
    "\n",
    "### Step 1: Understand what went wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many leaves does the tree have?\n",
    "print(f\"Number of training samples: {len(X_train)}\")\n",
    "print(f\"Number of tree leaves: {tree_overfit.get_n_leaves()}\")\n",
    "print(f\"\\nðŸ¤” Ratio: {tree_overfit.get_n_leaves() / len(X_train):.1%} of training samples have their own leaf!\")\n",
    "\n",
    "if tree_overfit.get_n_leaves() > len(X_train) * 0.1:\n",
    "    print(\"\\nâŒ This tree has MEMORIZED the training data.\")\n",
    "    print(\"   Each leaf represents a tiny group of similar samples.\")\n",
    "    print(\"   It learned noise, not patterns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the problem: accuracy vs depth\n",
    "depths = range(1, 21)\n",
    "results = []\n",
    "\n",
    "for depth in depths:\n",
    "    tree_temp = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree_temp.fit(X_train, y_train)\n",
    "    \n",
    "    results.append({\n",
    "        'depth': depth,\n",
    "        'train_acc': accuracy_score(y_train, tree_temp.predict(X_train)),\n",
    "        'test_acc': accuracy_score(y_test, tree_temp.predict(X_test)),\n",
    "        'n_leaves': tree_temp.get_n_leaves()\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.plot(results_df['depth'], results_df['train_acc'], 'b-o', label='Train', linewidth=2)\n",
    "ax1.plot(results_df['depth'], results_df['test_acc'], 'r-o', label='Test', linewidth=2)\n",
    "ax1.axvline(x=tree_overfit.get_depth(), color='purple', linestyle='--', alpha=0.5, label='Colleague\\'s depth')\n",
    "ax1.fill_between(results_df['depth'], results_df['train_acc'], results_df['test_acc'], \n",
    "                 alpha=0.2, color='red', label='Overfit gap')\n",
    "ax1.set_xlabel('Max Depth')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Train vs Test Accuracy by Depth')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.plot(results_df['depth'], results_df['n_leaves'], 'g-s', linewidth=2)\n",
    "ax2.set_xlabel('Max Depth')\n",
    "ax2.set_ylabel('Number of Leaves')\n",
    "ax2.set_title('Model Complexity (Leaves) by Depth')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ” Key observation: Test accuracy PEAKS then DROPS as depth increases!\")\n",
    "print(\"   The sweet spot is around depth 3-5, not 20.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal depth\n",
    "optimal_idx = results_df['test_acc'].idxmax()\n",
    "optimal_depth = results_df.loc[optimal_idx, 'depth']\n",
    "\n",
    "print(f\"Optimal depth (by test accuracy): {optimal_depth}\")\n",
    "print(f\"Test accuracy at optimal: {results_df.loc[optimal_idx, 'test_acc']:.1%}\")\n",
    "print(f\"Leaves at optimal: {results_df.loc[optimal_idx, 'n_leaves']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Fix the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a tree with proper constraints\n",
    "\n",
    "# Uncomment and complete:\n",
    "\n",
    "# tree_fixed = DecisionTreeClassifier(\n",
    "#     max_depth=???,           # Use optimal depth from above\n",
    "#     min_samples_leaf=10,     # Require at least 10 samples per leaf\n",
    "#     min_samples_split=20,    # Require at least 20 samples to split\n",
    "#     random_state=42\n",
    "# )\n",
    "# \n",
    "# tree_fixed.fit(X_train, y_train)\n",
    "# \n",
    "# train_acc_fixed = accuracy_score(y_train, tree_fixed.predict(X_train))\n",
    "# test_acc_fixed = accuracy_score(y_test, tree_fixed.predict(X_test))\n",
    "# \n",
    "# print(\"=== Fixed Tree ===\")\n",
    "# print(f\"  Max Depth: {tree_fixed.get_depth()}\")\n",
    "# print(f\"  Number of Leaves: {tree_fixed.get_n_leaves()}\")\n",
    "# print(f\"\\n  Train Accuracy: {train_acc_fixed:.1%}\")\n",
    "# print(f\"  Test Accuracy:  {test_acc_fixed:.1%}\")\n",
    "# print(f\"  Gap: {train_acc_fixed - test_acc_fixed:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare the trees\n",
    "\n",
    "# Uncomment:\n",
    "\n",
    "# print(\"\\n=== Comparison ===\")\n",
    "# print(f\"                    Overfit Tree    Fixed Tree\")\n",
    "# print(f\"  Depth:            {tree_overfit.get_depth():>8}        {tree_fixed.get_depth():>8}\")\n",
    "# print(f\"  Leaves:           {tree_overfit.get_n_leaves():>8}        {tree_fixed.get_n_leaves():>8}\")\n",
    "# print(f\"  Train Acc:        {train_acc:>8.1%}        {train_acc_fixed:>8.1%}\")\n",
    "# print(f\"  Test Acc:         {test_acc:>8.1%}        {test_acc_fixed:>8.1%}\")\n",
    "# print(f\"  Gap:              {train_acc - test_acc:>8.1%}        {train_acc_fixed - test_acc_fixed:>8.1%}\")\n",
    "# print(f\"\\n  Test improvement: +{test_acc_fixed - test_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SELF-CHECK: Did you fix the overfitting?\n",
    "# ============================================\n",
    "\n",
    "# Uncomment:\n",
    "\n",
    "# assert tree_fixed.get_depth() < tree_overfit.get_depth(), \"Fixed tree should be shallower!\"\n",
    "# assert tree_fixed.get_n_leaves() < tree_overfit.get_n_leaves() / 2, \"Fixed tree should have far fewer leaves!\"\n",
    "# assert (train_acc_fixed - test_acc_fixed) < 0.15, \"Train-test gap should be < 15%\"\n",
    "# assert test_acc_fixed >= test_acc - 0.02, \"Test accuracy should not drop significantly\"\n",
    "# \n",
    "# print(\"âœ“ Overfitting fixed!\")\n",
    "# print(f\"âœ“ Reduced from {tree_overfit.get_n_leaves()} leaves to {tree_fixed.get_n_leaves()} leaves\")\n",
    "# print(f\"âœ“ Train-test gap reduced from {train_acc - test_acc:.1%} to {train_acc_fixed - test_acc_fixed:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Visualize the fixed tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the fixed tree (should be readable!)\n",
    "\n",
    "# Uncomment:\n",
    "\n",
    "# plt.figure(figsize=(16, 10))\n",
    "# plot_tree(\n",
    "#     tree_fixed,\n",
    "#     feature_names=available_features,\n",
    "#     class_names=['Retained', 'Churned'],\n",
    "#     filled=True,\n",
    "#     rounded=True,\n",
    "#     fontsize=10\n",
    "# )\n",
    "# plt.title('Fixed Decision Tree (Interpretable!)')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# \n",
    "# print(\"\\nâœ“ This tree is simple enough to explain to stakeholders!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Write your postmortem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postmortem = \"\"\"\n",
    "## Postmortem: The Memorizing Tree\n",
    "\n",
    "### What happened:\n",
    "- (Your answer: What symptoms indicated overfitting?)\n",
    "\n",
    "### Root cause:\n",
    "- (Your answer: Which hyperparameters were wrong and why?)\n",
    "\n",
    "### How to prevent:\n",
    "- (Your answer: What checks should we do before deploying a tree model?)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(postmortem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Drill Complete!\n",
    "\n",
    "**Key lessons:**\n",
    "\n",
    "1. **Deeper â‰  Better.** Very deep trees memorize training noise instead of learning patterns.\n",
    "\n",
    "2. **The symptom:** Train accuracy >> Test accuracy (large gap).\n",
    "\n",
    "3. **The fixes:**\n",
    "   - `max_depth`: Start with 3-5, tune with validation\n",
    "   - `min_samples_leaf`: Require 5-20 samples per leaf\n",
    "   - `min_samples_split`: Require 10-50 samples to split\n",
    "\n",
    "4. **Trade off:** Lower train accuracy for better generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## Overfitting Prevention Cheatsheet\n",
    "\n",
    "| Symptom | Likely Cause | Fix |\n",
    "|---------|-------------|-----|\n",
    "| Train >> Test accuracy | Tree too deep | Reduce `max_depth` |\n",
    "| Too many leaves | No leaf size constraint | Increase `min_samples_leaf` |\n",
    "| Fragile predictions | Overfitting to noise | Increase `min_samples_split` |\n",
    "| 100% train accuracy | Perfect memorization | All of the above |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
