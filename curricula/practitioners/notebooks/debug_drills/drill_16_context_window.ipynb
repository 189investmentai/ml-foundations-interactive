{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Drill: The Forgotten Context\n",
    "\n",
    "**Scenario:**\n",
    "Your support chatbot is giving inconsistent answers.\n",
    "\n",
    "\"I told it my order number at the start, but then it asked me for it again!\" users complain.\n",
    "\n",
    "The bot seems to \"forget\" information from earlier in the conversation.\n",
    "\n",
    "**Your Task:**\n",
    "1. Understand why the transformer forgets early context\n",
    "2. Simulate context window limitations\n",
    "3. Implement a solution\n",
    "4. Write a 3-bullet postmortem\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a conversation that exceeds context window\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi, I need help with order ORD-12345\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'd be happy to help with order ORD-12345. What's the issue?\"},\n",
    "    {\"role\": \"user\", \"content\": \"The item arrived damaged. Can I get a replacement?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm sorry to hear that. Yes, you're eligible for a free replacement. I'll start the process now.\"},\n",
    "    # ... many more exchanges about the replacement process ...\n",
    "    {\"role\": \"user\", \"content\": \"Great, thanks! Can you also check if there's a shipping discount on my account?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Let me check your account for any available discounts.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Also, I noticed I was charged twice last month. Can you look into that?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'll investigate the billing issue. Can you confirm your account email?\"},\n",
    "    {\"role\": \"user\", \"content\": \"It's user@example.com. By the way, when will the replacement arrive?\"},\n",
    "    # Bot forgets the order number from the beginning!\n",
    "    {\"role\": \"assistant\", \"content\": \"To check the replacement status, could you please provide your order number?\"},  # <-- BUG!\n",
    "]\n",
    "\n",
    "print(\"=== Conversation (Notice the Bug) ===\")\n",
    "for msg in conversation:\n",
    "    prefix = \"User:\" if msg['role'] == 'user' else \"Bot:\"\n",
    "    print(f\"{prefix} {msg['content']}\")\n",
    "    if \"could you please provide your order number\" in msg['content']:\n",
    "        print(\"\\n❌ BUG: Bot asked for order number that was given at the start!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== COLLEAGUE'S CODE (BUG: No context management) =====\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Rough token count (words * 1.3 for English).\"\"\"\n",
    "    return int(len(text.split()) * 1.3)\n",
    "\n",
    "def format_conversation(messages):\n",
    "    \"\"\"Format conversation for LLM.\"\"\"\n",
    "    return \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages])\n",
    "\n",
    "# Simulate a small context window (like an older model)\n",
    "MAX_CONTEXT_TOKENS = 100  # Very small for demo\n",
    "\n",
    "def naive_chat(messages, max_tokens=MAX_CONTEXT_TOKENS):\n",
    "    \"\"\"Naive approach: just truncate from the beginning.\"\"\"\n",
    "    formatted = format_conversation(messages)\n",
    "    total_tokens = count_tokens(formatted)\n",
    "    \n",
    "    # Simple truncation: drop early messages\n",
    "    while count_tokens(format_conversation(messages)) > max_tokens and len(messages) > 2:\n",
    "        messages = messages[1:]  # Drop oldest message\n",
    "    \n",
    "    return messages\n",
    "\n",
    "# Simulate what the model sees\n",
    "truncated = naive_chat(conversation.copy())\n",
    "\n",
    "print(\"=== What the Model Actually Sees (After Truncation) ===\")\n",
    "print(f\"Original messages: {len(conversation)}\")\n",
    "print(f\"After truncation: {len(truncated)}\")\n",
    "print()\n",
    "for msg in truncated:\n",
    "    print(f\"{msg['role']}: {msg['content'][:50]}...\")\n",
    "\n",
    "print(\"\\n❌ The order number from the first message is GONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Your Investigation\n",
    "\n",
    "### Step 1: Understand context window limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Context Window Analysis ===\")\n",
    "print()\n",
    "print(\"Transformers have a fixed context window (max input tokens).\")\n",
    "print(\"When conversation exceeds this, something must be dropped.\")\n",
    "print()\n",
    "print(\"Naive approach: Drop oldest messages\")\n",
    "print(\"  ❌ Loses important early context (order numbers, names)\")\n",
    "print()\n",
    "print(\"Better approaches:\")\n",
    "print(\"  1. Summarize early conversation\")\n",
    "print(\"  2. Extract and preserve key facts\")\n",
    "print(\"  3. Use sliding window with summary\")\n",
    "print(\"  4. Store facts in external memory (database)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: TODO - Implement smart context management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract key facts from conversation\n",
    "\n",
    "# Uncomment and complete:\n",
    "\n",
    "# import re\n",
    "# \n",
    "# def extract_key_facts(messages):\n",
    "#     \"\"\"Extract important facts that should never be forgotten.\"\"\"\n",
    "#     facts = {}\n",
    "#     \n",
    "#     for msg in messages:\n",
    "#         content = msg['content']\n",
    "#         \n",
    "#         # Extract order numbers\n",
    "#         order_match = re.search(r'ORD-\\d+', content)\n",
    "#         if order_match:\n",
    "#             facts['order_id'] = order_match.group()\n",
    "#         \n",
    "#         # Extract email\n",
    "#         email_match = re.search(r'[\\w.-]+@[\\w.-]+\\.\\w+', content)\n",
    "#         if email_match:\n",
    "#             facts['email'] = email_match.group()\n",
    "#         \n",
    "#         # Extract issues mentioned\n",
    "#         if 'damaged' in content.lower():\n",
    "#             facts['issue'] = 'damaged item'\n",
    "#         if 'replacement' in content.lower():\n",
    "#             facts['resolution'] = 'replacement requested'\n",
    "#     \n",
    "#     return facts\n",
    "# \n",
    "# facts = extract_key_facts(conversation)\n",
    "# print(\"=== Extracted Key Facts ===\")\n",
    "# for key, value in facts.items():\n",
    "#     print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build context with preserved facts\n",
    "\n",
    "# Uncomment:\n",
    "\n",
    "# def smart_context(messages, facts, max_tokens=MAX_CONTEXT_TOKENS):\n",
    "#     \"\"\"\n",
    "#     Build context that preserves key facts.\n",
    "#     \n",
    "#     Structure:\n",
    "#     1. System message with key facts\n",
    "#     2. Recent conversation (as much as fits)\n",
    "#     \"\"\"\n",
    "#     # Create system context with facts\n",
    "#     facts_summary = \"Key facts from this conversation:\\n\"\n",
    "#     for key, value in facts.items():\n",
    "#         facts_summary += f\"- {key}: {value}\\n\"\n",
    "#     \n",
    "#     system_msg = {\"role\": \"system\", \"content\": facts_summary}\n",
    "#     system_tokens = count_tokens(facts_summary)\n",
    "#     \n",
    "#     # Fill remaining space with recent messages\n",
    "#     remaining_tokens = max_tokens - system_tokens\n",
    "#     recent_messages = []\n",
    "#     \n",
    "#     for msg in reversed(messages):\n",
    "#         msg_tokens = count_tokens(msg['content'])\n",
    "#         if msg_tokens <= remaining_tokens:\n",
    "#             recent_messages.insert(0, msg)\n",
    "#             remaining_tokens -= msg_tokens\n",
    "#         else:\n",
    "#             break\n",
    "#     \n",
    "#     return [system_msg] + recent_messages\n",
    "# \n",
    "# smart = smart_context(conversation, facts)\n",
    "# \n",
    "# print(\"=== Smart Context (Facts Preserved) ===\")\n",
    "# for msg in smart:\n",
    "#     print(f\"{msg['role']}: {msg['content'][:60]}...\" if len(msg['content']) > 60 else f\"{msg['role']}: {msg['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SELF-CHECK\n",
    "# ============================================\n",
    "\n",
    "# Uncomment:\n",
    "\n",
    "# assert 'order_id' in facts, \"Should extract order ID\"\n",
    "# assert facts['order_id'] == 'ORD-12345', f\"Order ID should be ORD-12345, got {facts.get('order_id')}\"\n",
    "# \n",
    "# # Check that smart context includes the order\n",
    "# context_text = ' '.join([m['content'] for m in smart])\n",
    "# assert 'ORD-12345' in context_text, \"Smart context should preserve order ID\"\n",
    "# \n",
    "# print(\"✓ Context management fixed!\")\n",
    "# print(f\"✓ Order ID preserved: {facts['order_id']}\")\n",
    "# print(f\"✓ Context includes key facts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Write your postmortem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postmortem = \"\"\"\n",
    "## Postmortem: The Forgotten Context\n",
    "\n",
    "### What happened:\n",
    "- (Your answer: What user complaint indicated the bot was forgetting context?)\n",
    "\n",
    "### Root cause:\n",
    "- (Your answer: Why do transformers forget early conversation?)\n",
    "\n",
    "### How to prevent:\n",
    "- (Your answer: What strategies preserve important context?)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(postmortem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ✅ Drill Complete!\n",
    "\n",
    "**Key lessons:**\n",
    "\n",
    "1. **Transformers have fixed context windows.** They can only see a limited amount of text.\n",
    "\n",
    "2. **Naive truncation loses important early context.** Order numbers, names, issues get dropped.\n",
    "\n",
    "3. **Extract and preserve key facts.** Use regex or NER to identify important entities.\n",
    "\n",
    "4. **Add facts to system prompt.** Ensures they're always in context.\n",
    "\n",
    "---\n",
    "\n",
    "## Context Management Strategies\n",
    "\n",
    "| Strategy | Pros | Cons |\n",
    "|----------|------|------|\n",
    "| Naive truncation | Simple | Loses early context |\n",
    "| Summarization | Preserves gist | Loses specifics |\n",
    "| Fact extraction | Keeps key info | Requires parsing |\n",
    "| External memory | Unlimited history | Added complexity |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
