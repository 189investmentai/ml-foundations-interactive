{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Drill: The Hallucinating RAG\n",
    "\n",
    "**Scenario:**\n",
    "Your RAG-powered support bot is giving confident but wrong answers.\n",
    "\n",
    "\"It told me I have 60 days for a refund, but your policy says 30 days!\" a customer complains.\n",
    "\n",
    "Despite having the correct information in the knowledge base, the bot is hallucinating details.\n",
    "\n",
    "**Your Task:**\n",
    "1. Identify where the hallucination is occurring\n",
    "2. Implement faithfulness checking\n",
    "3. Fix the prompt to reduce hallucination\n",
    "4. Write a 3-bullet postmortem\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Note: In a real system, you'd use an actual LLM API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge base with CORRECT information\n",
    "knowledge_base = {\n",
    "    \"refund_policy\": {\n",
    "        \"title\": \"Refund Policy\",\n",
    "        \"content\": \"We offer full refunds within 30 days of purchase. After 30 days, only store credit is available. Items must be unused and in original packaging.\"\n",
    "    },\n",
    "    \"shipping\": {\n",
    "        \"title\": \"Shipping Information\", \n",
    "        \"content\": \"Standard shipping takes 5-7 business days. Express shipping takes 2-3 business days. Free shipping on orders over $50.\"\n",
    "    },\n",
    "    \"return_process\": {\n",
    "        \"title\": \"How to Return\",\n",
    "        \"content\": \"Start a return from Order History. Select the item, click Return, and print the prepaid label. Drop off at any UPS location.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=== Knowledge Base (Ground Truth) ===\")\n",
    "for key, doc in knowledge_base.items():\n",
    "    print(f\"\\n{doc['title']}:\")\n",
    "    print(f\"  {doc['content'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== COLLEAGUE'S CODE (BUG: Weak prompt allows hallucination) =====\n",
    "\n",
    "def weak_rag_prompt(query: str, context: str) -> str:\n",
    "    \"\"\"Weak prompt that allows hallucination.\"\"\"\n",
    "    return f\"\"\"You are a helpful support agent.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Simulated LLM response (hallucinating!)\n",
    "def simulate_weak_llm(query: str, context: str) -> str:\n",
    "    \"\"\"Simulate an LLM that hallucinate details not in context.\"\"\"\n",
    "    # This simulates common hallucination patterns\n",
    "    hallucinated_responses = {\n",
    "        \"refund\": \"You have 60 days to request a full refund. After that, you can still get 50% back for another 30 days. Just contact support!\",  # WRONG: says 60 days\n",
    "        \"shipping\": \"Standard shipping is 3-5 days, express is next day. All orders over $25 get free shipping!\",  # WRONG: different numbers\n",
    "        \"return\": \"You can return items by mailing them back or dropping at any FedEx or UPS location. We'll process your refund in 24 hours.\",  # WRONG: adds FedEx, 24 hours\n",
    "    }\n",
    "    \n",
    "    for key, response in hallucinated_responses.items():\n",
    "        if key in query.lower():\n",
    "            return response\n",
    "    \n",
    "    return \"I'm not sure about that. Please contact support.\"\n",
    "\n",
    "# Test the broken RAG\n",
    "query = \"What's your refund policy?\"\n",
    "context = knowledge_base['refund_policy']['content']\n",
    "\n",
    "print(\"=== Broken RAG Response ===\")\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(f\"\\nContext (ground truth):\")\n",
    "print(f\"  {context}\")\n",
    "\n",
    "response = simulate_weak_llm(query, context)\n",
    "print(f\"\\nLLM Response (HALLUCINATED):\")\n",
    "print(f\"  {response}\")\n",
    "\n",
    "print(\"\\n❌ Response says 60 days, but policy says 30 days!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Your Investigation\n",
    "\n",
    "### Step 1: Implement faithfulness checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_number_faithfulness(response: str, context: str) -> Tuple[bool, List[str]]:\n",
    "    \"\"\"\n",
    "    Check if numbers in the response appear in the context.\n",
    "    Common hallucination: making up specific numbers.\n",
    "    \"\"\"\n",
    "    # Extract numbers from both\n",
    "    response_numbers = set(re.findall(r'\\d+', response))\n",
    "    context_numbers = set(re.findall(r'\\d+', context))\n",
    "    \n",
    "    # Find hallucinated numbers\n",
    "    hallucinated = response_numbers - context_numbers\n",
    "    \n",
    "    is_faithful = len(hallucinated) == 0\n",
    "    return is_faithful, list(hallucinated)\n",
    "\n",
    "# Test faithfulness checker\n",
    "is_faithful, issues = check_number_faithfulness(response, context)\n",
    "\n",
    "print(\"=== Faithfulness Check ===\")\n",
    "print(f\"\\nContext numbers: {set(re.findall(r'd+', context))}\")\n",
    "print(f\"Response numbers: {set(re.findall(r'd+', response))}\")\n",
    "print(f\"\\nIs faithful: {is_faithful}\")\n",
    "if not is_faithful:\n",
    "    print(f\"❌ Hallucinated numbers: {issues}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: TODO - Create a stronger prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a prompt that prevents hallucination\n",
    "\n",
    "# Uncomment and complete:\n",
    "\n",
    "# def strong_rag_prompt(query: str, context: str) -> str:\n",
    "#     \"\"\"Strong prompt that reduces hallucination.\"\"\"\n",
    "#     return f\"\"\"You are a support agent. Answer questions using ONLY the information provided below.\n",
    "#     \n",
    "# IMPORTANT RULES:\n",
    "# 1. Only use facts from the CONTEXT below\n",
    "# 2. Do NOT add information that isn't explicitly stated\n",
    "# 3. If the context doesn't contain the answer, say \"I don't have that information\"\n",
    "# 4. Quote specific numbers and policies exactly as stated\n",
    "# \n",
    "# CONTEXT:\n",
    "# {context}\n",
    "# \n",
    "# QUESTION: {query}\n",
    "# \n",
    "# ANSWER (using only the context above):\"\"\"\n",
    "# \n",
    "# # Display the improved prompt\n",
    "# print(\"=== Improved Prompt ===\")\n",
    "# print(strong_rag_prompt(query, context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Simulate a faithful response\n",
    "\n",
    "# Uncomment:\n",
    "\n",
    "# def simulate_faithful_llm(query: str, context: str) -> str:\n",
    "#     \"\"\"Simulate an LLM that only uses provided context.\"\"\"\n",
    "#     # Extract key facts from context\n",
    "#     if \"30 days\" in context:\n",
    "#         return \"According to our policy, we offer full refunds within 30 days of purchase. After 30 days, only store credit is available. Items must be unused and in original packaging.\"\n",
    "#     elif \"5-7\" in context:\n",
    "#         return \"Standard shipping takes 5-7 business days, and express shipping takes 2-3 business days. Free shipping is available on orders over $50.\"\n",
    "#     else:\n",
    "#         return \"Based on the information provided: \" + context\n",
    "# \n",
    "# faithful_response = simulate_faithful_llm(query, context)\n",
    "# \n",
    "# print(\"=== Faithful Response ===\")\n",
    "# print(f\"\\n{faithful_response}\")\n",
    "# \n",
    "# # Check faithfulness\n",
    "# is_faithful, issues = check_number_faithfulness(faithful_response, context)\n",
    "# print(f\"\\n✓ Is faithful: {is_faithful}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement response validation pipeline\n",
    "\n",
    "# Uncomment:\n",
    "\n",
    "# def validate_response(response: str, context: str) -> Dict:\n",
    "#     \"\"\"Validate that response is faithful to context.\"\"\"\n",
    "#     result = {\n",
    "#         'response': response,\n",
    "#         'checks': []\n",
    "#     }\n",
    "#     \n",
    "#     # Check 1: Number faithfulness\n",
    "#     is_faithful, hallucinated = check_number_faithfulness(response, context)\n",
    "#     result['checks'].append({\n",
    "#         'name': 'number_faithfulness',\n",
    "#         'passed': is_faithful,\n",
    "#         'issues': hallucinated\n",
    "#     })\n",
    "#     \n",
    "#     # Check 2: No \"I think\" or uncertainty markers combined with specific claims\n",
    "#     uncertain_specific = bool(re.search(r'(I think|probably|maybe|around).*\\d+', response))\n",
    "#     result['checks'].append({\n",
    "#         'name': 'no_uncertain_specifics',\n",
    "#         'passed': not uncertain_specific,\n",
    "#         'issues': ['Combines uncertainty with specific numbers'] if uncertain_specific else []\n",
    "#     })\n",
    "#     \n",
    "#     result['all_passed'] = all(c['passed'] for c in result['checks'])\n",
    "#     return result\n",
    "# \n",
    "# # Test validation\n",
    "# print(\"=== Response Validation ===\")\n",
    "# print(\"\\nHallucinated response:\")\n",
    "# validation = validate_response(response, context)\n",
    "# for check in validation['checks']:\n",
    "#     status = '✓' if check['passed'] else '❌'\n",
    "#     print(f\"  {status} {check['name']}: {check['issues'] if check['issues'] else 'OK'}\")\n",
    "# \n",
    "# print(\"\\nFaithful response:\")\n",
    "# validation_good = validate_response(faithful_response, context)\n",
    "# for check in validation_good['checks']:\n",
    "#     status = '✓' if check['passed'] else '❌'\n",
    "#     print(f\"  {status} {check['name']}: {check['issues'] if check['issues'] else 'OK'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SELF-CHECK\n",
    "# ============================================\n",
    "\n",
    "# Uncomment:\n",
    "\n",
    "# assert callable(check_number_faithfulness), \"Should have faithfulness checker\"\n",
    "# assert callable(strong_rag_prompt), \"Should have improved prompt\"\n",
    "# assert validation_good['all_passed'], \"Faithful response should pass all checks\"\n",
    "# \n",
    "# print(\"✓ Hallucination detection implemented!\")\n",
    "# print(\"✓ Strong prompt created\")\n",
    "# print(\"✓ Validation pipeline working\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Write your postmortem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postmortem = \"\"\"\n",
    "## Postmortem: The Hallucinating RAG\n",
    "\n",
    "### What happened:\n",
    "- (Your answer: What incorrect information did the bot give?)\n",
    "\n",
    "### Root cause:\n",
    "- (Your answer: Why did the LLM hallucinate despite having correct context?)\n",
    "\n",
    "### How to prevent:\n",
    "- (Your answer: What prompt techniques and validations reduce hallucination?)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(postmortem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ✅ Drill Complete!\n",
    "\n",
    "**Key lessons:**\n",
    "\n",
    "1. **RAG doesn't prevent hallucination automatically.** The LLM can still make things up.\n",
    "\n",
    "2. **Strong prompts constrain the model.** Explicitly tell it to only use provided context.\n",
    "\n",
    "3. **Validate outputs.** Check that specific claims (numbers, dates) appear in context.\n",
    "\n",
    "4. **Numbers are common hallucination targets.** Always verify numerical claims.\n",
    "\n",
    "---\n",
    "\n",
    "## Anti-Hallucination Checklist\n",
    "\n",
    "| Technique | Purpose |\n",
    "|-----------|----------|\n",
    "| \"Only use provided context\" | Constrains model |\n",
    "| \"Say 'I don't know' if unsure\" | Prevents guessing |\n",
    "| Number verification | Catches made-up stats |\n",
    "| Quote exact policy text | Forces faithfulness |\n",
    "| LLM-as-judge | Automated validation |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
