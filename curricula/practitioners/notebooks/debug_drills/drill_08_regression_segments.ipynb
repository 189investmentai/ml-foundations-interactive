{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Drill: The Hidden Segments\n",
    "\n",
    "**Scenario:**\n",
    "A colleague built an LTV prediction model for StreamCart. They're thrilled with the results.\n",
    "\n",
    "\"R¬≤ = 0.85 and MAE = $40!\" they report. \"We're ready to deploy!\"\n",
    "\n",
    "But when the marketing team uses it, they complain: \"Your predictions for our premium customers are terrible!\"\n",
    "\n",
    "**Your Task:**\n",
    "1. Run the model and verify the overall metrics look good\n",
    "2. Investigate segment-level performance\n",
    "3. Diagnose why certain segments fail\n",
    "4. Write a 3-bullet postmortem\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic customer data with hidden segment structure\n",
    "n_samples = 1000\n",
    "\n",
    "# Create three customer segments with different relationships\n",
    "segments = np.random.choice(['Standard', 'Premium', 'Enterprise'], n_samples, p=[0.7, 0.2, 0.1])\n",
    "\n",
    "# Features\n",
    "tenure_months = np.random.uniform(1, 36, n_samples)\n",
    "monthly_spend = np.random.uniform(20, 200, n_samples)\n",
    "orders = np.random.poisson(5, n_samples)\n",
    "\n",
    "# Generate LTV with DIFFERENT patterns per segment (the hidden bug)\n",
    "ltv = np.zeros(n_samples)\n",
    "\n",
    "for i, seg in enumerate(segments):\n",
    "    if seg == 'Standard':\n",
    "        # Linear relationship\n",
    "        ltv[i] = 50 + 10 * tenure_months[i] + 2 * monthly_spend[i] + np.random.normal(0, 30)\n",
    "    elif seg == 'Premium':\n",
    "        # QUADRATIC relationship (model can't capture this!)\n",
    "        ltv[i] = 200 + 5 * tenure_months[i]**1.5 + 3 * monthly_spend[i] + np.random.normal(0, 50)\n",
    "    else:  # Enterprise\n",
    "        # Very high, different scale\n",
    "        ltv[i] = 1000 + 50 * tenure_months[i] + 10 * monthly_spend[i] + np.random.normal(0, 200)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'tenure_months': tenure_months,\n",
    "    'monthly_spend': monthly_spend,\n",
    "    'orders': orders,\n",
    "    'segment': segments,\n",
    "    'ltv': ltv\n",
    "})\n",
    "\n",
    "print(f\"Dataset: {len(df)} customers\")\n",
    "print(f\"\\nSegment distribution:\")\n",
    "print(df['segment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== COLLEAGUE'S CODE =====\n",
    "\n",
    "# Train a simple linear regression on ALL data\n",
    "X = df[['tenure_months', 'monthly_spend', 'orders']]\n",
    "y = df['ltv']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Overall metrics look great!\n",
    "print(\"=== Colleague's Report ===\")\n",
    "print(f\"\\nOverall Test Metrics:\")\n",
    "print(f\"  R¬≤:  {r2_score(y_test, y_pred):.3f}\")\n",
    "print(f\"  MAE: ${mean_absolute_error(y_test, y_pred):.2f}\")\n",
    "print(f\"  RMSE: ${np.sqrt(mean_squared_error(y_test, y_pred)):.2f}\")\n",
    "print(\"\\n‚úÖ Looks good! Ready to deploy...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Your Investigation\n",
    "\n",
    "The marketing team says the model fails for premium customers. Let's dig deeper.\n",
    "\n",
    "### Step 1: Analyze by segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get segment labels for test set\n",
    "test_idx = X_test.index\n",
    "test_segments = df.loc[test_idx, 'segment']\n",
    "\n",
    "# Calculate metrics per segment\n",
    "print(\"=== Segment-Level Performance ===\")\n",
    "print(f\"{'Segment':<12} {'Count':<8} {'MAE':<12} {'RMSE':<12} {'R¬≤':<8}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for seg in ['Standard', 'Premium', 'Enterprise']:\n",
    "    mask = test_segments == seg\n",
    "    if mask.sum() > 0:\n",
    "        y_true_seg = y_test[mask]\n",
    "        y_pred_seg = y_pred[mask]\n",
    "        \n",
    "        mae = mean_absolute_error(y_true_seg, y_pred_seg)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_seg, y_pred_seg))\n",
    "        r2 = r2_score(y_true_seg, y_pred_seg)\n",
    "        \n",
    "        flag = \"‚ùå\" if r2 < 0.5 else \"‚úì\"\n",
    "        print(f\"{seg:<12} {mask.sum():<8} ${mae:<10.2f} ${rmse:<10.2f} {r2:<8.3f} {flag}\")\n",
    "\n",
    "print(\"\\nüîç Key Finding: Performance varies DRAMATICALLY by segment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the problem: residuals by segment\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot of residuals by segment\n",
    "ax1 = axes[0]\n",
    "segment_residuals = [residuals[test_segments == seg].values for seg in ['Standard', 'Premium', 'Enterprise']]\n",
    "ax1.boxplot(segment_residuals, labels=['Standard', 'Premium', 'Enterprise'])\n",
    "ax1.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "ax1.set_ylabel('Residual (Actual - Predicted)')\n",
    "ax1.set_title('Residual Distribution by Segment')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter: predicted vs actual by segment\n",
    "ax2 = axes[1]\n",
    "colors = {'Standard': '#3b82f6', 'Premium': '#f97316', 'Enterprise': '#22c55e'}\n",
    "for seg in ['Standard', 'Premium', 'Enterprise']:\n",
    "    mask = test_segments == seg\n",
    "    ax2.scatter(y_test[mask], y_pred[mask], alpha=0.5, label=seg, c=colors[seg])\n",
    "\n",
    "# Perfect prediction line\n",
    "ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', label='Perfect')\n",
    "ax2.set_xlabel('Actual LTV')\n",
    "ax2.set_ylabel('Predicted LTV')\n",
    "ax2.set_title('Predicted vs Actual by Segment')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç Notice: Enterprise predictions are systematically LOW (below the diagonal)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Diagnose the root cause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the actual LTV distribution by segment\n",
    "print(\"=== LTV Statistics by Segment ===\")\n",
    "print(df.groupby('segment')['ltv'].describe().round(1))\n",
    "\n",
    "print(\"\\nüîç Root Cause Analysis:\")\n",
    "print(\"  1. Enterprise customers have MUCH higher LTV (different scale)\")\n",
    "print(\"  2. Premium customers have non-linear LTV pattern\")\n",
    "print(\"  3. A single linear model can't capture both patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: TODO - Propose a fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train separate models per segment and compare\n",
    "\n",
    "# Uncomment and complete:\n",
    "\n",
    "# print(\"=== Segment-Specific Models ===\")\n",
    "# \n",
    "# y_pred_fixed = pd.Series(index=X_test.index, dtype=float)\n",
    "# \n",
    "# for seg in ['Standard', 'Premium', 'Enterprise']:\n",
    "#     # Train on segment\n",
    "#     train_mask = df.loc[X_train.index, 'segment'] == seg\n",
    "#     test_mask = df.loc[X_test.index, 'segment'] == seg\n",
    "#     \n",
    "#     if train_mask.sum() > 10:  # Need enough samples\n",
    "#         seg_model = LinearRegression()\n",
    "#         seg_model.fit(X_train[train_mask], y_train[train_mask])\n",
    "#         \n",
    "#         seg_pred = seg_model.predict(X_test[test_mask])\n",
    "#         y_pred_fixed[test_mask] = seg_pred\n",
    "#         \n",
    "#         mae = mean_absolute_error(y_test[test_mask], seg_pred)\n",
    "#         r2 = r2_score(y_test[test_mask], seg_pred)\n",
    "#         print(f\"  {seg}: MAE=${mae:.2f}, R¬≤={r2:.3f}\")\n",
    "# \n",
    "# # Compare overall\n",
    "# print(f\"\\nOverall (segment models): MAE=${mean_absolute_error(y_test, y_pred_fixed):.2f}\")\n",
    "# print(f\"Overall (single model):   MAE=${mean_absolute_error(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SELF-CHECK: Did you improve segment performance?\n",
    "# ============================================\n",
    "\n",
    "# Uncomment after completing:\n",
    "\n",
    "# assert 'y_pred_fixed' in dir(), \"Should have created segment-specific predictions\"\n",
    "# mae_original = mean_absolute_error(y_test, y_pred)\n",
    "# mae_fixed = mean_absolute_error(y_test, y_pred_fixed)\n",
    "# assert mae_fixed < mae_original, f\"Segment models ({mae_fixed:.1f}) should beat single model ({mae_original:.1f})\"\n",
    "# \n",
    "# print(\"‚úì Segment-specific models improved overall MAE!\")\n",
    "# print(f\"  Original: ${mae_original:.2f}\")\n",
    "# print(f\"  Fixed: ${mae_fixed:.2f}\")\n",
    "# print(f\"  Improvement: {(mae_original - mae_fixed) / mae_original * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Write your postmortem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postmortem = \"\"\"\n",
    "## Postmortem: The Hidden Segments\n",
    "\n",
    "### What happened:\n",
    "- (Your answer: What symptom did the marketing team observe?)\n",
    "\n",
    "### Root cause:\n",
    "- (Your answer: Why did good overall metrics hide poor segment performance?)\n",
    "\n",
    "### How to prevent:\n",
    "- (Your answer: What should we check before deploying a regression model?)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(postmortem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Drill Complete!\n",
    "\n",
    "**Key lessons:**\n",
    "\n",
    "1. **Overall metrics can hide segment-level failures.** A model can have great R¬≤ overall but terrible performance on important subgroups.\n",
    "\n",
    "2. **Always check performance by segment.** Business users care about specific segments, not just averages.\n",
    "\n",
    "3. **Consider segment-specific models** when different groups have different patterns.\n",
    "\n",
    "4. **Beware of scale differences.** Enterprise customers with 10x higher LTV can dominate MAE/RMSE.\n",
    "\n",
    "---\n",
    "\n",
    "## Segment Analysis Checklist\n",
    "\n",
    "| Check | Why It Matters |\n",
    "|-------|----------------|\n",
    "| MAE by segment | Different value groups have different tolerances |\n",
    "| R¬≤ by segment | Model may not capture patterns for all groups |\n",
    "| Residual distribution | Systematic bias indicates model limitations |\n",
    "| MAPE by segment | Percentage error normalizes across scales |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
