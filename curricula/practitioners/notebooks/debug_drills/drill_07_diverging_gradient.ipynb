{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Drill: The Exploding Loss\n",
    "\n",
    "**Scenario:**\n",
    "A colleague is training a model using gradient descent. They're frustrated.\n",
    "\n",
    "\"My loss went from 2.5 to 50 to infinity!\" they say. \"The training is broken!\"\n",
    "\n",
    "**Your Task:**\n",
    "1. Run the training and observe the problem\n",
    "2. Diagnose why the loss is exploding\n",
    "3. Fix the hyperparameters\n",
    "4. Write a 3-bullet postmortem\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple quadratic loss surface for demonstration\n",
    "def loss_fn(x, y):\n",
    "    \"\"\"Bowl-shaped loss surface. Minimum at (0, 0).\"\"\"\n",
    "    return x**2 + y**2\n",
    "\n",
    "def gradient_fn(x, y):\n",
    "    \"\"\"Gradient of the loss function.\"\"\"\n",
    "    return np.array([2*x, 2*y])\n",
    "\n",
    "def gradient_descent(start, lr, n_steps=50):\n",
    "    \"\"\"Run vanilla gradient descent.\"\"\"\n",
    "    position = np.array(start, dtype=float)\n",
    "    path = [position.copy()]\n",
    "    losses = [loss_fn(position[0], position[1])]\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        grad = gradient_fn(position[0], position[1])\n",
    "        position = position - lr * grad\n",
    "        path.append(position.copy())\n",
    "        losses.append(loss_fn(position[0], position[1]))\n",
    "    \n",
    "    return np.array(path), np.array(losses)\n",
    "\n",
    "print(\"✓ Functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== COLLEAGUE'S CODE (CONTAINS BUG) =====\n",
    "\n",
    "# \"I want to train faster, so I'll use a big learning rate!\"\n",
    "\n",
    "START_POSITION = [-2.0, 2.0]\n",
    "LEARNING_RATE = 1.2  # <-- BUG: Way too high!\n",
    "\n",
    "path_bad, losses_bad = gradient_descent(START_POSITION, LEARNING_RATE, n_steps=20)\n",
    "\n",
    "print(\"=== Colleague's Training ===\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Starting position: {START_POSITION}\")\n",
    "print(f\"\\nLoss over training:\")\n",
    "for i in range(min(10, len(losses_bad))):\n",
    "    if np.isfinite(losses_bad[i]):\n",
    "        print(f\"  Step {i}: {losses_bad[i]:.2f}\")\n",
    "    else:\n",
    "        print(f\"  Step {i}: EXPLODED!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what's happening\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = loss_fn(X, Y)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Path on contour plot\n",
    "ax1 = axes[0]\n",
    "ax1.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.5)\n",
    "path_clipped = np.clip(path_bad, -5, 5)\n",
    "ax1.plot(path_clipped[:, 0], path_clipped[:, 1], 'r-o', markersize=4, linewidth=1, label='GD Path')\n",
    "ax1.scatter([0], [0], color='green', s=100, marker='*', zorder=5, label='Minimum')\n",
    "ax1.scatter([START_POSITION[0]], [START_POSITION[1]], color='blue', s=100, marker='s', zorder=5, label='Start')\n",
    "ax1.set_xlabel('θ₁')\n",
    "ax1.set_ylabel('θ₂')\n",
    "ax1.set_title(f'Gradient Descent Path (lr={LEARNING_RATE})')\n",
    "ax1.legend()\n",
    "ax1.set_xlim(-5, 5)\n",
    "ax1.set_ylim(-5, 5)\n",
    "\n",
    "# Loss curve\n",
    "ax2 = axes[1]\n",
    "losses_clipped = np.clip(losses_bad, 0, 100)\n",
    "ax2.plot(losses_clipped, 'r-', linewidth=2)\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Loss Over Time (EXPLODING!)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n❌ The optimizer is DIVERGING!\")\n",
    "print(\"   Instead of descending toward the minimum, it's overshooting and bouncing away.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Your Investigation\n",
    "\n",
    "The loss is exploding instead of decreasing. Classic sign of a learning rate that's too high.\n",
    "\n",
    "### Step 1: Understand the math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why does this happen?\n",
    "print(\"=== Why Learning Rate Matters ===\")\n",
    "print()\n",
    "print(\"For a quadratic loss f(x) = x², the gradient is g(x) = 2x\")\n",
    "print()\n",
    "print(\"Update rule: x_new = x - lr * gradient\")\n",
    "print(\"           = x - lr * 2x\")\n",
    "print(\"           = x * (1 - 2*lr)\")\n",
    "print()\n",
    "print(\"For the update to move TOWARD zero:\")\n",
    "print(\"  |1 - 2*lr| < 1\")\n",
    "print(\"  → -1 < 1 - 2*lr < 1\")\n",
    "print(\"  → 0 < lr < 1\")\n",
    "print()\n",
    "print(f\"Colleague's lr = {LEARNING_RATE}\")\n",
    "print(f\"1 - 2*lr = {1 - 2*LEARNING_RATE}\")\n",
    "print()\n",
    "if abs(1 - 2*LEARNING_RATE) > 1:\n",
    "    print(\"❌ |1 - 2*lr| > 1, so updates AMPLIFY the error!\")\n",
    "else:\n",
    "    print(\"✓ |1 - 2*lr| < 1, so updates reduce the error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates\n",
    "test_lrs = [0.01, 0.1, 0.3, 0.5, 0.8, 0.95, 1.0, 1.2]\n",
    "print(\"Learning Rate Behavior:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for lr in test_lrs:\n",
    "    path, losses = gradient_descent(START_POSITION, lr, n_steps=50)\n",
    "    final_loss = losses[-1] if np.isfinite(losses[-1]) else float('inf')\n",
    "    \n",
    "    if final_loss < 0.01:\n",
    "        status = \"✓ Converged\"\n",
    "    elif final_loss < losses[0]:\n",
    "        status = \"~ Slow progress\"\n",
    "    elif np.isfinite(final_loss):\n",
    "        status = \"⚠ Oscillating\"\n",
    "    else:\n",
    "        status = \"❌ DIVERGED\"\n",
    "    \n",
    "    print(f\"lr={lr:<4} → {status} (final loss: {final_loss:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Find a good learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find a learning rate that converges reliably\n",
    "\n",
    "# Uncomment and complete:\n",
    "\n",
    "# LEARNING_RATE_FIXED = ???  # Pick from the experiment above (0.1 is usually safe)\n",
    "# \n",
    "# path_fixed, losses_fixed = gradient_descent(START_POSITION, LEARNING_RATE_FIXED, n_steps=50)\n",
    "# \n",
    "# print(f\"=== Fixed Training (lr={LEARNING_RATE_FIXED}) ===\")\n",
    "# print(f\"Starting loss: {losses_fixed[0]:.4f}\")\n",
    "# print(f\"Final loss: {losses_fixed[-1]:.6f}\")\n",
    "# print(f\"Final position: ({path_fixed[-1, 0]:.4f}, {path_fixed[-1, 1]:.4f})\")\n",
    "# print(f\"\\nConverged: {losses_fixed[-1] < 0.01}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the comparison\n",
    "\n",
    "# Uncomment:\n",
    "\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "# \n",
    "# # Paths\n",
    "# ax1 = axes[0]\n",
    "# ax1.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.5)\n",
    "# ax1.plot(np.clip(path_bad[:10], -5, 5)[:, 0], np.clip(path_bad[:10], -5, 5)[:, 1], \n",
    "#          'r-o', markersize=4, linewidth=1, label=f'Divergent (lr={LEARNING_RATE})', alpha=0.7)\n",
    "# ax1.plot(path_fixed[:, 0], path_fixed[:, 1], \n",
    "#          'b-o', markersize=4, linewidth=1, label=f'Converged (lr={LEARNING_RATE_FIXED})')\n",
    "# ax1.scatter([0], [0], color='green', s=100, marker='*', zorder=5)\n",
    "# ax1.set_xlabel('θ₁')\n",
    "# ax1.set_ylabel('θ₂')\n",
    "# ax1.set_title('Gradient Descent Paths')\n",
    "# ax1.legend()\n",
    "# ax1.set_xlim(-5, 5)\n",
    "# ax1.set_ylim(-5, 5)\n",
    "# \n",
    "# # Loss curves\n",
    "# ax2 = axes[1]\n",
    "# ax2.plot(np.clip(losses_bad, 0, 50), 'r-', linewidth=2, label=f'lr={LEARNING_RATE} (diverges)')\n",
    "# ax2.plot(losses_fixed, 'b-', linewidth=2, label=f'lr={LEARNING_RATE_FIXED} (converges)')\n",
    "# ax2.set_xlabel('Step')\n",
    "# ax2.set_ylabel('Loss')\n",
    "# ax2.set_title('Loss Over Time')\n",
    "# ax2.legend()\n",
    "# ax2.grid(True, alpha=0.3)\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SELF-CHECK: Did you fix the divergence?\n",
    "# ============================================\n",
    "\n",
    "# Uncomment:\n",
    "\n",
    "# assert losses_fixed[-1] < losses_fixed[0], \"Loss should decrease, not increase\"\n",
    "# assert losses_fixed[-1] < 0.1, \"Should converge close to minimum\"\n",
    "# assert LEARNING_RATE_FIXED < LEARNING_RATE, \"Fixed lr should be smaller\"\n",
    "# \n",
    "# print(\"✓ Divergence fixed!\")\n",
    "# print(f\"✓ Changed lr from {LEARNING_RATE} to {LEARNING_RATE_FIXED}\")\n",
    "# print(f\"✓ Loss now decreases from {losses_fixed[0]:.2f} to {losses_fixed[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Write your postmortem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postmortem = \"\"\"\n",
    "## Postmortem: The Exploding Loss\n",
    "\n",
    "### What happened:\n",
    "- (Your answer: What symptom indicated divergence?)\n",
    "\n",
    "### Root cause:\n",
    "- (Your answer: Why was the learning rate too high?)\n",
    "\n",
    "### How to prevent:\n",
    "- (Your answer: What's a safe starting point for learning rate? How would you tune it?)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(postmortem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ✅ Drill Complete!\n",
    "\n",
    "**Key lessons:**\n",
    "\n",
    "1. **Too high learning rate → divergence.** The optimizer overshoots the minimum and bounces away.\n",
    "\n",
    "2. **The symptom:** Loss increases instead of decreases, or goes to infinity/NaN.\n",
    "\n",
    "3. **The fix:** Reduce learning rate. Start with 0.001-0.1, then tune.\n",
    "\n",
    "4. **Rule of thumb:** If loss explodes, cut learning rate by 10x.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Rate Troubleshooting\n",
    "\n",
    "| Symptom | Likely Cause | Fix |\n",
    "|---------|-------------|-----|\n",
    "| Loss explodes/NaN | LR too high | Reduce by 10x |\n",
    "| Loss oscillates wildly | LR slightly too high | Reduce by 2-5x |\n",
    "| Loss decreases very slowly | LR too low | Increase by 2-10x |\n",
    "| Loss plateaus early | LR too low (stuck) | Increase, or use scheduler |\n",
    "| Loss bounces at end | LR too high for fine-tuning | Use LR decay/scheduler |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
