{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 12: Embeddings\n",
    "\n",
    "**Goal:** Understand embeddings, compute similarity, and build a simple semantic search system.\n",
    "\n",
    "**Prerequisites:** Basic Python, Module 10 (Feature Engineering concepts)\n",
    "\n",
    "**Expected Runtime:** ~25 minutes\n",
    "\n",
    "**Outputs:**\n",
    "- Text embeddings visualization\n",
    "- Similarity search results\n",
    "- Nearest neighbor exploration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: What Are Embeddings?\n",
    "\n",
    "Embeddings convert complex objects (text, users, items) into dense numerical vectors where similarity in the vector space reflects similarity in meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample support tickets\n",
    "tickets = [\n",
    "    \"I can't reset my password\",\n",
    "    \"How do I change my login credentials\",\n",
    "    \"Account access problem\",\n",
    "    \"Password reset not working\",\n",
    "    \"Where is my order?\",\n",
    "    \"Shipping is taking too long\",\n",
    "    \"Track my package delivery\",\n",
    "    \"Order hasn't arrived yet\",\n",
    "    \"I want a refund\",\n",
    "    \"Cancel my subscription please\",\n",
    "    \"How do I return this product\",\n",
    "    \"Get my money back\",\n",
    "]\n",
    "\n",
    "# For this demo, we'll use TF-IDF as simple embeddings\n",
    "# In production, use sentence-transformers for much better semantic embeddings\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "embeddings = vectorizer.fit_transform(tickets).toarray()\n",
    "\n",
    "print(f\"Number of texts: {len(tickets)}\")\n",
    "print(f\"Embedding dimensions: {embeddings.shape[1]}\")\n",
    "print(f\"\\nSample embedding (first 10 dims):\")\n",
    "print(embeddings[0][:10].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Similarity Metrics\n",
    "\n",
    "### Cosine Similarity\n",
    "Measures the angle between vectors (most common for text).\n",
    "\n",
    "### Euclidean Distance\n",
    "Measures straight-line distance (good when magnitude matters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity matrix\n",
    "sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(sim_matrix, cmap='RdYlGn', vmin=0, vmax=1)\n",
    "\n",
    "ax.set_xticks(range(len(tickets)))\n",
    "ax.set_yticks(range(len(tickets)))\n",
    "ax.set_xticklabels([t[:15] + '...' for t in tickets], rotation=45, ha='right', fontsize=8)\n",
    "ax.set_yticklabels([t[:15] + '...' for t in tickets], fontsize=8)\n",
    "\n",
    "plt.colorbar(im, label='Cosine Similarity')\n",
    "plt.title('Ticket Similarity Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Notice: Tickets about the same topic (password, shipping, refunds) have higher similarity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare cosine vs euclidean\n",
    "def compare_metrics(idx1, idx2):\n",
    "    cos_sim = cosine_similarity(embeddings[idx1:idx1+1], embeddings[idx2:idx2+1])[0][0]\n",
    "    euc_dist = euclidean_distances(embeddings[idx1:idx1+1], embeddings[idx2:idx2+1])[0][0]\n",
    "    \n",
    "    print(f\"Text 1: '{tickets[idx1]}'\")\n",
    "    print(f\"Text 2: '{tickets[idx2]}'\")\n",
    "    print(f\"Cosine Similarity: {cos_sim:.3f}\")\n",
    "    print(f\"Euclidean Distance: {euc_dist:.3f}\")\n",
    "    print()\n",
    "\n",
    "print(\"=== Similar Texts ===\")\n",
    "compare_metrics(0, 1)  # Both about password/login\n",
    "\n",
    "print(\"=== Different Texts ===\")\n",
    "compare_metrics(0, 8)  # Password vs refund"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Nearest Neighbor Search\n",
    "\n",
    "Find the most similar items to a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build nearest neighbors index\n",
    "nn = NearestNeighbors(n_neighbors=3, metric='cosine')\n",
    "nn.fit(embeddings)\n",
    "\n",
    "def search(query, k=3):\n",
    "    \"\"\"Find k most similar texts to a query.\"\"\"\n",
    "    query_embedding = vectorizer.transform([query]).toarray()\n",
    "    distances, indices = nn.kneighbors(query_embedding, n_neighbors=k)\n",
    "    \n",
    "    print(f\"Query: '{query}'\\n\")\n",
    "    print(\"Top matches:\")\n",
    "    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        sim = 1 - dist  # Convert distance to similarity\n",
    "        print(f\"  {i+1}. [{sim:.3f}] {tickets[idx]}\")\n",
    "    print()\n",
    "\n",
    "# Test searches\n",
    "search(\"login issue\")\n",
    "search(\"where is my package\")\n",
    "search(\"I want my money back\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Embedding Visualization\n",
    "\n",
    "Project high-dimensional embeddings to 2D for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use t-SNE for visualization\n",
    "# (For small datasets, PCA is also fine)\n",
    "pca = PCA(n_components=2)\n",
    "coords_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Define topic colors\n",
    "topics = ['password'] * 4 + ['shipping'] * 4 + ['refund'] * 4\n",
    "topic_colors = {'password': '#ef4444', 'shipping': '#22c55e', 'refund': '#0ea5e9'}\n",
    "colors = [topic_colors[t] for t in topics]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for topic in topic_colors:\n",
    "    mask = [t == topic for t in topics]\n",
    "    ax.scatter(coords_2d[mask, 0], coords_2d[mask, 1], \n",
    "               c=topic_colors[topic], label=topic.capitalize(), s=100, alpha=0.7)\n",
    "\n",
    "# Add labels\n",
    "for i, txt in enumerate(tickets):\n",
    "    ax.annotate(f\"{i+1}\", (coords_2d[i, 0] + 0.02, coords_2d[i, 1] + 0.02), fontsize=8)\n",
    "\n",
    "ax.set_xlabel('Component 1')\n",
    "ax.set_ylabel('Component 2')\n",
    "ax.set_title('Support Tickets in Embedding Space')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Similar tickets cluster together even without explicit labels!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Using Pre-trained Embeddings (Recommended)\n",
    "\n",
    "In production, use sentence-transformers for much better semantic embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if needed: pip install sentence-transformers\n",
    "# Uncomment to use real embeddings:\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# embeddings = model.encode(tickets)\n",
    "# print(f\"Embedding shape: {embeddings.shape}\")\n",
    "\n",
    "# For now, we'll simulate better embeddings\n",
    "print(\"=== Simulated Semantic Embeddings ===\")\n",
    "print(\"In production, use:\")\n",
    "print(\"  from sentence_transformers import SentenceTransformer\")\n",
    "print(\"  model = SentenceTransformer('all-MiniLM-L6-v2')\")\n",
    "print(\"  embeddings = model.encode(texts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: TODO - Build a Simple Duplicate Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find potential duplicate tickets\n",
    "# Tickets with similarity > 0.8 might be duplicates\n",
    "\n",
    "threshold = 0.7  # Adjust this\n",
    "\n",
    "print(f\"=== Potential Duplicates (similarity > {threshold}) ===\")\n",
    "duplicates_found = []\n",
    "\n",
    "for i in range(len(tickets)):\n",
    "    for j in range(i + 1, len(tickets)):\n",
    "        sim = sim_matrix[i, j]\n",
    "        if sim > threshold:\n",
    "            duplicates_found.append((i, j, sim))\n",
    "            print(f\"\\n[{sim:.3f}] Pair {i+1} & {j+1}:\")\n",
    "            print(f\"  â†’ '{tickets[i]}'\")\n",
    "            print(f\"  â†’ '{tickets[j]}'\")\n",
    "\n",
    "print(f\"\\nFound {len(duplicates_found)} potential duplicate pairs\")\n",
    "\n",
    "# TODO: Try different thresholds - what happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: TODO - Threshold Selection\n",
    "\n",
    "Different thresholds trade off precision vs recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how threshold affects results\n",
    "thresholds = np.arange(0.3, 1.0, 0.1)\n",
    "counts = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    count = np.sum(sim_matrix > thresh) - len(tickets)  # Subtract diagonal\n",
    "    count = count // 2  # Each pair counted twice\n",
    "    counts.append(count)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(thresholds, counts, width=0.08, color='#6366f1', alpha=0.7)\n",
    "plt.xlabel('Similarity Threshold')\n",
    "plt.ylabel('Number of Pairs')\n",
    "plt.title('Pairs Above Threshold vs Threshold Value')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Lower threshold = more results (higher recall, lower precision)\")\n",
    "print(\"   Higher threshold = fewer results (lower recall, higher precision)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: TODO - Stakeholder Summary\n",
    "\n",
    "Explain to a product manager:\n",
    "1. What embeddings do and why they're useful\n",
    "2. How you'd use them to improve the support system\n",
    "3. What trade-offs exist in threshold selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Summary:\n",
    "\n",
    "*Write your explanation here...*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Embeddings** turn text/objects into numbers that preserve meaning\n",
    "2. **Cosine similarity** is the standard metric for text similarity\n",
    "3. **Nearest neighbors** enable fast similarity search\n",
    "4. **Threshold selection** trades off precision vs recall\n",
    "5. **Pre-trained models** (sentence-transformers) are much better than TF-IDF\n",
    "\n",
    "### Next Steps\n",
    "- Explore the interactive playground\n",
    "- Complete the quiz\n",
    "- Try sentence-transformers for production-quality embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
