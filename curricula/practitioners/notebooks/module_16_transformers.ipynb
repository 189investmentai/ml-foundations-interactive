{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 16: Transformers\n",
        "\n",
        "**Goal:** Understand how attention works and why transformers revolutionized NLP.\n",
        "\n",
        "**Prerequisites:** Module 15 (Neural Networks)\n",
        "\n",
        "**Expected Runtime:** ~25 minutes\n",
        "\n",
        "**Outputs:**\n",
        "- Implemented basic attention mechanism\n",
        "- Visualized attention patterns\n",
        "- Used pre-trained transformers\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "plt.rcParams['figure.figsize'] = (12, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: The Attention Mechanism from Scratch\n",
        "\n",
        "Let's implement self-attention step by step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute softmax along last axis.\"\"\"\n",
        "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return exp_x / exp_x.sum(axis=-1, keepdims=True)\n",
        "\n",
        "def attention(Q, K, V):\n",
        "    \"\"\"\n",
        "    Compute scaled dot-product attention.\n",
        "    \n",
        "    Q: Query matrix (seq_len, d_k)\n",
        "    K: Key matrix (seq_len, d_k)\n",
        "    V: Value matrix (seq_len, d_v)\n",
        "    \n",
        "    Returns: \n",
        "        output: Attention output (seq_len, d_v)\n",
        "        weights: Attention weights (seq_len, seq_len)\n",
        "    \"\"\"\n",
        "    d_k = K.shape[-1]\n",
        "    \n",
        "    # Step 1: Compute attention scores (Q @ K^T)\n",
        "    scores = Q @ K.T\n",
        "    \n",
        "    # Step 2: Scale by sqrt(d_k)\n",
        "    scores = scores / np.sqrt(d_k)\n",
        "    \n",
        "    # Step 3: Apply softmax to get attention weights\n",
        "    weights = softmax(scores)\n",
        "    \n",
        "    # Step 4: Weighted sum of values\n",
        "    output = weights @ V\n",
        "    \n",
        "    return output, weights\n",
        "\n",
        "print(\"âœ“ Attention function defined\")\n",
        "print(\"\\nFormula: Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Ã— V\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simple example with 4 tokens, embedding dimension 8\n",
        "np.random.seed(42)\n",
        "\n",
        "seq_len = 4\n",
        "d_model = 8\n",
        "\n",
        "# Simulate input embeddings\n",
        "tokens = ['The', 'cat', 'sat', 'down']\n",
        "X = np.random.randn(seq_len, d_model)  # Input embeddings\n",
        "\n",
        "# Weight matrices for Q, K, V\n",
        "W_Q = np.random.randn(d_model, d_model) * 0.1\n",
        "W_K = np.random.randn(d_model, d_model) * 0.1\n",
        "W_V = np.random.randn(d_model, d_model) * 0.1\n",
        "\n",
        "# Compute Q, K, V\n",
        "Q = X @ W_Q\n",
        "K = X @ W_K\n",
        "V = X @ W_V\n",
        "\n",
        "# Run attention\n",
        "output, weights = attention(Q, K, V)\n",
        "\n",
        "print(f\"Input shape: {X.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Attention weights shape: {weights.shape}\")\n",
        "print(f\"\\nWeights sum per row: {weights.sum(axis=1)}\")\n",
        "print(\"(Each row sums to 1 after softmax)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Visualizing Attention Patterns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize attention weights\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "\n",
        "sns.heatmap(weights, annot=True, fmt='.2f', cmap='RdPu',\n",
        "            xticklabels=tokens, yticklabels=tokens, ax=ax)\n",
        "\n",
        "ax.set_xlabel('Attending to (Keys)')\n",
        "ax.set_ylabel('Token (Queries)')\n",
        "ax.set_title('Self-Attention Weights')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ðŸ’¡ Each row shows how much one token 'attends' to all others.\")\n",
        "print(\"   Row 'cat' shows what 'cat' pays attention to when understanding itself.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: How Attention Captures Meaning\n",
        "\n",
        "Let's see how attention could resolve ambiguity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulate attention for an ambiguous sentence\n",
        "# \"The bank by the river had many fish\"\n",
        "\n",
        "tokens_amb = ['The', 'bank', 'by', 'the', 'river', 'had', 'many', 'fish']\n",
        "n = len(tokens_amb)\n",
        "\n",
        "# Create mock attention weights that show \"bank\" attending to \"river\" and \"fish\"\n",
        "# (learned to disambiguate bank = riverbank, not financial institution)\n",
        "mock_attention = np.ones((n, n)) * 0.05  # Low baseline\n",
        "\n",
        "# Each token attends somewhat to itself\n",
        "np.fill_diagonal(mock_attention, 0.3)\n",
        "\n",
        "# \"bank\" (idx 1) attends to \"river\" (idx 4) and \"fish\" (idx 7)\n",
        "mock_attention[1, 4] = 0.35  # river\n",
        "mock_attention[1, 7] = 0.2   # fish\n",
        "\n",
        "# \"fish\" attends to \"river\" and \"bank\"\n",
        "mock_attention[7, 4] = 0.3\n",
        "mock_attention[7, 1] = 0.25\n",
        "\n",
        "# Normalize rows\n",
        "mock_attention = mock_attention / mock_attention.sum(axis=1, keepdims=True)\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "sns.heatmap(mock_attention, annot=True, fmt='.2f', cmap='RdPu',\n",
        "            xticklabels=tokens_amb, yticklabels=tokens_amb, ax=ax)\n",
        "\n",
        "ax.set_xlabel('Attending to')\n",
        "ax.set_ylabel('Token')\n",
        "ax.set_title('\"The bank by the river had many fish\" - Attention Disambiguates')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ðŸ’¡ Notice how 'bank' attends strongly to 'river' and 'fish'\")\n",
        "print(\"   This context helps the model understand bank = riverbank, not financial bank\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Multi-Head Attention\n",
        "\n",
        "Different \"heads\" learn different patterns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def multi_head_attention(X, num_heads=4):\n",
        "    \"\"\"\n",
        "    Simplified multi-head attention.\n",
        "    \n",
        "    Each head learns different Q, K, V projections.\n",
        "    \"\"\"\n",
        "    seq_len, d_model = X.shape\n",
        "    d_head = d_model // num_heads\n",
        "    \n",
        "    all_weights = []\n",
        "    all_outputs = []\n",
        "    \n",
        "    for head in range(num_heads):\n",
        "        # Each head has its own projection matrices\n",
        "        np.random.seed(42 + head)\n",
        "        W_Q = np.random.randn(d_model, d_head) * 0.1\n",
        "        W_K = np.random.randn(d_model, d_head) * 0.1\n",
        "        W_V = np.random.randn(d_model, d_head) * 0.1\n",
        "        \n",
        "        Q = X @ W_Q\n",
        "        K = X @ W_K\n",
        "        V = X @ W_V\n",
        "        \n",
        "        output, weights = attention(Q, K, V)\n",
        "        all_weights.append(weights)\n",
        "        all_outputs.append(output)\n",
        "    \n",
        "    # Concatenate head outputs\n",
        "    multi_output = np.concatenate(all_outputs, axis=-1)\n",
        "    \n",
        "    return multi_output, all_weights\n",
        "\n",
        "# Run multi-head attention\n",
        "multi_output, head_weights = multi_head_attention(X, num_heads=4)\n",
        "\n",
        "print(f\"Multi-head output shape: {multi_output.shape}\")\n",
        "print(f\"Number of attention heads: {len(head_weights)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize different attention heads\n",
        "fig, axes = plt.subplots(1, 4, figsize=(14, 3))\n",
        "\n",
        "head_names = ['Head 1: Syntactic', 'Head 2: Semantic', 'Head 3: Local', 'Head 4: Mixed']\n",
        "\n",
        "for i, (weights, name) in enumerate(zip(head_weights, head_names)):\n",
        "    sns.heatmap(weights, annot=True, fmt='.2f', cmap='RdPu',\n",
        "                xticklabels=tokens, yticklabels=tokens, ax=axes[i],\n",
        "                cbar=False)\n",
        "    axes[i].set_title(name, fontsize=10)\n",
        "    if i > 0:\n",
        "        axes[i].set_ylabel('')\n",
        "        axes[i].set_yticklabels([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ðŸ’¡ Different heads learn to focus on different relationships.\")\n",
        "print(\"   Some track syntax, some semantics, some nearby words, etc.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Positional Encoding\n",
        "\n",
        "Transformers need position information since they process all tokens in parallel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def sinusoidal_positional_encoding(seq_len, d_model):\n",
        "    \"\"\"\n",
        "    Create sinusoidal positional encodings.\n",
        "    \n",
        "    PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
        "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
        "    \"\"\"\n",
        "    PE = np.zeros((seq_len, d_model))\n",
        "    \n",
        "    for pos in range(seq_len):\n",
        "        for i in range(0, d_model, 2):\n",
        "            div = 10000 ** (i / d_model)\n",
        "            PE[pos, i] = np.sin(pos / div)\n",
        "            if i + 1 < d_model:\n",
        "                PE[pos, i+1] = np.cos(pos / div)\n",
        "    \n",
        "    return PE\n",
        "\n",
        "# Generate positional encodings\n",
        "seq_len = 50\n",
        "d_model = 64\n",
        "PE = sinusoidal_positional_encoding(seq_len, d_model)\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "\n",
        "im = ax.imshow(PE.T, aspect='auto', cmap='RdBu_r')\n",
        "ax.set_xlabel('Position')\n",
        "ax.set_ylabel('Dimension')\n",
        "ax.set_title('Sinusoidal Positional Encoding')\n",
        "plt.colorbar(im)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ðŸ’¡ Each position gets a unique pattern of sine/cosine waves.\")\n",
        "print(\"   This lets the model distinguish word 1 from word 2 from word 50.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Using Pre-trained Transformers\n",
        "\n",
        "In practice, use libraries like HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Pre-trained Transformers with HuggingFace\n",
        "# Installation: pip install transformers torch\n",
        "\n",
        "print(\"=== Pre-trained Transformer Usage ===\")\n",
        "print(\"\"\"\n",
        "Installation:\n",
        "  pip install transformers torch\n",
        "\n",
        "Example code:\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# 1. Sentiment Analysis\n",
        "sentiment = pipeline('sentiment-analysis')\n",
        "result = sentiment(\"This product is amazing!\")\n",
        "# Output: [{'label': 'POSITIVE', 'score': 0.9998}]\n",
        "\n",
        "# 2. Text Generation\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "result = generator(\"Machine learning is\", max_length=30)\n",
        "# Output: [{'generated_text': 'Machine learning is a powerful...'}]\n",
        "\n",
        "# 3. Zero-Shot Classification (no training needed!)\n",
        "classifier = pipeline('zero-shot-classification')\n",
        "result = classifier(\n",
        "    \"I need to cancel my order\",\n",
        "    candidate_labels=['support', 'sales', 'billing']\n",
        ")\n",
        "# Output: {'labels': ['support', ...], 'scores': [0.85, ...]}\n",
        "\n",
        "# 4. Named Entity Recognition\n",
        "ner = pipeline('ner', grouped_entities=True)\n",
        "result = ner(\"Apple CEO Tim Cook announced new products in California\")\n",
        "# Output: [{'entity_group': 'ORG', 'word': 'Apple'}, ...]\n",
        "\n",
        "ðŸ’¡ Key models:\n",
        "  â€¢ distilbert-base-uncased: Fast, good for classification\n",
        "  â€¢ gpt2: Text generation (local, free)\n",
        "  â€¢ all-MiniLM-L6-v2: Embeddings (via sentence-transformers)\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Context Window Limitations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Demonstrate context window constraints\n",
        "\n",
        "context_windows = {\n",
        "    'GPT-2': 1024,\n",
        "    'GPT-3': 4096,\n",
        "    'GPT-4': 8192,\n",
        "    'GPT-4-Turbo': 128000,\n",
        "    'Claude-3': 200000,\n",
        "}\n",
        "\n",
        "# Approximate words (tokens â‰ˆ 0.75 words)\n",
        "approx_words = {k: int(v * 0.75) for k, v in context_windows.items()}\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "models = list(context_windows.keys())\n",
        "tokens = list(context_windows.values())\n",
        "\n",
        "bars = ax.bar(models, tokens, color='#ec4899')\n",
        "ax.set_ylabel('Context Window (tokens)')\n",
        "ax.set_title('Transformer Context Windows')\n",
        "ax.set_yscale('log')\n",
        "\n",
        "# Add labels\n",
        "for bar, tok, words in zip(bars, tokens, approx_words.values()):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
        "            f'{tok:,}\\n(~{words:,} words)', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ðŸ’¡ Attention is O(nÂ²) in sequence length - that's why context windows are limited.\")\n",
        "print(\"   Newer models use techniques to extend context while managing compute costs.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Check\n",
        "\n",
        "Uncomment and run the asserts below to verify your attention implementation is correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# SELF-CHECK: Verify your attention implementation\n",
        "assert weights.shape[0] == weights.shape[1], \"Attention weights should be square (seq_len x seq_len)\"\n",
        "assert np.allclose(weights.sum(axis=1), 1.0), \"Attention weights should sum to 1 per row\"\n",
        "assert output.shape == (len(tokens), d_model), \"Output should match input dimensions\"\n",
        "print(f\"âœ… Self-check passed! Attention matrix: {weights.shape}, output: {output.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Stakeholder Summary\n",
        "\n",
        "### TODO: Write a 3-bullet summary (~100 words) for the PM\n",
        "\n",
        "Template:\n",
        "â€¢ **What attention does:** Allows the model to focus on relevant parts of input. Like a spotlight that highlights [important words/context].\n",
        "â€¢ **Why transformers win:** Process text in parallel (fast training), capture long-range dependencies better than [RNNs/LSTMs].\n",
        "â€¢ **Context window impact:** Our model can process up to ____ tokens (~____ words). For longer documents, we need [chunking/summarization]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Summary:\n",
        "\n",
        "*Write your explanation here...*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Attention** = learning which parts of input are relevant to each other\n",
        "2. **Self-attention formula:** softmax(QK^T / âˆšd) Ã— V\n",
        "3. **Multi-head attention** learns multiple relationship types\n",
        "4. **Positional encoding** adds sequence order information\n",
        "5. **Context windows** limit how much text models can process\n",
        "6. **Pre-trained models** (BERT, GPT) are the standard approach\n",
        "\n",
        "### Next Steps\n",
        "- Explore the interactive playground\n",
        "- Complete the quiz\n",
        "- Move to Module 17: LLM Fundamentals"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}