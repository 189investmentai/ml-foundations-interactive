{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 15: Neural Networks\n",
    "\n",
    "**Goal:** Understand how neural networks learn and when to use them vs simpler models.\n",
    "\n",
    "**Prerequisites:** Modules 7 (Optimization), 11 (Regularization)\n",
    "\n",
    "**Expected Runtime:** ~30 minutes\n",
    "\n",
    "**Outputs:**\n",
    "- Built and trained a simple neural network\n",
    "- Visualized decision boundaries\n",
    "- Compared NN vs simpler models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.datasets import make_circles, make_moons, make_classification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams['figure.figsize'] = (12, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Need for Neural Networks\n",
    "\n",
    "Linear models can't learn non-linear patterns. Let's see why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate XOR-like data (not linearly separable)\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "X = np.random.randn(n, 2)\n",
    "y = ((X[:, 0] > 0) != (X[:, 1] > 0)).astype(int)  # XOR pattern\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for label in [0, 1]:\n",
    "    mask = y == label\n",
    "    axes[0].scatter(X[mask, 0], X[mask, 1], label=f'Class {label}', alpha=0.7)\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].set_title('XOR Pattern (Not Linearly Separable)')\n",
    "axes[0].legend()\n",
    "axes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Try logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X, y)\n",
    "lr_acc = lr.score(X, y)\n",
    "\n",
    "# Try neural network\n",
    "nn = MLPClassifier(hidden_layer_sizes=(8,), max_iter=1000, random_state=42)\n",
    "nn.fit(X, y)\n",
    "nn_acc = nn.score(X, y)\n",
    "\n",
    "axes[1].bar(['Logistic\\nRegression', 'Neural\\nNetwork'], [lr_acc, nn_acc], \n",
    "            color=['#ef4444', '#22c55e'])\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Model Comparison on XOR')\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Logistic Regression: {lr_acc:.1%}\")\n",
    "print(f\"Neural Network: {nn_acc:.1%}\")\n",
    "print(\"\\nüí° Linear models can't solve XOR. Neural networks can!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building a Neural Network\n",
    "\n",
    "Using sklearn's MLPClassifier (Multi-Layer Perceptron)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate more complex data\n",
    "X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features (important for neural networks!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build neural network\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(16, 8),  # Two hidden layers: 16 and 8 neurons\n",
    "    activation='relu',           # ReLU activation\n",
    "    solver='adam',               # Adam optimizer\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    early_stopping=True,         # Stop when validation stops improving\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"=== Network Architecture ===\")\n",
    "print(f\"Input layer: 2 features\")\n",
    "for i, size in enumerate(model.hidden_layer_sizes):\n",
    "    print(f\"Hidden layer {i+1}: {size} neurons\")\n",
    "print(f\"Output layer: 2 classes\")\n",
    "\n",
    "print(f\"\\nTotal parameters: {sum(c.size for c in model.coefs_) + sum(i.size for i in model.intercepts_)}\")\n",
    "print(f\"Training iterations: {model.n_iter_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "train_acc = model.score(X_train_scaled, y_train)\n",
    "test_acc = model.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.1%}\")\n",
    "print(f\"Test Accuracy: {test_acc:.1%}\")\n",
    "\n",
    "if train_acc > test_acc + 0.1:\n",
    "    print(\"‚ö†Ô∏è Gap suggests possible overfitting\")\n",
    "else:\n",
    "    print(\"‚úì Good generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Visualizing Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title, ax):\n",
    "    \"\"\"Plot decision boundary for a classifier.\"\"\"\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "    ax.scatter(X[y==0, 0], X[y==0, 1], c='#ef4444', label='Class 0', edgecolor='white')\n",
    "    ax.scatter(X[y==1, 0], X[y==1, 1], c='#0ea5e9', label='Class 1', edgecolor='white')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "# Compare models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "plot_decision_boundary(lr, X_train_scaled, y_train, \n",
    "                       f'Logistic Regression ({lr.score(X_test_scaled, y_test):.1%})', axes[0])\n",
    "\n",
    "# Neural Network - shallow\n",
    "nn_shallow = MLPClassifier(hidden_layer_sizes=(4,), max_iter=500, random_state=42)\n",
    "nn_shallow.fit(X_train_scaled, y_train)\n",
    "plot_decision_boundary(nn_shallow, X_train_scaled, y_train,\n",
    "                       f'NN - 1 Layer, 4 Neurons ({nn_shallow.score(X_test_scaled, y_test):.1%})', axes[1])\n",
    "\n",
    "# Neural Network - deep\n",
    "nn_deep = MLPClassifier(hidden_layer_sizes=(16, 8), max_iter=500, random_state=42)\n",
    "nn_deep.fit(X_train_scaled, y_train)\n",
    "plot_decision_boundary(nn_deep, X_train_scaled, y_train,\n",
    "                       f'NN - 2 Layers, 16-8 Neurons ({nn_deep.score(X_test_scaled, y_test):.1%})', axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° More layers and neurons ‚Üí more complex decision boundaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Activation Functions\n",
    "\n",
    "Different activations create different decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation functions\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "activations = {\n",
    "    'ReLU': np.maximum(0, x),\n",
    "    'Sigmoid': 1 / (1 + np.exp(-x)),\n",
    "    'Tanh': np.tanh(x),\n",
    "    'Identity (Linear)': x\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(14, 3))\n",
    "\n",
    "for ax, (name, y) in zip(axes, activations.items()):\n",
    "    ax.plot(x, y, color='#f97316', linewidth=2)\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('Input')\n",
    "    ax.set_ylabel('Output')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° ReLU is most common for hidden layers (fast, avoids vanishing gradients)\")\n",
    "print(\"   Sigmoid/Softmax used for output layer (produces probabilities)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Overfitting Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate small dataset (prone to overfitting)\n",
    "X_small, y_small = make_moons(n_samples=100, noise=0.3, random_state=42)\n",
    "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_small, y_small, test_size=0.3)\n",
    "\n",
    "scaler_s = StandardScaler()\n",
    "X_train_s_scaled = scaler_s.fit_transform(X_train_s)\n",
    "X_test_s_scaled = scaler_s.transform(X_test_s)\n",
    "\n",
    "# Compare different architectures\n",
    "architectures = [\n",
    "    ((4,), 'Small (4)'),\n",
    "    ((16, 8), 'Medium (16, 8)'),\n",
    "    ((64, 32, 16), 'Large (64, 32, 16)'),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for arch, name in architectures:\n",
    "    model = MLPClassifier(hidden_layer_sizes=arch, max_iter=1000, random_state=42)\n",
    "    model.fit(X_train_s_scaled, y_train_s)\n",
    "    \n",
    "    train_acc = model.score(X_train_s_scaled, y_train_s)\n",
    "    test_acc = model.score(X_test_s_scaled, y_test_s)\n",
    "    \n",
    "    results.append({\n",
    "        'Architecture': name,\n",
    "        'Train Acc': train_acc,\n",
    "        'Test Acc': test_acc,\n",
    "        'Gap': train_acc - test_acc\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"=== Overfitting Analysis (Small Dataset, 70 training samples) ===\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nüí° Larger networks can overfit on small data. Gap between train/test indicates overfitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: NN vs Gradient Boosting on Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate tabular classification data\n",
    "X_tab, y_tab = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=10,\n",
    "    n_redundant=5, n_clusters_per_class=2, random_state=42\n",
    ")\n",
    "\n",
    "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(X_tab, y_tab, test_size=0.2)\n",
    "\n",
    "scaler_t = StandardScaler()\n",
    "X_train_t_scaled = scaler_t.fit_transform(X_train_t)\n",
    "X_test_t_scaled = scaler_t.transform(X_test_t)\n",
    "\n",
    "# Compare models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Neural Network': MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42)\n",
    "}\n",
    "\n",
    "print(\"=== Tabular Data Comparison (1000 samples, 20 features) ===\")\n",
    "comparison = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name == 'Gradient Boosting':\n",
    "        model.fit(X_train_t, y_train_t)  # GBM doesn't need scaling\n",
    "        test_acc = model.score(X_test_t, y_test_t)\n",
    "    else:\n",
    "        model.fit(X_train_t_scaled, y_train_t)\n",
    "        test_acc = model.score(X_test_t_scaled, y_test_t)\n",
    "    \n",
    "    comparison.append({'Model': name, 'Test Accuracy': test_acc})\n",
    "    print(f\"{name}: {test_acc:.1%}\")\n",
    "\n",
    "print(\"\\nüí° For tabular data, gradient boosting often matches or beats neural networks!\")\n",
    "print(\"   NNs shine on unstructured data (images, text) with large datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: TODO - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with different hyperparameters\n",
    "# Try different combinations and record results\n",
    "\n",
    "# Hyperparameters to try:\n",
    "# - hidden_layer_sizes: (8,), (16, 8), (32, 16, 8)\n",
    "# - activation: 'relu', 'tanh'\n",
    "# - alpha: 0.0001, 0.001, 0.01 (L2 regularization)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(8,), (16, 8), (32, 16)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': [0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Uncomment to run grid search (takes a few minutes)\n",
    "# grid = GridSearchCV(MLPClassifier(max_iter=500, random_state=42), \n",
    "#                     param_grid, cv=5, scoring='accuracy')\n",
    "# grid.fit(X_train_t_scaled, y_train_t)\n",
    "# print(f\"Best params: {grid.best_params_}\")\n",
    "# print(f\"Best CV score: {grid.best_score_:.3f}\")\n",
    "\n",
    "print(\"TODO: Uncomment grid search code above to find best hyperparameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: TODO - Stakeholder Summary\n",
    "\n",
    "Explain to a product manager:\n",
    "1. When to use neural networks vs simpler models\n",
    "2. What the tradeoffs are (interpretability, data requirements, compute)\n",
    "3. How to know if a neural network is overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Summary:\n",
    "\n",
    "*Write your explanation here...*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Neural networks** learn features automatically through layers\n",
    "2. **Activation functions** add non-linearity (ReLU is default)\n",
    "3. **More layers/neurons** ‚Üí more complex patterns, but risk overfitting\n",
    "4. **Always scale features** before training neural networks\n",
    "5. **For tabular data**, gradient boosting often works as well or better\n",
    "6. **NNs shine** on unstructured data with large datasets\n",
    "\n",
    "### Next Steps\n",
    "- Explore the interactive playground\n",
    "- Complete the quiz\n",
    "- Move to Module 16: Transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
