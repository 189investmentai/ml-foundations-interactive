{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 6 Lab: Ensemble Methods for Churn Prediction\n",
        "\n",
        "**Goal:** Compare single trees, Random Forests, and Gradient Boosting for churn prediction. Find the best ensemble configuration.\n",
        "\n",
        "**Time:** 45-60 minutes\n",
        "\n",
        "**Prerequisites:** Module 5 (Decision Trees)\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this lab, you will:\n",
        "1. Train and compare Random Forest and Gradient Boosting models\n",
        "2. Tune ensemble hyperparameters and observe their effects\n",
        "3. Use early stopping to prevent boosting overfitting\n",
        "4. Extract and interpret feature importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try to import XGBoost (optional but recommended)\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    HAS_XGBOOST = True\n",
        "    print(\"✓ XGBoost available\")\n",
        "except ImportError:\n",
        "    HAS_XGBOOST = False\n",
        "    print(\"⚠ XGBoost not installed, using sklearn GradientBoosting\")\n",
        "\n",
        "print(\"✓ Libraries loaded\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DATA_URL = 'https://raw.githubusercontent.com/189investmentai/ml-foundations-interactive/main/shared/data/'\n",
        "\n",
        "customers = pd.read_csv(DATA_URL + 'streamcart_customers.csv')\n",
        "print(f\"Loaded {len(customers)} customers\")\n",
        "\n",
        "# Feature engineering\n",
        "if 'tenure_days' not in customers.columns:\n",
        "    customers['tenure_days'] = (pd.to_datetime('2024-01-01') - pd.to_datetime(customers['signup_date'])).dt.days\n",
        "if 'avg_order_value' not in customers.columns:\n",
        "    customers['avg_order_value'] = customers['total_spend'] / customers['total_orders'].replace(0, 1)\n",
        "\n",
        "# Select features\n",
        "feature_cols = ['tenure_days', 'total_orders', 'total_spend', 'support_tickets', 'avg_order_value']\n",
        "available_features = [c for c in feature_cols if c in customers.columns]\n",
        "print(f\"Features: {available_features}\")\n",
        "\n",
        "X = customers[available_features].fillna(0)\n",
        "y = customers['is_churned']\n",
        "\n",
        "# Train-validation-test split\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp)\n",
        "\n",
        "print(f\"\\nTrain: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)}\")\n",
        "print(f\"Churn rate: {y.mean():.1%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Baseline: Single Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train a single tree for baseline\n",
        "single_tree = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "single_tree.fit(X_train, y_train)\n",
        "\n",
        "print(\"Single Decision Tree:\")\n",
        "print(f\"  Train Accuracy: {single_tree.score(X_train, y_train):.1%}\")\n",
        "print(f\"  Val Accuracy: {single_tree.score(X_val, y_val):.1%}\")\n",
        "print(f\"  Test Accuracy: {single_tree.score(X_test, y_test):.1%}\")\n",
        "print(f\"  Depth: {single_tree.get_depth()}, Leaves: {single_tree.get_n_leaves()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=None,  # Let trees grow\n",
        "    min_samples_leaf=5,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Random Forest (100 trees):\")\n",
        "print(f\"  Train Accuracy: {rf.score(X_train, y_train):.1%}\")\n",
        "print(f\"  Val Accuracy: {rf.score(X_val, y_val):.1%}\")\n",
        "print(f\"  Test Accuracy: {rf.score(X_test, y_test):.1%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. TODO: Experiment with Number of Trees\n",
        "\n",
        "Your task: Find how many trees you need before returns diminish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Test different numbers of trees\n",
        "\n",
        "n_trees_list = [1, 5, 10, 25, 50, 100, 200, 500]\n",
        "rf_results = []\n",
        "\n",
        "for n_trees in n_trees_list:\n",
        "    rf_temp = RandomForestClassifier(\n",
        "        n_estimators=n_trees,\n",
        "        max_depth=None,\n",
        "        min_samples_leaf=5,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    rf_temp.fit(X_train, y_train)\n",
        "    \n",
        "    # TODO: Calculate train and test accuracy\n",
        "    # train_acc = ???\n",
        "    # val_acc = ???\n",
        "    # test_acc = ???\n",
        "    \n",
        "    rf_results.append({\n",
        "        'n_trees': n_trees,\n",
        "        'train_acc': train_acc,\n",
        "        'val_acc': val_acc,\n",
        "        'test_acc': test_acc\n",
        "    })\n",
        "\n",
        "rf_results_df = pd.DataFrame(rf_results)\n",
        "rf_results_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(rf_results_df['n_trees'], rf_results_df['train_acc'], 'b-o', label='Train', linewidth=2)\n",
        "plt.plot(rf_results_df['n_trees'], rf_results_df['val_acc'], 'g-s', label='Validation', linewidth=2)\n",
        "plt.plot(rf_results_df['n_trees'], rf_results_df['test_acc'], 'r-^', label='Test', linewidth=2)\n",
        "plt.xlabel('Number of Trees')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Random Forest: Accuracy vs Number of Trees')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xscale('log')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nObservation: Accuracy typically plateaus around 50-100 trees.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train Gradient Boosting\n",
        "if HAS_XGBOOST:\n",
        "    gb = XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=4,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42,\n",
        "        verbosity=0\n",
        "    )\n",
        "else:\n",
        "    gb = GradientBoostingClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=4,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "print(\"Gradient Boosting (100 rounds):\")\n",
        "print(f\"  Train Accuracy: {gb.score(X_train, y_train):.1%}\")\n",
        "print(f\"  Val Accuracy: {gb.score(X_val, y_val):.1%}\")\n",
        "print(f\"  Test Accuracy: {gb.score(X_test, y_test):.1%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Early Stopping (Boosting Best Practice)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Demonstrate overfitting without early stopping\n",
        "if HAS_XGBOOST:\n",
        "    gb_overfit = XGBClassifier(\n",
        "        n_estimators=500,  # Many rounds\n",
        "        max_depth=8,       # Deep trees\n",
        "        learning_rate=0.3, # Fast learning\n",
        "        random_state=42,\n",
        "        verbosity=0\n",
        "    )\n",
        "    gb_overfit.fit(X_train, y_train)\n",
        "    \n",
        "    print(\"Overfitting Example (500 rounds, depth 8, lr 0.3):\")\n",
        "    print(f\"  Train Accuracy: {gb_overfit.score(X_train, y_train):.1%}\")\n",
        "    print(f\"  Test Accuracy: {gb_overfit.score(X_test, y_test):.1%}\")\n",
        "    print(f\"  Gap: {gb_overfit.score(X_train, y_train) - gb_overfit.score(X_test, y_test):.1%} ← OVERFITTING!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fix with early stopping\n",
        "if HAS_XGBOOST:\n",
        "    gb_early = XGBClassifier(\n",
        "        n_estimators=500,\n",
        "        max_depth=4,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42,\n",
        "        verbosity=0,\n",
        "        early_stopping_rounds=10\n",
        "    )\n",
        "    \n",
        "    gb_early.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        verbose=False\n",
        "    )\n",
        "    \n",
        "    print(f\"With Early Stopping:\")\n",
        "    print(f\"  Stopped at round: {gb_early.best_iteration}\")\n",
        "    print(f\"  Train Accuracy: {gb_early.score(X_train, y_train):.1%}\")\n",
        "    print(f\"  Test Accuracy: {gb_early.score(X_test, y_test):.1%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Importance Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compare feature importance between methods\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Single Tree\n",
        "ax1 = axes[0]\n",
        "imp1 = pd.Series(single_tree.feature_importances_, index=available_features).sort_values()\n",
        "ax1.barh(imp1.index, imp1.values, color='#3b82f6')\n",
        "ax1.set_title('Single Tree')\n",
        "ax1.set_xlabel('Importance')\n",
        "\n",
        "# Random Forest\n",
        "ax2 = axes[1]\n",
        "imp2 = pd.Series(rf.feature_importances_, index=available_features).sort_values()\n",
        "ax2.barh(imp2.index, imp2.values, color='#22c55e')\n",
        "ax2.set_title('Random Forest')\n",
        "ax2.set_xlabel('Importance')\n",
        "\n",
        "# Gradient Boosting\n",
        "ax3 = axes[2]\n",
        "imp3 = pd.Series(gb.feature_importances_, index=available_features).sort_values()\n",
        "ax3.barh(imp3.index, imp3.values, color='#8b5cf6')\n",
        "ax3.set_title('Gradient Boosting')\n",
        "ax3.set_xlabel('Importance')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Final Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compare all models on test set\n",
        "models = {\n",
        "    'Single Tree': single_tree,\n",
        "    'Random Forest': rf,\n",
        "    'Gradient Boosting': gb\n",
        "}\n",
        "\n",
        "print(\"Final Model Comparison (Test Set)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    train_acc = model.score(X_train, y_train)\n",
        "    test_acc = model.score(X_test, y_test)\n",
        "    test_f1 = f1_score(y_test, model.predict(X_test))\n",
        "    \n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Train Acc': f'{train_acc:.1%}',\n",
        "        'Test Acc': f'{test_acc:.1%}',\n",
        "        'Test F1': f'{test_f1:.3f}',\n",
        "        'Overfit Gap': f'{train_acc - test_acc:.1%}'\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df.to_string(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Best model detailed report\n",
        "best_model = rf  # Usually Random Forest or Gradient Boosting\n",
        "best_name = 'Random Forest'\n",
        "\n",
        "print(f\"\\n{best_name} - Detailed Classification Report:\")\n",
        "print(classification_report(y_test, best_model.predict(X_test), target_names=['Retained', 'Churned']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Stakeholder Summary\n",
        "\n",
        "### TODO: Write a 150-250 word summary for the leadership team\n",
        "\n",
        "Include:\n",
        "1. Which model you recommend and why\n",
        "2. How it compares to a single decision tree\n",
        "3. Key features driving churn predictions\n",
        "4. Expected accuracy and any tradeoffs\n",
        "5. Recommendations for deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Summary:**\n",
        "\n",
        "_[Write your summary here]_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Self-Assessment Checklist\n",
        "\n",
        "- [ ] I compared single tree vs Random Forest vs Gradient Boosting\n",
        "- [ ] I observed diminishing returns with more trees\n",
        "- [ ] I used early stopping to prevent boosting overfitting\n",
        "- [ ] I extracted and compared feature importance\n",
        "- [ ] I can explain why ensembles outperform single trees\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. **Debug Drill:** Fix an overfit boosting model\n",
        "2. **Module 7:** Feature Engineering — make your features better"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}