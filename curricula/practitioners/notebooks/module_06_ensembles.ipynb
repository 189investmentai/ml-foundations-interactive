{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 6: Ensemble Methods for Churn Prediction\n",
        "\n",
        "**Goal:** Compare single trees, Random Forests, and Gradient Boosting for churn prediction. Find the best ensemble configuration.\n",
        "\n",
        "**Prerequisites:** Module 5 (Decision Trees)\n",
        "\n",
        "**Expected Runtime:** ~45 minutes\n",
        "\n",
        "**Outputs:**\n",
        "- Random Forest and Gradient Boosting model comparison\n",
        "- Hyperparameter tuning with early stopping\n",
        "- Feature importance extraction and interpretation\n",
        "- Stakeholder summary of ensemble results\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try to import XGBoost (optional but recommended)\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    HAS_XGBOOST = True\n",
        "    print(\"✓ XGBoost available\")\n",
        "except ImportError:\n",
        "    HAS_XGBOOST = False\n",
        "    print(\"⚠ XGBoost not installed, using sklearn GradientBoosting\")\n",
        "\n",
        "print(\"✓ Libraries loaded\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DATA_URL = 'https://raw.githubusercontent.com/189investmentai/ml-foundations-interactive/main/shared/data/'\n",
        "\n",
        "customers = pd.read_csv(DATA_URL + 'streamcart_customers.csv')\n",
        "print(f\"Loaded {len(customers)} customers\")\n",
        "\n",
        "# Feature engineering\n",
        "if 'tenure_days' not in customers.columns:\n",
        "    customers['tenure_days'] = (pd.to_datetime('2024-01-01') - pd.to_datetime(customers['signup_date'])).dt.days\n",
        "if 'avg_order_value' not in customers.columns:\n",
        "    customers['avg_order_value'] = customers['total_spend'] / customers['orders_total'].replace(0, 1)\n",
        "\n",
        "# Select features\n",
        "feature_cols = ['tenure_days', 'orders_total', 'total_spend', 'support_tickets_total', 'avg_order_value']\n",
        "available_features = [c for c in feature_cols if c in customers.columns]\n",
        "print(f\"Features: {available_features}\")\n",
        "\n",
        "X = customers[available_features].fillna(0)\n",
        "y = customers['churn_30d']\n",
        "\n",
        "# Train-validation-test split\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp)\n",
        "\n",
        "print(f\"\\nTrain: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)}\")\n",
        "print(f\"Churn rate: {y.mean():.1%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Baseline: Single Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train a single tree for baseline\n",
        "single_tree = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "single_tree.fit(X_train, y_train)\n",
        "\n",
        "print(\"Single Decision Tree:\")\n",
        "print(f\"  Train Accuracy: {single_tree.score(X_train, y_train):.1%}\")\n",
        "print(f\"  Val Accuracy: {single_tree.score(X_val, y_val):.1%}\")\n",
        "print(f\"  Test Accuracy: {single_tree.score(X_test, y_test):.1%}\")\n",
        "print(f\"  Depth: {single_tree.get_depth()}, Leaves: {single_tree.get_n_leaves()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=None,  # Let trees grow\n",
        "    min_samples_leaf=5,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Random Forest (100 trees):\")\n",
        "print(f\"  Train Accuracy: {rf.score(X_train, y_train):.1%}\")\n",
        "print(f\"  Val Accuracy: {rf.score(X_val, y_val):.1%}\")\n",
        "print(f\"  Test Accuracy: {rf.score(X_test, y_test):.1%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. TODO: Experiment with Number of Trees\n",
        "\n",
        "Your task: Find how many trees you need before returns diminish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Experiment: Test different numbers of trees to see diminishing returns\n",
        "\n",
        "n_trees_list = [1, 5, 10, 25, 50, 100, 200, 500]\n",
        "rf_results = []\n",
        "\n",
        "for n_trees in n_trees_list:\n",
        "    rf_temp = RandomForestClassifier(\n",
        "        n_estimators=n_trees,\n",
        "        max_depth=None,\n",
        "        min_samples_leaf=5,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    rf_temp.fit(X_train, y_train)\n",
        "    \n",
        "    # Calculate train, validation, and test accuracy\n",
        "    train_acc = rf_temp.score(X_train, y_train)\n",
        "    val_acc = rf_temp.score(X_val, y_val)\n",
        "    test_acc = rf_temp.score(X_test, y_test)\n",
        "    \n",
        "    rf_results.append({\n",
        "        'n_trees': n_trees,\n",
        "        'train_acc': train_acc,\n",
        "        'val_acc': val_acc,\n",
        "        'test_acc': test_acc\n",
        "    })\n",
        "\n",
        "rf_results_df = pd.DataFrame(rf_results)\n",
        "rf_results_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(rf_results_df['n_trees'], rf_results_df['train_acc'], 'b-o', label='Train', linewidth=2)\n",
        "plt.plot(rf_results_df['n_trees'], rf_results_df['val_acc'], 'g-s', label='Validation', linewidth=2)\n",
        "plt.plot(rf_results_df['n_trees'], rf_results_df['test_acc'], 'r-^', label='Test', linewidth=2)\n",
        "plt.xlabel('Number of Trees')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Random Forest: Accuracy vs Number of Trees')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xscale('log')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nObservation: Accuracy typically plateaus around 50-100 trees.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train Gradient Boosting\n",
        "if HAS_XGBOOST:\n",
        "    gb = XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=4,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42,\n",
        "        verbosity=0\n",
        "    )\n",
        "else:\n",
        "    gb = GradientBoostingClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=4,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "print(\"Gradient Boosting (100 rounds):\")\n",
        "print(f\"  Train Accuracy: {gb.score(X_train, y_train):.1%}\")\n",
        "print(f\"  Val Accuracy: {gb.score(X_val, y_val):.1%}\")\n",
        "print(f\"  Test Accuracy: {gb.score(X_test, y_test):.1%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Early Stopping (Boosting Best Practice)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Demonstrate overfitting without early stopping\n",
        "if HAS_XGBOOST:\n",
        "    gb_overfit = XGBClassifier(\n",
        "        n_estimators=500,  # Many rounds\n",
        "        max_depth=8,       # Deep trees\n",
        "        learning_rate=0.3, # Fast learning\n",
        "        random_state=42,\n",
        "        verbosity=0\n",
        "    )\n",
        "    gb_overfit.fit(X_train, y_train)\n",
        "    \n",
        "    print(\"Overfitting Example (500 rounds, depth 8, lr 0.3):\")\n",
        "    print(f\"  Train Accuracy: {gb_overfit.score(X_train, y_train):.1%}\")\n",
        "    print(f\"  Test Accuracy: {gb_overfit.score(X_test, y_test):.1%}\")\n",
        "    print(f\"  Gap: {gb_overfit.score(X_train, y_train) - gb_overfit.score(X_test, y_test):.1%} ← OVERFITTING!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fix with early stopping\n",
        "if HAS_XGBOOST:\n",
        "    gb_early = XGBClassifier(\n",
        "        n_estimators=500,\n",
        "        max_depth=4,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42,\n",
        "        verbosity=0,\n",
        "        early_stopping_rounds=10\n",
        "    )\n",
        "    \n",
        "    gb_early.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        verbose=False\n",
        "    )\n",
        "    \n",
        "    print(f\"With Early Stopping:\")\n",
        "    print(f\"  Stopped at round: {gb_early.best_iteration}\")\n",
        "    print(f\"  Train Accuracy: {gb_early.score(X_train, y_train):.1%}\")\n",
        "    print(f\"  Test Accuracy: {gb_early.score(X_test, y_test):.1%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Importance Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compare feature importance between methods\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Single Tree\n",
        "ax1 = axes[0]\n",
        "imp1 = pd.Series(single_tree.feature_importances_, index=available_features).sort_values()\n",
        "ax1.barh(imp1.index, imp1.values, color='#3b82f6')\n",
        "ax1.set_title('Single Tree')\n",
        "ax1.set_xlabel('Importance')\n",
        "\n",
        "# Random Forest\n",
        "ax2 = axes[1]\n",
        "imp2 = pd.Series(rf.feature_importances_, index=available_features).sort_values()\n",
        "ax2.barh(imp2.index, imp2.values, color='#22c55e')\n",
        "ax2.set_title('Random Forest')\n",
        "ax2.set_xlabel('Importance')\n",
        "\n",
        "# Gradient Boosting\n",
        "ax3 = axes[2]\n",
        "imp3 = pd.Series(gb.feature_importances_, index=available_features).sort_values()\n",
        "ax3.barh(imp3.index, imp3.values, color='#8b5cf6')\n",
        "ax3.set_title('Gradient Boosting')\n",
        "ax3.set_xlabel('Importance')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Final Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compare all models on test set\n",
        "models = {\n",
        "    'Single Tree': single_tree,\n",
        "    'Random Forest': rf,\n",
        "    'Gradient Boosting': gb\n",
        "}\n",
        "\n",
        "print(\"Final Model Comparison (Test Set)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    train_acc = model.score(X_train, y_train)\n",
        "    test_acc = model.score(X_test, y_test)\n",
        "    test_f1 = f1_score(y_test, model.predict(X_test))\n",
        "    \n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Train Acc': f'{train_acc:.1%}',\n",
        "        'Test Acc': f'{test_acc:.1%}',\n",
        "        'Test F1': f'{test_f1:.3f}',\n",
        "        'Overfit Gap': f'{train_acc - test_acc:.1%}'\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df.to_string(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Best model detailed report\n",
        "best_model = rf  # Usually Random Forest or Gradient Boosting\n",
        "best_name = 'Random Forest'\n",
        "\n",
        "print(f\"\\n{best_name} - Detailed Classification Report:\")\n",
        "print(classification_report(y_test, best_model.predict(X_test), target_names=['Retained', 'Churned']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Stakeholder Summary\n",
        "\n",
        "### TODO: Write a 3-bullet summary (~100 words) for the leadership team\n",
        "\n",
        "Template:\n",
        "• **Recommendation:** [Which model do you recommend (RF or GB)? Why?]\n",
        "• **Performance:** Test accuracy ___% vs single tree ___% - ensemble is [X]% better/worse\n",
        "• **Key drivers:** Top 2-3 features driving predictions and what they suggest about churn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Summary:**\n",
        "\n",
        "_[Write your summary here]_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Check\n",
        "\n",
        "Uncomment and run the asserts below to verify your ensemble models work correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# SELF-CHECK: Verify your ensemble models\n",
        "# Run this after training all models\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "tree_acc = accuracy_score(y_test, single_tree.predict(X_test))\n",
        "rf_acc = accuracy_score(y_test, rf.predict(X_test))\n",
        "gb_acc = accuracy_score(y_test, gb.predict(X_test))\n",
        "\n",
        "assert rf_acc >= tree_acc - 0.02, f\"Random Forest ({rf_acc:.3f}) should match or beat single tree ({tree_acc:.3f})\"\n",
        "assert gb_acc >= tree_acc - 0.02, f\"Gradient Boosting ({gb_acc:.3f}) should match or beat single tree ({tree_acc:.3f})\"\n",
        "assert len(rf.feature_importances_) > 0, \"Feature importances should be available\"\n",
        "assert len(rf_results_df) > 0, \"Should have results from n_trees experiment\"\n",
        "\n",
        "print(\"✅ Self-check passed!\")\n",
        "print(f\"   Single Tree: {tree_acc:.1%}\")\n",
        "print(f\"   Random Forest: {rf_acc:.1%}\")\n",
        "print(f\"   Gradient Boosting: {gb_acc:.1%}\")\n",
        "print(f\"   RF improvement over tree: {rf_acc - tree_acc:+.1%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Self-Assessment Checklist\n",
        "\n",
        "- [ ] I compared single tree vs Random Forest vs Gradient Boosting\n",
        "- [ ] I observed diminishing returns with more trees\n",
        "- [ ] I used early stopping to prevent boosting overfitting\n",
        "- [ ] I extracted and compared feature importance\n",
        "- [ ] I can explain why ensembles outperform single trees\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. **Debug Drill:** Fix an overfit boosting model\n",
        "2. **Module 7:** Feature Engineering - make your features better"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}