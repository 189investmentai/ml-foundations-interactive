{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 9: Classification Metrics\n",
        "\n",
        "**Goal:** Understand precision, recall, and why accuracy can be misleading. Learn to choose the right metric for your business problem.\n",
        "\n",
        "**Prerequisites:** Module 4 (Logistic Regression)\n",
        "\n",
        "**Expected Runtime:** ~25 minutes\n",
        "\n",
        "**Outputs:**\n",
        "- Confusion matrix analysis\n",
        "- ROC and PR curves\n",
        "- Threshold optimization\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report,\n",
        "    precision_score, recall_score, f1_score,\n",
        "    roc_curve, roc_auc_score,\n",
        "    precision_recall_curve, average_precision_score\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 11"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Generate Imbalanced Churn Data\n",
        "\n",
        "Real-world classification problems are often imbalanced. Let's create a dataset where only 10% of customers churn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n_samples = 2000\n",
        "churn_rate = 0.10  # 10% churn\n",
        "\n",
        "# Features\n",
        "tenure = np.random.uniform(0, 60, n_samples)\n",
        "monthly_charges = np.random.uniform(20, 120, n_samples)\n",
        "support_tickets = np.random.poisson(2, n_samples)\n",
        "usage_decline = np.random.uniform(-20, 50, n_samples)\n",
        "\n",
        "# Churn probability (logistic relationship)\n",
        "logit = -3 + (0.02 * usage_decline) + (0.1 * support_tickets) - (0.03 * tenure) + (0.01 * monthly_charges)\n",
        "prob = 1 / (1 + np.exp(-logit))\n",
        "churn = (np.random.random(n_samples) < prob).astype(int)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'tenure': tenure,\n",
        "    'monthly_charges': monthly_charges,\n",
        "    'support_tickets': support_tickets,\n",
        "    'usage_decline': usage_decline,\n",
        "    'churn': churn\n",
        "})\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nChurn distribution:\")\n",
        "print(df['churn'].value_counts(normalize=True).round(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Train a Model and Get Probabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = df[['tenure', 'monthly_charges', 'support_tickets', 'usage_decline']]\n",
        "y = df['churn']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get probabilities (not just predictions)\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "y_pred = model.predict(X_test)  # Default threshold = 0.5\n",
        "\n",
        "print(f\"Test set: {len(y_test)} samples\")\n",
        "print(f\"Actual churners: {y_test.sum()} ({y_test.mean()*100:.1f}%)\")\n",
        "print(f\"Predicted churners (threshold=0.5): {y_pred.sum()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: The Accuracy Trap\n",
        "\n",
        "Let's see why accuracy can be misleading."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate accuracy\n",
        "accuracy = (y_pred == y_test).mean()\n",
        "\n",
        "# What if we predicted NO CHURN for everyone?\n",
        "naive_pred = np.zeros_like(y_test)\n",
        "naive_accuracy = (naive_pred == y_test).mean()\n",
        "\n",
        "print(\"=== The Accuracy Trap ===\")\n",
        "print(f\"Our model accuracy: {accuracy*100:.1f}%\")\n",
        "print(f\"Naive 'no churn' accuracy: {naive_accuracy*100:.1f}%\")\n",
        "print(f\"\\nâš ï¸  Predicting 'no churn' for everyone gets {naive_accuracy*100:.1f}% accuracy!\")\n",
        "print(f\"    But catches 0% of actual churners.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Confusion Matrix Deep Dive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print(\"=== Confusion Matrix ===\")\n",
        "print(f\"\\n              Predicted\")\n",
        "print(f\"              No Churn    Churn\")\n",
        "print(f\"Actual No    TN={tn:<6}  FP={fp}\")\n",
        "print(f\"Actual Yes   FN={fn:<6}  TP={tp}\")\n",
        "\n",
        "print(\"\\n=== Interpretation ===\")\n",
        "print(f\"True Positives (TP):  {tp} - Correctly identified churners\")\n",
        "print(f\"True Negatives (TN):  {tn} - Correctly identified loyal customers\")\n",
        "print(f\"False Positives (FP): {fp} - Wrongly flagged as churners (wasted outreach)\")\n",
        "print(f\"False Negatives (FN): {fn} - Missed churners (lost customers!)\")\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(cm, cmap='Blues')\n",
        "\n",
        "ax.set_xticks([0, 1])\n",
        "ax.set_yticks([0, 1])\n",
        "ax.set_xticklabels(['Predicted: No Churn', 'Predicted: Churn'])\n",
        "ax.set_yticklabels(['Actual: No Churn', 'Actual: Churn'])\n",
        "\n",
        "labels = [['TN', 'FP'], ['FN', 'TP']]\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        text = f\"{labels[i][j]}\\n{cm[i,j]}\"\n",
        "        ax.text(j, i, text, ha='center', va='center', fontsize=14, \n",
        "                color='white' if cm[i,j] > cm.max()/2 else 'black')\n",
        "\n",
        "ax.set_title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Precision, Recall, and F1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"=== Key Metrics (at threshold=0.5) ===\")\n",
        "print(f\"\\nPrecision: {precision:.3f}\")\n",
        "print(f\"  â†’ Of {y_pred.sum()} predicted churners, {tp} were actual churners\")\n",
        "print(f\"  â†’ 'When we flag someone, we're right {precision*100:.0f}% of the time'\")\n",
        "\n",
        "print(f\"\\nRecall: {recall:.3f}\")\n",
        "print(f\"  â†’ Of {y_test.sum()} actual churners, we caught {tp}\")\n",
        "print(f\"  â†’ 'We catch {recall*100:.0f}% of all churners'\")\n",
        "\n",
        "print(f\"\\nF1 Score: {f1:.3f}\")\n",
        "print(f\"  â†’ Harmonic mean of precision and recall\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: The Threshold Effect\n",
        "\n",
        "The default threshold of 0.5 is rarely optimal. Let's explore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate metrics at different thresholds\n",
        "thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "results = []\n",
        "\n",
        "for t in thresholds:\n",
        "    y_pred_t = (y_prob >= t).astype(int)\n",
        "    cm_t = confusion_matrix(y_test, y_pred_t)\n",
        "    tn_t, fp_t, fn_t, tp_t = cm_t.ravel()\n",
        "    \n",
        "    prec = tp_t / (tp_t + fp_t) if (tp_t + fp_t) > 0 else 0\n",
        "    rec = tp_t / (tp_t + fn_t) if (tp_t + fn_t) > 0 else 0\n",
        "    f1_t = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
        "    \n",
        "    results.append({\n",
        "        'threshold': t,\n",
        "        'precision': prec,\n",
        "        'recall': rec,\n",
        "        'f1': f1_t,\n",
        "        'predicted_positive': y_pred_t.sum()\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1 = axes[0]\n",
        "ax1.plot(results_df['threshold'], results_df['precision'], 'b-', linewidth=2, label='Precision')\n",
        "ax1.plot(results_df['threshold'], results_df['recall'], 'g-', linewidth=2, label='Recall')\n",
        "ax1.plot(results_df['threshold'], results_df['f1'], 'r--', linewidth=2, label='F1')\n",
        "ax1.axvline(x=0.5, color='gray', linestyle=':', label='Default (0.5)')\n",
        "ax1.set_xlabel('Threshold')\n",
        "ax1.set_ylabel('Score')\n",
        "ax1.set_title('Precision-Recall Tradeoff')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2 = axes[1]\n",
        "ax2.bar(results_df['threshold'], results_df['predicted_positive'], width=0.04, color='steelblue')\n",
        "ax2.axhline(y=y_test.sum(), color='red', linestyle='--', label=f'Actual churners ({y_test.sum()})')\n",
        "ax2.set_xlabel('Threshold')\n",
        "ax2.set_ylabel('Number Predicted Positive')\n",
        "ax2.set_title('How Many We Flag')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Key Insight: Lower threshold = more flags = higher recall but lower precision\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: ROC Curve and AUC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate ROC curve\n",
        "fpr, tpr, roc_thresholds = roc_curve(y_test, y_prob)\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Calculate PR curve\n",
        "precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_test, y_prob)\n",
        "pr_auc = average_precision_score(y_test, y_prob)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# ROC Curve\n",
        "ax1 = axes[0]\n",
        "ax1.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {roc_auc:.3f})')\n",
        "ax1.plot([0, 1], [0, 1], 'k--', label='Random (AUC = 0.5)')\n",
        "ax1.fill_between(fpr, tpr, alpha=0.2)\n",
        "ax1.set_xlabel('False Positive Rate')\n",
        "ax1.set_ylabel('True Positive Rate (Recall)')\n",
        "ax1.set_title('ROC Curve')\n",
        "ax1.legend(loc='lower right')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# PR Curve\n",
        "ax2 = axes[1]\n",
        "ax2.plot(recall_curve, precision_curve, 'g-', linewidth=2, label=f'PR (AP = {pr_auc:.3f})')\n",
        "ax2.axhline(y=y_test.mean(), color='k', linestyle='--', label=f'Random ({y_test.mean():.2f})')\n",
        "ax2.fill_between(recall_curve, precision_curve, alpha=0.2, color='green')\n",
        "ax2.set_xlabel('Recall')\n",
        "ax2.set_ylabel('Precision')\n",
        "ax2.set_title('Precision-Recall Curve')\n",
        "ax2.legend(loc='lower left')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n=== Threshold-Independent Metrics ===\")\n",
        "print(f\"ROC-AUC: {roc_auc:.3f} - Model ranks churners higher than non-churners {roc_auc*100:.0f}% of the time\")\n",
        "print(f\"PR-AUC:  {pr_auc:.3f} - Better metric for imbalanced data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: TODO - Cost-Based Threshold Optimization\n",
        "\n",
        "If you know the business costs of errors, you can find the optimal threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Set your business costs\n",
        "# Cost of missing a churner (FN): lost customer revenue\n",
        "cost_fn = 500  # $ lost per missed churner\n",
        "\n",
        "# Cost of false alarm (FP): wasted marketing\n",
        "cost_fp = 30   # $ wasted per unnecessary outreach\n",
        "\n",
        "# Calculate total cost at each threshold\n",
        "threshold_costs = []\n",
        "\n",
        "for t in np.arange(0.05, 0.95, 0.01):\n",
        "    y_pred_t = (y_prob >= t).astype(int)\n",
        "    cm_t = confusion_matrix(y_test, y_pred_t)\n",
        "    tn_t, fp_t, fn_t, tp_t = cm_t.ravel()\n",
        "    \n",
        "    total_cost = (fn_t * cost_fn) + (fp_t * cost_fp)\n",
        "    \n",
        "    threshold_costs.append({\n",
        "        'threshold': t,\n",
        "        'total_cost': total_cost,\n",
        "        'fn_cost': fn_t * cost_fn,\n",
        "        'fp_cost': fp_t * cost_fp,\n",
        "        'fn': fn_t,\n",
        "        'fp': fp_t\n",
        "    })\n",
        "\n",
        "cost_df = pd.DataFrame(threshold_costs)\n",
        "optimal_idx = cost_df['total_cost'].idxmin()\n",
        "optimal_threshold = cost_df.loc[optimal_idx, 'threshold']\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(cost_df['threshold'], cost_df['total_cost'], 'b-', linewidth=2, label='Total Cost')\n",
        "plt.plot(cost_df['threshold'], cost_df['fn_cost'], 'r--', linewidth=1, label='FN Cost (missed churners)')\n",
        "plt.plot(cost_df['threshold'], cost_df['fp_cost'], 'g--', linewidth=1, label='FP Cost (wasted outreach)')\n",
        "plt.axvline(x=optimal_threshold, color='orange', linestyle='-', linewidth=2, label=f'Optimal ({optimal_threshold:.2f})')\n",
        "plt.axvline(x=0.5, color='gray', linestyle=':', label='Default (0.5)')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Cost ($)')\n",
        "plt.title('Cost-Based Threshold Optimization')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n=== Cost-Based Analysis ===\")\n",
        "print(f\"Cost assumptions: FN=${cost_fn}, FP=${cost_fp}\")\n",
        "print(f\"Optimal threshold: {optimal_threshold:.2f}\")\n",
        "print(f\"Expected theoretical optimal: {cost_fp / (cost_fp + cost_fn):.2f}\")\n",
        "\n",
        "# Compare default vs optimal\n",
        "default_cost = cost_df[cost_df['threshold'] == 0.5]['total_cost'].values[0] if 0.5 in cost_df['threshold'].values else cost_df.loc[(cost_df['threshold'] - 0.5).abs().idxmin(), 'total_cost']\n",
        "optimal_cost = cost_df.loc[optimal_idx, 'total_cost']\n",
        "\n",
        "print(f\"\\nCost at default (0.5): ${default_cost:,.0f}\")\n",
        "print(f\"Cost at optimal ({optimal_threshold:.2f}): ${optimal_cost:,.0f}\")\n",
        "print(f\"Savings: ${default_cost - optimal_cost:,.0f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: Stakeholder Summary\n",
        "\n",
        "### TODO: Write a 3-bullet summary (~100 words) for the marketing team\n",
        "\n",
        "Template:\n",
        "â€¢ **Model quality:** ROC-AUC = ____, meaning the model ranks churners above non-churners ____% of the time.\n",
        "â€¢ **Recommended threshold:** Use ____ instead of 0.5 because [cost reason]. This catches ____% of churners (recall).\n",
        "â€¢ **Expected impact:** At this threshold, expected cost savings are $____ compared to [baseline]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Summary:\n",
        "\n",
        "*Write your stakeholder summary here...*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Check\n",
        "\n",
        "Uncomment and run the asserts below to verify your classification metrics are correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# SELF-CHECK: Verify your classification metrics\n",
        "assert 0.5 <= roc_auc <= 1.0, f\"ROC-AUC should be between 0.5 and 1.0, got {roc_auc:.3f}\"\n",
        "assert 0 < pr_auc <= 1.0, f\"PR-AUC should be between 0 and 1.0, got {pr_auc:.3f}\"\n",
        "assert cm.shape == (2, 2), \"Confusion matrix should be 2x2\"\n",
        "assert 0.05 <= optimal_threshold <= 0.95, f\"Optimal threshold should be reasonable, got {optimal_threshold:.2f}\"\n",
        "print(f\"âœ… Self-check passed! ROC-AUC: {roc_auc:.3f}, PR-AUC: {pr_auc:.3f}, Threshold: {optimal_threshold:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Accuracy is often meaningless** on imbalanced data\n",
        "2. **Precision** = \"When I flag, am I right?\" (use when FP is costly)\n",
        "3. **Recall** = \"Did I catch all positives?\" (use when FN is costly)\n",
        "4. **Threshold** is a business decision, not a model decision\n",
        "5. **ROC-AUC** measures ranking ability; **PR-AUC** is better for imbalanced data\n",
        "\n",
        "### Next Steps\n",
        "- Explore the interactive playground to see the precision-recall tradeoff\n",
        "- Complete the quiz to test your understanding"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}