{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 11: Regularization\n",
    "\n",
    "**Goal:** Understand L1 and L2 regularization, tune lambda via cross-validation, and prevent overfitting.\n",
    "\n",
    "**Prerequisites:** Modules 3-4 (Linear/Logistic Regression), Module 10 (Feature Engineering)\n",
    "\n",
    "**Expected Runtime:** ~20 minutes\n",
    "\n",
    "**Outputs:**\n",
    "- L1 vs L2 coefficient comparison\n",
    "- Cross-validation lambda tuning\n",
    "- Train vs test performance analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, RidgeCV, LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams['figure.figsize'] = (12, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Generate Data with Known Sparsity\n",
    "\n",
    "We'll create data where only some features actually matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data where only first 5 features matter\n",
    "n_samples = 200\n",
    "n_features = 20\n",
    "n_informative = 5\n",
    "\n",
    "# Random features\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# True coefficients (only first 5 are non-zero)\n",
    "true_coefs = np.zeros(n_features)\n",
    "true_coefs[:n_informative] = np.array([3, -2, 1.5, -1, 0.5])\n",
    "\n",
    "# Generate target\n",
    "y = X @ true_coefs + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "print(f\"Dataset: {n_samples} samples, {n_features} features\")\n",
    "print(f\"True informative features: {n_informative}\")\n",
    "print(f\"\\nTrue coefficients:\")\n",
    "for i, c in enumerate(true_coefs):\n",
    "    if c != 0:\n",
    "        print(f\"  X{i+1}: {c:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Split and Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# IMPORTANT: Scale features before regularization!\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train: {len(X_train)} samples\")\n",
    "print(f\"Test: {len(X_test)} samples\")\n",
    "print(\"\\nâœ… Features scaled (mean=0, std=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Compare L1 vs L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models with same regularization strength\n",
    "alpha = 0.1\n",
    "\n",
    "ridge = Ridge(alpha=alpha)\n",
    "lasso = Lasso(alpha=alpha)\n",
    "\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Compare coefficients\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "x_pos = np.arange(n_features)\n",
    "\n",
    "# Ridge coefficients\n",
    "ax1 = axes[0]\n",
    "colors = ['#0ea5e9' if c > 0 else '#ef4444' for c in ridge.coef_]\n",
    "ax1.bar(x_pos, ridge.coef_, color=colors, alpha=0.7)\n",
    "ax1.axhline(y=0, color='gray', linestyle='--')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels([f'X{i+1}' for i in range(n_features)], rotation=45)\n",
    "ax1.set_title(f'Ridge (L2) Coefficients (alpha={alpha})')\n",
    "ax1.set_ylabel('Coefficient')\n",
    "\n",
    "# Lasso coefficients\n",
    "ax2 = axes[1]\n",
    "colors = ['#22c55e' if c > 0 else '#ef4444' if c < 0 else '#94a3b8' for c in lasso.coef_]\n",
    "ax2.bar(x_pos, lasso.coef_, color=colors, alpha=0.7)\n",
    "ax2.axhline(y=0, color='gray', linestyle='--')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels([f'X{i+1}' for i in range(n_features)], rotation=45)\n",
    "ax2.set_title(f'Lasso (L1) Coefficients (alpha={alpha})')\n",
    "ax2.set_ylabel('Coefficient')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== Coefficient Comparison ===\")\n",
    "print(f\"Ridge: {(np.abs(ridge.coef_) > 0.01).sum()} non-zero coefficients\")\n",
    "print(f\"Lasso: {(np.abs(lasso.coef_) > 0.01).sum()} non-zero coefficients\")\n",
    "print(f\"\\nðŸ’¡ Lasso zeros out irrelevant features â€” automatic feature selection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Regularization Path\n",
    "\n",
    "Let's see how coefficients change as we increase regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different alphas\n",
    "alphas = np.logspace(-3, 2, 50)\n",
    "\n",
    "ridge_coefs = []\n",
    "lasso_coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a).fit(X_train_scaled, y_train)\n",
    "    lasso = Lasso(alpha=a, max_iter=10000).fit(X_train_scaled, y_train)\n",
    "    ridge_coefs.append(ridge.coef_)\n",
    "    lasso_coefs.append(lasso.coef_)\n",
    "\n",
    "ridge_coefs = np.array(ridge_coefs)\n",
    "lasso_coefs = np.array(lasso_coefs)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Ridge path\n",
    "ax1 = axes[0]\n",
    "for i in range(n_features):\n",
    "    ax1.plot(alphas, ridge_coefs[:, i], label=f'X{i+1}' if i < 5 else None)\n",
    "ax1.set_xscale('log')\n",
    "ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('Alpha (log scale)')\n",
    "ax1.set_ylabel('Coefficient')\n",
    "ax1.set_title('Ridge (L2) Coefficient Path')\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "# Lasso path\n",
    "ax2 = axes[1]\n",
    "for i in range(n_features):\n",
    "    ax2.plot(alphas, lasso_coefs[:, i], label=f'X{i+1}' if i < 5 else None)\n",
    "ax2.set_xscale('log')\n",
    "ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Alpha (log scale)')\n",
    "ax2.set_ylabel('Coefficient')\n",
    "ax2.set_title('Lasso (L1) Coefficient Path')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Notice: Lasso coefficients hit exactly zero; Ridge just shrinks toward zero.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Finding Optimal Alpha via Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use built-in CV to find best alpha\n",
    "alphas_to_try = np.logspace(-3, 2, 100)\n",
    "\n",
    "ridge_cv = RidgeCV(alphas=alphas_to_try, cv=5)\n",
    "lasso_cv = LassoCV(alphas=alphas_to_try, cv=5, max_iter=10000)\n",
    "\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"=== Cross-Validation Results ===\")\n",
    "print(f\"\\nRidge optimal alpha: {ridge_cv.alpha_:.4f}\")\n",
    "print(f\"Lasso optimal alpha: {lasso_cv.alpha_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n=== Test Set Performance ===\")\n",
    "print(f\"Ridge Test RÂ²: {r2_score(y_test, ridge_cv.predict(X_test_scaled)):.4f}\")\n",
    "print(f\"Lasso Test RÂ²: {r2_score(y_test, lasso_cv.predict(X_test_scaled)):.4f}\")\n",
    "\n",
    "print(f\"\\nLasso selected {(np.abs(lasso_cv.coef_) > 0.01).sum()} features out of {n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Train vs Test Error Curve\n",
    "\n",
    "Visualize the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors_ridge = []\n",
    "test_errors_ridge = []\n",
    "train_errors_lasso = []\n",
    "test_errors_lasso = []\n",
    "\n",
    "for a in alphas:\n",
    "    # Ridge\n",
    "    ridge = Ridge(alpha=a).fit(X_train_scaled, y_train)\n",
    "    train_errors_ridge.append(mean_squared_error(y_train, ridge.predict(X_train_scaled)))\n",
    "    test_errors_ridge.append(mean_squared_error(y_test, ridge.predict(X_test_scaled)))\n",
    "    \n",
    "    # Lasso\n",
    "    lasso = Lasso(alpha=a, max_iter=10000).fit(X_train_scaled, y_train)\n",
    "    train_errors_lasso.append(mean_squared_error(y_train, lasso.predict(X_train_scaled)))\n",
    "    test_errors_lasso.append(mean_squared_error(y_test, lasso.predict(X_test_scaled)))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Ridge\n",
    "ax1 = axes[0]\n",
    "ax1.plot(alphas, train_errors_ridge, 'b-', label='Train MSE')\n",
    "ax1.plot(alphas, test_errors_ridge, 'r-', label='Test MSE')\n",
    "ax1.axvline(x=ridge_cv.alpha_, color='green', linestyle='--', label=f'CV optimal ({ridge_cv.alpha_:.3f})')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Alpha (log scale)')\n",
    "ax1.set_ylabel('MSE')\n",
    "ax1.set_title('Ridge: Train vs Test Error')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Lasso\n",
    "ax2 = axes[1]\n",
    "ax2.plot(alphas, train_errors_lasso, 'b-', label='Train MSE')\n",
    "ax2.plot(alphas, test_errors_lasso, 'r-', label='Test MSE')\n",
    "ax2.axvline(x=lasso_cv.alpha_, color='green', linestyle='--', label=f'CV optimal ({lasso_cv.alpha_:.3f})')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_xlabel('Alpha (log scale)')\n",
    "ax2.set_ylabel('MSE')\n",
    "ax2.set_title('Lasso: Train vs Test Error')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Insight:\")\n",
    "print(\"  â€¢ Left side (low alpha): Overfit â€” train error low, test error high\")\n",
    "print(\"  â€¢ Right side (high alpha): Underfit â€” both errors high\")\n",
    "print(\"  â€¢ Sweet spot: CV finds where test error is minimized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: TODO - Compare Feature Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare how well each method recovers the true features\n",
    "\n",
    "print(\"=== Feature Recovery Comparison ===\")\n",
    "print(\"\\nTrue coefficients (first 5 are informative):\")\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Feature': [f'X{i+1}' for i in range(n_features)],\n",
    "    'True': true_coefs,\n",
    "    'Ridge': ridge_cv.coef_,\n",
    "    'Lasso': lasso_cv.coef_\n",
    "})\n",
    "\n",
    "# Show first 10\n",
    "print(results.head(10).to_string(index=False))\n",
    "\n",
    "# Calculate recovery accuracy\n",
    "true_nonzero = set(np.where(np.abs(true_coefs) > 0.01)[0])\n",
    "lasso_nonzero = set(np.where(np.abs(lasso_cv.coef_) > 0.01)[0])\n",
    "\n",
    "print(f\"\\nTrue informative features: {true_nonzero}\")\n",
    "print(f\"Lasso selected features: {lasso_nonzero}\")\n",
    "print(f\"Correctly identified: {true_nonzero & lasso_nonzero}\")\n",
    "print(f\"False positives: {lasso_nonzero - true_nonzero}\")\n",
    "print(f\"Missed: {true_nonzero - lasso_nonzero}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: TODO - Elastic Net\n",
    "\n",
    "When to use Elastic Net: combines L1 and L2 benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try Elastic Net with different l1_ratio\n",
    "# l1_ratio=1 is pure Lasso, l1_ratio=0 is pure Ridge\n",
    "\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "# Test different L1/L2 mixes\n",
    "l1_ratios = [0.1, 0.5, 0.9]\n",
    "\n",
    "print(\"=== Elastic Net Comparison ===\")\n",
    "for ratio in l1_ratios:\n",
    "    elastic = ElasticNetCV(l1_ratio=ratio, alphas=alphas_to_try, cv=5, max_iter=10000)\n",
    "    elastic.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    n_selected = (np.abs(elastic.coef_) > 0.01).sum()\n",
    "    test_r2 = r2_score(y_test, elastic.predict(X_test_scaled))\n",
    "    \n",
    "    print(f\"\\nl1_ratio={ratio:.1f}: {n_selected} features, RÂ²={test_r2:.4f}, alpha={elastic.alpha_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: TODO - Stakeholder Summary\n",
    "\n",
    "Explain to a PM:\n",
    "1. What regularization does and why it helps\n",
    "2. Which type (L1 or L2) you'd recommend for this problem\n",
    "3. How you chose the regularization strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Summary:\n",
    "\n",
    "*Write your explanation here...*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **L1 (Lasso)** zeros out coefficients â†’ automatic feature selection\n",
    "2. **L2 (Ridge)** shrinks all coefficients â†’ stable when features correlated\n",
    "3. **Always scale** before applying regularization\n",
    "4. **Use cross-validation** to find optimal alpha\n",
    "5. **Monitor train vs test** error to detect over/underfitting\n",
    "\n",
    "### sklearn Parameter Gotcha\n",
    "- Ridge/Lasso: `alpha` = Î» (higher = more regularization)\n",
    "- LogisticRegression: `C` = 1/Î» (higher = LESS regularization)\n",
    "\n",
    "### Next Steps\n",
    "- Explore the interactive playground\n",
    "- Complete the quiz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
