{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 11: Regularization\n",
        "\n",
        "**Goal:** Understand L1 and L2 regularization, tune lambda via cross-validation, and prevent overfitting.\n",
        "\n",
        "**Prerequisites:** Modules 3-4 (Linear/Logistic Regression), Module 10 (Feature Engineering)\n",
        "\n",
        "**Expected Runtime:** ~20 minutes\n",
        "\n",
        "**Outputs:**\n",
        "- L1 vs L2 coefficient comparison\n",
        "- Cross-validation lambda tuning\n",
        "- Train vs test performance analysis\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet, RidgeCV, LassoCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "plt.rcParams['figure.figsize'] = (12, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Generate Data with Known Sparsity\n",
        "\n",
        "We'll create data where only some features actually matter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate data where only first 5 features matter\n",
        "n_samples = 200\n",
        "n_features = 20\n",
        "n_informative = 5\n",
        "\n",
        "# Random features\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "\n",
        "# True coefficients (only first 5 are non-zero)\n",
        "true_coefs = np.zeros(n_features)\n",
        "true_coefs[:n_informative] = np.array([3, -2, 1.5, -1, 0.5])\n",
        "\n",
        "# Generate target\n",
        "y = X @ true_coefs + np.random.randn(n_samples) * 0.5\n",
        "\n",
        "print(f\"Dataset: {n_samples} samples, {n_features} features\")\n",
        "print(f\"True informative features: {n_informative}\")\n",
        "print(f\"\\nTrue coefficients:\")\n",
        "for i, c in enumerate(true_coefs):\n",
        "    if c != 0:\n",
        "        print(f\"  X{i+1}: {c:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Split and Scale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# IMPORTANT: Scale features before regularization!\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Train: {len(X_train)} samples\")\n",
        "print(f\"Test: {len(X_test)} samples\")\n",
        "print(\"\\n‚úÖ Features scaled (mean=0, std=1)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DEMO: What happens WITHOUT scaling?\n",
        "# Let's see Lasso fail when features are on different scales\n",
        "\n",
        "# Create data with varying scales\n",
        "X_varied = X.copy()\n",
        "X_varied[:, 0] *= 1000  # First feature: range ~1000\n",
        "X_varied[:, 1] *= 100   # Second feature: range ~100\n",
        "X_varied[:, 2] *= 0.01  # Third feature: range ~0.01\n",
        "\n",
        "# Lasso without scaling\n",
        "lasso_unscaled = Lasso(alpha=1.0).fit(X_varied, y)\n",
        "\n",
        "print(\"=== ‚ö†Ô∏è FAILURE MODE: Lasso WITHOUT Scaling ===\")\n",
        "print(\"\\nTrue coefficients: X1=3, X2=-2, X3=1.5, X4=-1, X5=0.5\")\n",
        "print(\"\\nLasso coefficients (UNSCALED features):\")\n",
        "for i in range(5):\n",
        "    coef = lasso_unscaled.coef_[i]\n",
        "    status = \"ZEROED!\" if abs(coef) < 0.001 else f\"{coef:.4f}\"\n",
        "    print(f\"  X{i+1}: {status}\")\n",
        "\n",
        "print(\"\\nüö® PROBLEM: Feature scale affects which coefficients get zeroed!\")\n",
        "print(\"   X1 (scale 1000) ‚Üí coefficient appears tiny ‚Üí wrongly kept\")\n",
        "print(\"   X3 (scale 0.01) ‚Üí coefficient appears huge ‚Üí wrongly zeroed\")\n",
        "print(\"\\nüí° SOLUTION: Always scale features BEFORE regularization!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Compare L1 vs L2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fit models with same regularization strength\n",
        "alpha = 0.1\n",
        "\n",
        "ridge = Ridge(alpha=alpha)\n",
        "lasso = Lasso(alpha=alpha)\n",
        "\n",
        "ridge.fit(X_train_scaled, y_train)\n",
        "lasso.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Compare coefficients\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "x_pos = np.arange(n_features)\n",
        "\n",
        "# Ridge coefficients\n",
        "ax1 = axes[0]\n",
        "colors = ['#0ea5e9' if c > 0 else '#ef4444' for c in ridge.coef_]\n",
        "ax1.bar(x_pos, ridge.coef_, color=colors, alpha=0.7)\n",
        "ax1.axhline(y=0, color='gray', linestyle='--')\n",
        "ax1.set_xticks(x_pos)\n",
        "ax1.set_xticklabels([f'X{i+1}' for i in range(n_features)], rotation=45)\n",
        "ax1.set_title(f'Ridge (L2) Coefficients (alpha={alpha})')\n",
        "ax1.set_ylabel('Coefficient')\n",
        "\n",
        "# Lasso coefficients\n",
        "ax2 = axes[1]\n",
        "colors = ['#22c55e' if c > 0 else '#ef4444' if c < 0 else '#94a3b8' for c in lasso.coef_]\n",
        "ax2.bar(x_pos, lasso.coef_, color=colors, alpha=0.7)\n",
        "ax2.axhline(y=0, color='gray', linestyle='--')\n",
        "ax2.set_xticks(x_pos)\n",
        "ax2.set_xticklabels([f'X{i+1}' for i in range(n_features)], rotation=45)\n",
        "ax2.set_title(f'Lasso (L1) Coefficients (alpha={alpha})')\n",
        "ax2.set_ylabel('Coefficient')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=== Coefficient Comparison ===\")\n",
        "print(f\"Ridge: {(np.abs(ridge.coef_) > 0.01).sum()} non-zero coefficients\")\n",
        "print(f\"Lasso: {(np.abs(lasso.coef_) > 0.01).sum()} non-zero coefficients\")\n",
        "print(f\"\\nüí° Lasso zeros out irrelevant features - automatic feature selection!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Regularization Path\n",
        "\n",
        "Let's see how coefficients change as we increase regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test different alphas\n",
        "alphas = np.logspace(-3, 2, 50)\n",
        "\n",
        "ridge_coefs = []\n",
        "lasso_coefs = []\n",
        "\n",
        "for a in alphas:\n",
        "    ridge = Ridge(alpha=a).fit(X_train_scaled, y_train)\n",
        "    lasso = Lasso(alpha=a, max_iter=10000).fit(X_train_scaled, y_train)\n",
        "    ridge_coefs.append(ridge.coef_)\n",
        "    lasso_coefs.append(lasso.coef_)\n",
        "\n",
        "ridge_coefs = np.array(ridge_coefs)\n",
        "lasso_coefs = np.array(lasso_coefs)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Ridge path\n",
        "ax1 = axes[0]\n",
        "for i in range(n_features):\n",
        "    ax1.plot(alphas, ridge_coefs[:, i], label=f'X{i+1}' if i < 5 else None)\n",
        "ax1.set_xscale('log')\n",
        "ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
        "ax1.set_xlabel('Alpha (log scale)')\n",
        "ax1.set_ylabel('Coefficient')\n",
        "ax1.set_title('Ridge (L2) Coefficient Path')\n",
        "ax1.legend(loc='upper right')\n",
        "\n",
        "# Lasso path\n",
        "ax2 = axes[1]\n",
        "for i in range(n_features):\n",
        "    ax2.plot(alphas, lasso_coefs[:, i], label=f'X{i+1}' if i < 5 else None)\n",
        "ax2.set_xscale('log')\n",
        "ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
        "ax2.set_xlabel('Alpha (log scale)')\n",
        "ax2.set_ylabel('Coefficient')\n",
        "ax2.set_title('Lasso (L1) Coefficient Path')\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üí° Notice: Lasso coefficients hit exactly zero; Ridge just shrinks toward zero.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Finding Optimal Alpha via Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Use built-in CV to find best alpha\n",
        "alphas_to_try = np.logspace(-3, 2, 100)\n",
        "\n",
        "ridge_cv = RidgeCV(alphas=alphas_to_try, cv=5)\n",
        "lasso_cv = LassoCV(alphas=alphas_to_try, cv=5, max_iter=10000)\n",
        "\n",
        "ridge_cv.fit(X_train_scaled, y_train)\n",
        "lasso_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"=== Cross-Validation Results ===\")\n",
        "print(f\"\\nRidge optimal alpha: {ridge_cv.alpha_:.4f}\")\n",
        "print(f\"Lasso optimal alpha: {lasso_cv.alpha_:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"\\n=== Test Set Performance ===\")\n",
        "print(f\"Ridge Test R¬≤: {r2_score(y_test, ridge_cv.predict(X_test_scaled)):.4f}\")\n",
        "print(f\"Lasso Test R¬≤: {r2_score(y_test, lasso_cv.predict(X_test_scaled)):.4f}\")\n",
        "\n",
        "print(f\"\\nLasso selected {(np.abs(lasso_cv.coef_) > 0.01).sum()} features out of {n_features}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Train vs Test Error Curve\n",
        "\n",
        "Visualize the bias-variance tradeoff."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_errors_ridge = []\n",
        "test_errors_ridge = []\n",
        "train_errors_lasso = []\n",
        "test_errors_lasso = []\n",
        "\n",
        "for a in alphas:\n",
        "    # Ridge\n",
        "    ridge = Ridge(alpha=a).fit(X_train_scaled, y_train)\n",
        "    train_errors_ridge.append(mean_squared_error(y_train, ridge.predict(X_train_scaled)))\n",
        "    test_errors_ridge.append(mean_squared_error(y_test, ridge.predict(X_test_scaled)))\n",
        "    \n",
        "    # Lasso\n",
        "    lasso = Lasso(alpha=a, max_iter=10000).fit(X_train_scaled, y_train)\n",
        "    train_errors_lasso.append(mean_squared_error(y_train, lasso.predict(X_train_scaled)))\n",
        "    test_errors_lasso.append(mean_squared_error(y_test, lasso.predict(X_test_scaled)))\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Ridge\n",
        "ax1 = axes[0]\n",
        "ax1.plot(alphas, train_errors_ridge, 'b-', label='Train MSE')\n",
        "ax1.plot(alphas, test_errors_ridge, 'r-', label='Test MSE')\n",
        "ax1.axvline(x=ridge_cv.alpha_, color='green', linestyle='--', label=f'CV optimal ({ridge_cv.alpha_:.3f})')\n",
        "ax1.set_xscale('log')\n",
        "ax1.set_xlabel('Alpha (log scale)')\n",
        "ax1.set_ylabel('MSE')\n",
        "ax1.set_title('Ridge: Train vs Test Error')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Lasso\n",
        "ax2 = axes[1]\n",
        "ax2.plot(alphas, train_errors_lasso, 'b-', label='Train MSE')\n",
        "ax2.plot(alphas, test_errors_lasso, 'r-', label='Test MSE')\n",
        "ax2.axvline(x=lasso_cv.alpha_, color='green', linestyle='--', label=f'CV optimal ({lasso_cv.alpha_:.3f})')\n",
        "ax2.set_xscale('log')\n",
        "ax2.set_xlabel('Alpha (log scale)')\n",
        "ax2.set_ylabel('MSE')\n",
        "ax2.set_title('Lasso: Train vs Test Error')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üí° Insight:\")\n",
        "print(\"  ‚Ä¢ Left side (low alpha): Overfit - train error low, test error high\")\n",
        "print(\"  ‚Ä¢ Right side (high alpha): Underfit - both errors high\")\n",
        "print(\"  ‚Ä¢ Sweet spot: CV finds where test error is minimized\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: TODO - Compare Feature Recovery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Compare how well each method recovers the true features\n",
        "\n",
        "print(\"=== Feature Recovery Comparison ===\")\n",
        "print(\"\\nTrue coefficients (first 5 are informative):\")\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    'Feature': [f'X{i+1}' for i in range(n_features)],\n",
        "    'True': true_coefs,\n",
        "    'Ridge': ridge_cv.coef_,\n",
        "    'Lasso': lasso_cv.coef_\n",
        "})\n",
        "\n",
        "# Show first 10\n",
        "print(results.head(10).to_string(index=False))\n",
        "\n",
        "# Calculate recovery accuracy\n",
        "true_nonzero = set(np.where(np.abs(true_coefs) > 0.01)[0])\n",
        "lasso_nonzero = set(np.where(np.abs(lasso_cv.coef_) > 0.01)[0])\n",
        "\n",
        "print(f\"\\nTrue informative features: {true_nonzero}\")\n",
        "print(f\"Lasso selected features: {lasso_nonzero}\")\n",
        "print(f\"Correctly identified: {true_nonzero & lasso_nonzero}\")\n",
        "print(f\"False positives: {lasso_nonzero - true_nonzero}\")\n",
        "print(f\"Missed: {true_nonzero - lasso_nonzero}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: TODO - Elastic Net\n",
        "\n",
        "When to use Elastic Net: combines L1 and L2 benefits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Try Elastic Net with different l1_ratio\n",
        "# l1_ratio=1 is pure Lasso, l1_ratio=0 is pure Ridge\n",
        "\n",
        "from sklearn.linear_model import ElasticNetCV\n",
        "\n",
        "# Test different L1/L2 mixes\n",
        "l1_ratios = [0.1, 0.5, 0.9]\n",
        "\n",
        "print(\"=== Elastic Net Comparison ===\")\n",
        "for ratio in l1_ratios:\n",
        "    elastic = ElasticNetCV(l1_ratio=ratio, alphas=alphas_to_try, cv=5, max_iter=10000)\n",
        "    elastic.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    n_selected = (np.abs(elastic.coef_) > 0.01).sum()\n",
        "    test_r2 = r2_score(y_test, elastic.predict(X_test_scaled))\n",
        "    \n",
        "    print(f\"\\nl1_ratio={ratio:.1f}: {n_selected} features, R¬≤={test_r2:.4f}, alpha={elastic.alpha_:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Check\n",
        "\n",
        "Uncomment and run the asserts below to verify your regularization models are correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# SELF-CHECK: Verify your regularization models\n",
        "assert hasattr(ridge, 'coef_'), \"Ridge model should be fitted\"\n",
        "assert hasattr(lasso, 'coef_'), \"Lasso model should be fitted\"\n",
        "n_lasso_zero = (np.abs(lasso.coef_) < 0.01).sum()\n",
        "assert n_lasso_zero > 0, \"Lasso should zero out at least some coefficients\"\n",
        "print(f\"‚úÖ Self-check passed! Lasso zeroed {n_lasso_zero}/{len(lasso.coef_)} coefficients\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: Stakeholder Summary\n",
        "\n",
        "### TODO: Write a 3-bullet summary (~100 words) for the PM\n",
        "\n",
        "Template:\n",
        "‚Ä¢ **What regularization does:** Prevents overfitting by [penalty description]. This is important when [scenario].\n",
        "‚Ä¢ **Recommendation:** Use [L1/L2/ElasticNet] because [reason based on feature selection needs].\n",
        "‚Ä¢ **How we tuned it:** Cross-validation found optimal Œª = ____, selecting ____ features with R¬≤ = ____."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Summary:\n",
        "\n",
        "*Write your explanation here...*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **L1 (Lasso)** zeros out coefficients ‚Üí automatic feature selection\n",
        "2. **L2 (Ridge)** shrinks all coefficients ‚Üí stable when features correlated\n",
        "3. **Always scale** before applying regularization\n",
        "4. **Use cross-validation** to find optimal alpha\n",
        "5. **Monitor train vs test** error to detect over/underfitting\n",
        "\n",
        "### sklearn Parameter Gotcha\n",
        "- Ridge/Lasso: `alpha` = Œª (higher = more regularization)\n",
        "- LogisticRegression: `C` = 1/Œª (higher = LESS regularization)\n",
        "\n",
        "### Next Steps\n",
        "- Explore the interactive playground\n",
        "- Complete the quiz"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}