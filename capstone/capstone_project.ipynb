{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: StreamCart Retention System\n",
    "\n",
    "**Scenario:** StreamCart's retention team can call 500 customers per week. Your job is to identify WHICH 500 customers to call.\n",
    "\n",
    "**Your Deliverables:**\n",
    "1. Problem framing document\n",
    "2. Feature engineering pipeline\n",
    "3. Trained model with proper evaluation\n",
    "4. Business recommendation\n",
    "\n",
    "**Runtime:** ~15 minutes on free Colab\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Install lightgbm if needed\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except:\n",
    "    !pip install lightgbm -q\n",
    "    import lightgbm as lgb\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "DATA_URL = \"https://raw.githubusercontent.com/189investmentai/ml-foundations-interactive/main/\"\n",
    "\n",
    "customers = pd.read_csv(DATA_URL + \"streamcart_customers.csv\")\n",
    "events = pd.read_csv(DATA_URL + \"streamcart_events.csv\")\n",
    "\n",
    "print(f\"Customers: {len(customers):,} rows\")\n",
    "print(f\"Events: {len(events):,} rows\")\n",
    "print(f\"\\nChurn rate: {customers['churn_30d'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Problem Framing (20 points)\n",
    "\n",
    "Before touching any models, define the problem clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The Business Context\n",
    "\n",
    "Read the scenario carefully:\n",
    "\n",
    "> The retention team can make **500 outbound calls per week** to offer personalized discounts. Currently they call at random. Historical data shows that when the team calls a customer who was going to churn, they save that customer **30% of the time**. Each saved customer is worth **$200** in future revenue. Each call costs **$15** in labor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the 7-line framing template\n",
    "\n",
    "framing = \"\"\"\n",
    "=== ML PROBLEM FRAMING ===\n",
    "\n",
    "1. Business Goal: [What outcome does StreamCart want?]\n",
    "   YOUR ANSWER: \n",
    "\n",
    "2. ML Task Type: [Classification / Regression / Ranking / Clustering]\n",
    "   YOUR ANSWER: \n",
    "\n",
    "3. Target (y): [What exactly are we predicting? Be specific about time window]\n",
    "   YOUR ANSWER: \n",
    "\n",
    "4. Prediction Point: [When do we make predictions?]\n",
    "   YOUR ANSWER: \n",
    "\n",
    "5. Features (X): [What data is available BEFORE prediction point?]\n",
    "   YOUR ANSWER: \n",
    "\n",
    "6. Success Metric: [How do we measure model quality given the 500-call constraint?]\n",
    "   YOUR ANSWER: \n",
    "\n",
    "7. Business Action: [What happens with predictions?]\n",
    "   YOUR ANSWER: \n",
    "\n",
    "\"\"\"\n",
    "print(framing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Baseline Calculation\n",
    "\n",
    "Before building any model, calculate what random selection would achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the baseline (random selection)\n",
    "\n",
    "base_churn_rate = customers['churn_30d'].mean()\n",
    "calls_per_week = 500\n",
    "save_rate = 0.30  # 30% of churners can be saved\n",
    "value_per_save = 200  # $200 per saved customer\n",
    "cost_per_call = 15  # $15 per call\n",
    "\n",
    "# Random selection baseline\n",
    "# TODO: Calculate expected churners reached with random 500 calls\n",
    "expected_churners_random = None  # YOUR CODE HERE\n",
    "\n",
    "# TODO: Calculate expected saves (churners reached * save_rate)\n",
    "expected_saves_random = None  # YOUR CODE HERE\n",
    "\n",
    "# TODO: Calculate net value (saves * value - calls * cost)\n",
    "net_value_random = None  # YOUR CODE HERE\n",
    "\n",
    "print(f\"=== RANDOM SELECTION BASELINE ===\")\n",
    "print(f\"Churn rate: {base_churn_rate:.1%}\")\n",
    "print(f\"Expected churners in 500 random calls: {expected_churners_random:.0f}\")\n",
    "print(f\"Expected saves: {expected_saves_random:.0f}\")\n",
    "print(f\"Net value: ${net_value_random:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-check: Baseline calculation\n",
    "assert expected_churners_random is not None, \"Calculate expected_churners_random\"\n",
    "assert 30 < expected_churners_random < 100, f\"Check your calculation: {expected_churners_random}\"\n",
    "print(\"✓ Part 1.2 baseline calculation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Feature Engineering (20 points)\n",
    "\n",
    "Create meaningful features from raw data. Remember: **no leakage!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data first\n",
    "print(\"=== CUSTOMER DATA ===\")\n",
    "print(customers.columns.tolist())\n",
    "print(\"\\nSample:\")\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for potential leakage columns\n",
    "print(\"=== LEAKAGE CHECK ===\")\n",
    "print(\"\\nColumns to AVOID (contain future info):\")\n",
    "leaky_columns = ['churn_date', 'cancel_reason', 'churn_30d']  # target is fine to use as y\n",
    "for col in leaky_columns:\n",
    "    if col in customers.columns:\n",
    "        print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Create Derived Features\n",
    "\n",
    "Create at least 3 meaningful features. Ideas:\n",
    "- **Ratio features:** orders per month, logins per month\n",
    "- **Change features:** recent activity vs historical\n",
    "- **Interaction features:** combinations that might signal risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your feature engineering pipeline\n",
    "\n",
    "df = customers.copy()\n",
    "\n",
    "# Feature 1: Orders per month (ratio feature)\n",
    "# TODO: Create orders_per_month = orders_last_30d / (tenure_months + 1)\n",
    "df['orders_per_month'] = None  # YOUR CODE HERE\n",
    "\n",
    "# Feature 2: Support intensity (ratio feature)\n",
    "# TODO: Create support_intensity = support_tickets_last_30d / (tenure_months + 1)\n",
    "df['support_intensity'] = None  # YOUR CODE HERE\n",
    "\n",
    "# Feature 3: YOUR CHOICE - create at least one more meaningful feature\n",
    "# Ideas: engagement_score, login_trend, nps_risk, etc.\n",
    "# TODO: Create your feature\n",
    "df['your_feature'] = None  # YOUR CODE HERE - rename this column!\n",
    "\n",
    "print(\"Engineered features created:\")\n",
    "print(df[['orders_per_month', 'support_intensity', 'your_feature']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-check: Feature engineering\n",
    "assert df['orders_per_month'].notna().all(), \"orders_per_month has NaN values\"\n",
    "assert df['support_intensity'].notna().all(), \"support_intensity has NaN values\"\n",
    "print(\"✓ Part 2.1 feature engineering complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Leakage Audit\n",
    "\n",
    "For each feature, confirm it would be available at prediction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the leakage audit\n",
    "\n",
    "leakage_audit = \"\"\"\n",
    "=== FEATURE LEAKAGE AUDIT ===\n",
    "\n",
    "For each feature, answer: \"Would I have this at prediction time?\"\n",
    "\n",
    "| Feature | Source | Available at prediction? | Safe? |\n",
    "|---------|--------|-------------------------|-------|\n",
    "| tenure_months | Account age | Yes - historical | ✓ |\n",
    "| logins_last_30d | Activity logs | Yes - historical | ✓ |\n",
    "| orders_per_month | Derived from orders + tenure | Yes | ✓ |\n",
    "| support_intensity | Derived from tickets + tenure | Yes | ✓ |\n",
    "| your_feature | TODO: document source | TODO | TODO |\n",
    "\n",
    "Confirm: None of my features use churn_date, cancel_reason, or future data.\n",
    "\"\"\"\n",
    "print(leakage_audit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Prepare Final Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your final feature list\n",
    "\n",
    "# Base features (from raw data)\n",
    "base_features = [\n",
    "    'tenure_months',\n",
    "    'logins_last_30d',\n",
    "    'orders_last_30d',\n",
    "    'support_tickets_last_30d',\n",
    "    'nps_score'\n",
    "]\n",
    "\n",
    "# Engineered features (your creations)\n",
    "engineered_features = [\n",
    "    'orders_per_month',\n",
    "    'support_intensity',\n",
    "    # TODO: Add your custom feature name here\n",
    "]\n",
    "\n",
    "all_features = base_features + engineered_features\n",
    "\n",
    "# Prepare X and y\n",
    "X = df[all_features].fillna(0)\n",
    "y = df['churn_30d']\n",
    "\n",
    "print(f\"Features: {len(all_features)}\")\n",
    "print(f\"Samples: {len(X):,}\")\n",
    "print(f\"Target distribution: {y.mean():.1%} churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Model Training (20 points)\n",
    "\n",
    "Train at least two models: a simple baseline and an advanced model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create proper train/val/test splits\n",
    "\n",
    "# First split: 80% train+val, 20% test (holdout)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: 75% train, 25% val (from the 80%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train):,} samples\")\n",
    "print(f\"Validation set: {len(X_val):,} samples\")\n",
    "print(f\"Test set: {len(X_test):,} samples (HOLDOUT - don't touch until final eval!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Model 1: Logistic Regression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train logistic regression baseline\n",
    "\n",
    "# Scale features for logistic regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Train model\n",
    "model_lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model_lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate on validation\n",
    "probs_lr_val = model_lr.predict_proba(X_val_scaled)[:, 1]\n",
    "auc_lr = roc_auc_score(y_val, probs_lr_val)\n",
    "\n",
    "print(f\"Logistic Regression Validation AUC: {auc_lr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': all_features,\n",
    "    'coefficient': model_lr.coef_[0]\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance (Logistic Regression):\")\n",
    "for _, row in coef_df.iterrows():\n",
    "    direction = \"↑ churn\" if row['coefficient'] > 0 else \"↓ churn\"\n",
    "    print(f\"  {row['feature']}: {row['coefficient']:.3f} ({direction})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Model 2: LightGBM with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train LightGBM with proper regularization and early stopping\n",
    "\n",
    "model_lgb = lgb.LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    num_leaves=31,\n",
    "    max_depth=6,\n",
    "    min_child_samples=20,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Train with early stopping\n",
    "model_lgb.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    ")\n",
    "\n",
    "# Evaluate on validation\n",
    "probs_lgb_val = model_lgb.predict_proba(X_val)[:, 1]\n",
    "auc_lgb = roc_auc_score(y_val, probs_lgb_val)\n",
    "\n",
    "print(f\"LightGBM Validation AUC: {auc_lgb:.3f}\")\n",
    "print(f\"Trees used: {model_lgb.best_iteration_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overfitting\n",
    "probs_lgb_train = model_lgb.predict_proba(X_train)[:, 1]\n",
    "auc_lgb_train = roc_auc_score(y_train, probs_lgb_train)\n",
    "\n",
    "print(f\"\\nOverfitting check:\")\n",
    "print(f\"  Training AUC: {auc_lgb_train:.3f}\")\n",
    "print(f\"  Validation AUC: {auc_lgb:.3f}\")\n",
    "print(f\"  Gap: {auc_lgb_train - auc_lgb:.3f}\")\n",
    "\n",
    "if auc_lgb_train - auc_lgb > 0.05:\n",
    "    print(\"  ⚠️ Possible overfitting - consider more regularization\")\n",
    "else:\n",
    "    print(\"  ✓ Gap is acceptable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-check: Models trained\n",
    "assert auc_lr > 0.55, f\"Logistic regression AUC too low: {auc_lr}\"\n",
    "assert auc_lgb > 0.55, f\"LightGBM AUC too low: {auc_lgb}\"\n",
    "print(\"✓ Part 3 models trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Evaluation & Deployment (20 points)\n",
    "\n",
    "Evaluate models on the **test set** using business-relevant metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Final Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "probs_lr_test = model_lr.predict_proba(X_test_scaled)[:, 1]\n",
    "probs_lgb_test = model_lgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# AUC comparison\n",
    "auc_lr_test = roc_auc_score(y_test, probs_lr_test)\n",
    "auc_lgb_test = roc_auc_score(y_test, probs_lgb_test)\n",
    "\n",
    "print(\"=== TEST SET RESULTS ===\")\n",
    "print(f\"Logistic Regression AUC: {auc_lr_test:.3f}\")\n",
    "print(f\"LightGBM AUC: {auc_lgb_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Precision@500 (The Business Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate Precision@500 for both models\n",
    "\n",
    "K = 500  # Retention team capacity\n",
    "\n",
    "def precision_at_k(y_true, y_proba, k):\n",
    "    \"\"\"Calculate precision in top k predictions.\"\"\"\n",
    "    top_k_idx = np.argsort(y_proba)[::-1][:k]\n",
    "    return y_true.iloc[top_k_idx].mean()\n",
    "\n",
    "# Scale K to test set size (500 is for full dataset)\n",
    "k_test = int(K * len(y_test) / len(y))\n",
    "\n",
    "precision_lr = precision_at_k(y_test, probs_lr_test, k_test)\n",
    "precision_lgb = precision_at_k(y_test, probs_lgb_test, k_test)\n",
    "precision_random = y_test.mean()  # baseline\n",
    "\n",
    "print(f\"\\n=== PRECISION@{k_test} (scaled from @500) ===\")\n",
    "print(f\"Random baseline: {precision_random:.1%}\")\n",
    "print(f\"Logistic Regression: {precision_lr:.1%}\")\n",
    "print(f\"LightGBM: {precision_lgb:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate LIFT\n",
    "lift_lr = precision_lr / precision_random\n",
    "lift_lgb = precision_lgb / precision_random\n",
    "\n",
    "print(f\"\\n=== LIFT ===\")\n",
    "print(f\"Logistic Regression: {lift_lr:.1f}x better than random\")\n",
    "print(f\"LightGBM: {lift_lgb:.1f}x better than random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Business Impact Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate business impact for your best model\n",
    "\n",
    "# Use the better model\n",
    "best_precision = max(precision_lr, precision_lgb)\n",
    "best_model = \"LightGBM\" if precision_lgb > precision_lr else \"Logistic Regression\"\n",
    "\n",
    "# Scale back to full 500 calls\n",
    "expected_churners_model = 500 * best_precision\n",
    "expected_saves_model = expected_churners_model * save_rate\n",
    "net_value_model = (expected_saves_model * value_per_save) - (500 * cost_per_call)\n",
    "\n",
    "# Improvement over random\n",
    "value_improvement = net_value_model - net_value_random\n",
    "\n",
    "print(f\"\\n=== BUSINESS IMPACT ({best_model}) ===\")\n",
    "print(f\"Expected churners in top 500: {expected_churners_model:.0f}\")\n",
    "print(f\"Expected saves: {expected_saves_model:.0f}\")\n",
    "print(f\"Net value per week: ${net_value_model:,.0f}\")\n",
    "print(f\"\\nImprovement over random: ${value_improvement:,.0f}/week\")\n",
    "print(f\"Annual impact: ${value_improvement * 52:,.0f}/year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-check: Evaluation complete\n",
    "assert best_precision > precision_random, \"Model should beat random baseline\"\n",
    "assert value_improvement > 0, \"Model should create positive business value\"\n",
    "print(\"✓ Part 4 evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Random', 'Logistic Regression', 'LightGBM'],\n",
    "    'AUC': [0.50, auc_lr_test, auc_lgb_test],\n",
    "    f'Precision@{k_test}': [precision_random, precision_lr, precision_lgb],\n",
    "    'Lift': [1.0, lift_lr, lift_lgb],\n",
    "    'Interpretable': ['N/A', 'Yes', 'Limited']\n",
    "})\n",
    "\n",
    "print(\"\\n=== MODEL COMPARISON ===\")\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Communication (20 points)\n",
    "\n",
    "Write a clear recommendation for stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 PM Update\n",
    "\n",
    "Write a 200-word update for the VP of Customer Success. Include:\n",
    "- What you built\n",
    "- Key results (in business terms)\n",
    "- Recommendation\n",
    "- Next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your PM update (replace the template below)\n",
    "\n",
    "pm_update = \"\"\"\n",
    "=== WEEKLY UPDATE: CHURN PREDICTION MODEL ===\n",
    "\n",
    "Hi [VP Name],\n",
    "\n",
    "[WHAT WE BUILT]\n",
    "TODO: Describe what you built in 1-2 sentences. Avoid jargon.\n",
    "\n",
    "[KEY RESULTS]\n",
    "TODO: Share the business impact in concrete numbers:\n",
    "- How many more churners will the team reach?\n",
    "- What's the expected value improvement?\n",
    "- How does this compare to current random selection?\n",
    "\n",
    "[RECOMMENDATION]\n",
    "TODO: State your recommendation clearly:\n",
    "- Which model should we use?\n",
    "- What trade-offs should stakeholders be aware of?\n",
    "\n",
    "[NEXT STEPS]\n",
    "TODO: Propose 2-3 concrete next steps:\n",
    "- Pilot test?\n",
    "- Integration with call system?\n",
    "- Monitoring plan?\n",
    "\n",
    "Let me know if you have questions.\n",
    "\n",
    "[Your Name]\n",
    "\"\"\"\n",
    "\n",
    "print(pm_update)\n",
    "print(f\"\\nWord count: {len(pm_update.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Final Self-Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final checklist\n",
    "checklist = \"\"\"\n",
    "=== CAPSTONE CHECKLIST ===\n",
    "\n",
    "Part 1: Problem Framing (20 pts)\n",
    "[ ] Completed 7-line framing template\n",
    "[ ] Calculated random baseline\n",
    "\n",
    "Part 2: Feature Engineering (20 pts)\n",
    "[ ] Created 3+ meaningful features\n",
    "[ ] Completed leakage audit\n",
    "[ ] No future data used\n",
    "\n",
    "Part 3: Model Training (20 pts)\n",
    "[ ] Trained baseline model (logistic regression)\n",
    "[ ] Trained advanced model (LightGBM)\n",
    "[ ] Used early stopping\n",
    "[ ] Checked for overfitting\n",
    "\n",
    "Part 4: Evaluation (20 pts)\n",
    "[ ] Reported Precision@500\n",
    "[ ] Calculated Lift\n",
    "[ ] Computed business impact ($)\n",
    "[ ] Created comparison table\n",
    "\n",
    "Part 5: Communication (20 pts)\n",
    "[ ] Wrote PM update (150-250 words)\n",
    "[ ] Included business metrics\n",
    "[ ] Made clear recommendation\n",
    "[ ] Proposed next steps\n",
    "\"\"\"\n",
    "print(checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Bonus: Customer Segmentation (Optional)\n",
    "\n",
    "Segment customers to enable differentiated marketing campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Customer segmentation\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Cluster on behavioral features\n",
    "cluster_features = ['tenure_months', 'logins_last_30d', 'orders_last_30d', 'support_tickets_last_30d']\n",
    "X_cluster = df[cluster_features].fillna(0)\n",
    "X_cluster_scaled = StandardScaler().fit_transform(X_cluster)\n",
    "\n",
    "# Find optimal K using elbow method\n",
    "inertias = []\n",
    "for k in range(2, 8):\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    km.fit(X_cluster_scaled)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(2, 8), inertias, 'bo-')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply clustering with chosen K\n",
    "k_chosen = 4  # Based on elbow\n",
    "kmeans = KMeans(n_clusters=k_chosen, random_state=42, n_init=10)\n",
    "df['segment'] = kmeans.fit_predict(X_cluster_scaled)\n",
    "\n",
    "# Profile segments\n",
    "segment_profiles = df.groupby('segment').agg({\n",
    "    'tenure_months': 'mean',\n",
    "    'logins_last_30d': 'mean',\n",
    "    'orders_last_30d': 'mean',\n",
    "    'support_tickets_last_30d': 'mean',\n",
    "    'churn_30d': 'mean',\n",
    "    'customer_id': 'count'\n",
    "}).round(2)\n",
    "\n",
    "segment_profiles.columns = ['Tenure', 'Logins', 'Orders', 'Tickets', 'Churn Rate', 'Count']\n",
    "print(\"\\n=== SEGMENT PROFILES ===\")\n",
    "print(segment_profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the capstone project. You now have:\n",
    "\n",
    "1. **A clear problem framing** that ties ML to business action\n",
    "2. **Meaningful features** without data leakage\n",
    "3. **Two trained models** with proper validation\n",
    "4. **Business-oriented evaluation** (Precision@K, Lift, $ impact)\n",
    "5. **A stakeholder-ready recommendation**\n",
    "\n",
    "This is exactly what ML looks like in industry. Well done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
